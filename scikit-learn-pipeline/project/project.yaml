name: sk-project
functions:
- name: gen-iris
  spec:
    kind: job
    metadata:
      name: gen-iris
      tag: ''
      project: sk-project
    spec:
      command: ''
      args: []
      env: []
      default_handler: ''
      entry_points:
        iris_generator:
          name: iris_generator
          doc: ''
          parameters:
          - name: context
          - name: format
            default: csv
          outputs: []
          lineno: 11
      description: ''
      build:
        functionSourceCode: IyBHZW5lcmF0ZWQgYnkgbnVjbGlvLmV4cG9ydC5OdWNsaW9FeHBvcnRlcgoKaW1wb3J0IG9zCmZyb20gc2tsZWFybi5kYXRhc2V0cyBpbXBvcnQgbG9hZF9pcmlzCmZyb20gc2tsZWFybi5tb2RlbF9zZWxlY3Rpb24gaW1wb3J0IHRyYWluX3Rlc3Rfc3BsaXQKaW1wb3J0IG51bXB5IGFzIG5wCmZyb20gc2tsZWFybi5tZXRyaWNzIGltcG9ydCBhY2N1cmFjeV9zY29yZQpmcm9tIG1scnVuLmFydGlmYWN0cyBpbXBvcnQgVGFibGVBcnRpZmFjdCwgUGxvdEFydGlmYWN0CmltcG9ydCBwYW5kYXMgYXMgcGQKCmRlZiBpcmlzX2dlbmVyYXRvcihjb250ZXh0LCBmb3JtYXQ9J2NzdicpOgogICAgaXJpcyA9IGxvYWRfaXJpcygpCiAgICBpcmlzX2RhdGFzZXQgPSBwZC5EYXRhRnJhbWUoZGF0YT1pcmlzLmRhdGEsIGNvbHVtbnM9aXJpcy5mZWF0dXJlX25hbWVzKQogICAgaXJpc19sYWJlbHMgPSBwZC5EYXRhRnJhbWUoZGF0YT1pcmlzLnRhcmdldCwgY29sdW1ucz1bJ2xhYmVsJ10pCiAgICBpcmlzX2RhdGFzZXQgPSBwZC5jb25jYXQoW2lyaXNfZGF0YXNldCwgaXJpc19sYWJlbHNdLCBheGlzPTEpCiAgICAKICAgIGNvbnRleHQubG9nZ2VyLmluZm8oJ3NhdmluZyBpcmlzIGRhdGFmcmFtZSB0byB7fScuZm9ybWF0KGNvbnRleHQuYXJ0aWZhY3RfcGF0aCkpCiAgICBjb250ZXh0LmxvZ19kYXRhc2V0KCdpcmlzX2RhdGFzZXQnLCBkZj1pcmlzX2RhdGFzZXQsIGZvcm1hdD1mb3JtYXQsIGluZGV4PUZhbHNlKQoK
        base_image: mlrun/mlrun
        commands:
        - pip install sklearn
        - pip install pyarrow
- url: hub://describe
  name: describe
- url: hub://sklearn_classifier
  name: train
- url: hub://test_classifier
  name: test
- url: hub://model_server
  name: serving
- url: hub://model_server_tester
  name: live_tester
workflows:
- name: main
  code: "from kfp import dsl\nfrom mlrun import mount_v3io\n\nfuncs = {}\nDATASET\
    \ = 'iris_dataset'\nLABELS  = \"label\"\n\n\n# init functions is used to configure\
    \ function resources and local settings\ndef init_functions(functions: dict, project=None,\
    \ secrets=None):\n    for f in functions.values():\n        f.apply(mount_v3io())\n\
    \     \n    # uncomment this line to collect the inference results into a stream\n\
    \    # and specify a path in V3IO (<datacontainer>/<subpath>)\n    #functions['serving'].set_env('INFERENCE_STREAM',\
    \ 'users/admin/model_stream')\n\n    \n@dsl.pipeline(\n    name=\"Demo training\
    \ pipeline\",\n    description=\"Shows how to use mlrun.\"\n)\ndef kfpipeline():\n\
    \    \n    # build our ingestion function (container image)\n    builder = funcs['gen-iris'].deploy_step(skip_deployed=True)\n\
    \    \n    # run the ingestion function with the new image and params\n    ingest\
    \ = funcs['gen-iris'].as_step(\n        name=\"get-data\",\n        handler='iris_generator',\n\
    \        image=builder.outputs['image'],\n        params={'format': 'pq'},\n \
    \       outputs=[DATASET])\n\n    # analyze our dataset\n    describe = funcs[\"\
    describe\"].as_step(\n        name=\"summary\",\n        params={\"label_column\"\
    : LABELS},\n        inputs={\"table\": ingest.outputs[DATASET]})\n    \n    #\
    \ train with hyper-paremeters \n    train = funcs[\"train\"].as_step(\n      \
    \  name=\"train-skrf\",\n        params={\"sample\"          : -1, \n        \
    \        \"label_column\"    : LABELS,\n                \"test_size\"       :\
    \ 0.10},\n        hyperparams={'model_pkg_class': [\"sklearn.ensemble.RandomForestClassifier\"\
    , \n                                         \"sklearn.linear_model.LogisticRegression\"\
    ,\n                                         \"sklearn.ensemble.AdaBoostClassifier\"\
    ]},\n        selector='max.accuracy',\n        inputs={\"dataset\"         : ingest.outputs[DATASET]},\n\
    \        outputs=['model', 'test_set'])\n\n    # test and visualize our model\n\
    \    test = funcs[\"test\"].as_step(\n        name=\"test\",\n        params={\"\
    label_column\": LABELS},\n        inputs={\"models_path\" : train.outputs['model'],\n\
    \                \"test_set\"    : train.outputs['test_set']})\n\n    # deploy\
    \ our model as a serverless function\n    deploy = funcs[\"serving\"].deploy_step(models={f\"\
    {DATASET}_v1\": train.outputs['model']}, tag='v2')\n    \n    # test out new model\
    \ server (via REST API calls)\n    tester = funcs[\"live_tester\"].as_step(name='model-tester',\n\
    \        params={'addr': deploy.outputs['endpoint'], 'model': f\"{DATASET}_v1\"\
    },\n        inputs={'table': train.outputs['test_set']})\n"
artifacts: []
