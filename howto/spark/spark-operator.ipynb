{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job with Spark Operator\n",
    "Using spark operator for running spark job over k8s.<br>\n",
    "\n",
    "The `spark-on-k8s-operator` allows Spark applications to be defined in a declarative manner and supports one-time Spark applications with `SparkApplication` and cron-scheduled applications with `ScheduledSparkApplication`. <br>\n",
    "\n",
    "When sending a request with MLRun to Spark operator the request contains your full application configuration including the code and dependencies to run (packaged as a docker image or specified via URIs), the infrastructure parameters, (e.g. the memory, CPU, and storage volume specs to allocate to each Spark executor), and the Spark configuration.\n",
    "\n",
    "Kubernetes takes this request and starts the Spark driver in a Kubernetes pod (a k8s abstraction, just a docker container in this case). The Spark driver can then directly talk back to the Kubernetes master to request executor pods, scaling them up and down at runtime according to the load if dynamic allocation is enabled. Kubernetes takes care of the bin-packing of the pods onto Kubernetes nodes (the physical VMs), and will dynamically scale the various node pools to meet the requirements.\n",
    "\n",
    "When using Spark operator the resources will be allocated per task, means scale down to zero when the tesk is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperations\n",
    "The 1st step is to prepare the iris dataset that we will use in this example.  \n",
    "We will get the file using `mlrun.get_object()` directly from github and save it to our `projects` data container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "\n",
    "# Create the data folder and set the dataset filepath\n",
    "# We will save the dataset to our projects container\n",
    "iris_dataset_filepath = os.path.abspath('/v3io/projects/howto/spark-operator/iris.csv')\n",
    "os.makedirs(os.path.dirname(iris_dataset_filepath), exist_ok=True)\n",
    "\n",
    "# Get the dataset from git\n",
    "iris_dataset = mlrun.get_object('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "\n",
    "# Save the dataset at the designated path\n",
    "with open(iris_dataset_filepath, 'wb') as f:\n",
    "    f.write(iris_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Operator Function Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up new spark function with spark operator\n",
    "# command will use our spark code which needs to be located on our file system\n",
    "# the name param can have only non capital letters (k8s convention)\n",
    "read_csv_filepath = os.path.join(os.path.abspath('.'), 'spark_read_csv.py')\n",
    "sj = mlrun.new_function(kind='spark', command=read_csv_filepath, name='sparkreadcsv') \n",
    "\n",
    "# set spark driver config (gpu_type & gpus=<number_of_gpus>  supported too)\n",
    "sj.with_driver_limits(cpu=\"1300m\")\n",
    "sj.with_driver_requests(cpu=1, mem=\"512m\") \n",
    "\n",
    "# set spark executor config (gpu_type & gpus=<number_of_gpus> are supported too)\n",
    "sj.with_executor_limits(cpu=\"1400m\")\n",
    "sj.with_executor_requests(cpu=1, mem=\"512m\")\n",
    "\n",
    "# adds fuse, daemon & iguazio's jars support\n",
    "sj.with_igz_spark() \n",
    "\n",
    "# args are also supported\n",
    "sj.spec.args = ['-spark.eventLog.enabled','true']\n",
    "\n",
    "# add python module\n",
    "sj.spec.build.commands = ['pip install matplotlib']\n",
    "\n",
    "# Number of executors\n",
    "sj.spec.replicas = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the spark function\n",
    "### Build the docker image\n",
    "If our function requires additional packages that are not yet available via any of our images, we may want to build a new docker image for it.  \n",
    "Using the `fn.spec.build.baseImage` as base (defaults to base python 3) and the additional `fn.spec.build.commands` MLRun will build and deploy the image for you.\n",
    "\n",
    "> You can skip this step if you had provided a ready image for the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilds the image with MLRun\n",
    "sj.deploy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run task while setting the artifact path on which our run artifact (in any) will be saved\n",
    "sj.run(artifact_path='/User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
