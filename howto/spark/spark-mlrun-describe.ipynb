{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job with MLRun\n",
    "Using MLRun to run Spark job.\n",
    "The Spark job will run a describe function, which generates profile report<br>\n",
    "from an Apache Spark DataFrame (Based on pandas_profiling).<br>\n",
    "\n",
    "For each column the following statistics - if relevant for the column type - are presented:\n",
    "\n",
    "**Essentials:** `type`, `unique values`, `missing values`,\n",
    "\n",
    "**Quantile statistics:** `minimum value`, `Q1`, `median`, `Q3`, `maximum`, `range`, `interquartile range`.\n",
    "\n",
    "**Descriptive statistics:** `mean`, `mode`, `standard deviation`, `sum`, `median absolute deviation`,<br> \n",
    "                            `coefficient of variation`, `kurtosis`, `skewness`.<br>\n",
    "                        \n",
    "**Most frequent values:** for categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nuclio: ignore\n",
    "# get build base image name by running the following code\n",
    "# this will use us to config our spark job docker image\n",
    "# Note: using the ${env_var} annotation will only work when **running** the notebook.\n",
    "#       To allow creating a function from this notebook directly (instead of saving the function yaml\n",
    "#       via `fn.export()` and importing the resulted `yaml` file) please **copy** the `baseImage_address` \n",
    "#       value and set it as the `spec.build.baseImage` nuclio configuration directly.\n",
    "import os\n",
    "os.environ['baseImage_address'] = os.environ[\"IGZ_DATANODE_REGISTRY_URL\"] + '/iguazio/shell:' + os.environ[\"IGZ_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.build.baseImage to 'datanode-registry.iguazio-platform.app.hsbctesting3.iguazio-cd0.com:80/iguazio/shell:3.0_katyak_debug_b1089_20201214154653'\n"
     ]
    }
   ],
   "source": [
    "# set the function kind and docker image\n",
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.build.baseImage = \"${baseImage_address}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.platforms.iguazio import mount_v3io, mount_v3iod\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.execution import MLClientCtx\n",
    "\n",
    "import os\n",
    "from subprocess import run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Spark Describe Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache using fc-list. This may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import base64 as b64\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib\n",
    "\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from pkg_resources import resource_filename\n",
    "import six\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct,\n",
    "                                   max as df_max, mean, min as df_min,\n",
    "                                   sum as df_sum, when\n",
    "                                   )\n",
    "from pyspark.sql.functions import variance, stddev, kurtosis, skewness\n",
    "\n",
    "\n",
    "def describe(df, bins, corr_reject, config, **kwargs):\n",
    "    if not isinstance(df, SparkDataFrame):\n",
    "        raise TypeError(\"df must be of type pyspark.sql.DataFrame\")\n",
    "\n",
    "    # Number of rows:\n",
    "    table_stats = {\"n\": df.count()}\n",
    "    if table_stats[\"n\"] == 0:\n",
    "        raise ValueError(\"df cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        # reset matplotlib style before use\n",
    "        matplotlib.style.use(\"default\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Function to \"pretty name\" floats:\n",
    "    def pretty_name(x):\n",
    "        x *= 100\n",
    "        if x == int(x):\n",
    "            return '%.0f%%' % x\n",
    "        else:\n",
    "            return '%.1f%%' % x\n",
    "\n",
    "    # Function to compute the correlation matrix:\n",
    "    def corr_matrix(df, columns=None):\n",
    "        if columns is None:\n",
    "            columns = df.columns\n",
    "        combinations = list(product(columns,columns))\n",
    "\n",
    "        def separate(l, n):\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i+n]\n",
    "\n",
    "        grouped = list(separate(combinations,len(columns)))\n",
    "        df_cleaned = df.select(*columns).na.drop(how=\"any\")\n",
    "\n",
    "        for i in grouped:\n",
    "            for j in enumerate(i):\n",
    "                i[j[0]] = i[j[0]] + (df_cleaned.corr(str(j[1][0]), str(j[1][1])),)\n",
    "\n",
    "        df_pandas = pd.DataFrame(grouped).applymap(lambda x: x[2])\n",
    "        df_pandas.columns = columns\n",
    "        df_pandas.index = columns\n",
    "        \n",
    "        return df_pandas\n",
    "\n",
    "    # Compute histogram \n",
    "    def create_hist_data(df, column, minim, maxim, bins=10):\n",
    "\n",
    "        def create_all_conditions(current_col, column, left_edges, count=1):\n",
    "            \"\"\"\n",
    "            Recursive function that exploits the\n",
    "            ability to call the Spark SQL Column method\n",
    "            .when() in a recursive way.\n",
    "            \"\"\"\n",
    "            left_edges = left_edges[:]\n",
    "            if len(left_edges) == 0:\n",
    "                return current_col\n",
    "            if len(left_edges) == 1:\n",
    "                next_col = current_col.when(col(column) >= float(left_edges[0]), count)\n",
    "                left_edges.pop(0)\n",
    "                return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "            next_col = current_col.when((float(left_edges[0]) <= col(column))\n",
    "                                        & (col(column) < float(left_edges[1])), count)\n",
    "            left_edges.pop(0)\n",
    "            return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "\n",
    "        num_range = maxim - minim\n",
    "        bin_width = num_range / float(bins)\n",
    "        left_edges = [minim]\n",
    "        for _bin in range(bins):\n",
    "            left_edges = left_edges + [left_edges[-1] + bin_width]\n",
    "        left_edges.pop()\n",
    "        expression_col = when((float(left_edges[0]) <= col(column))\n",
    "                              & (col(column) < float(left_edges[1])), 0)\n",
    "        left_edges_copy = left_edges[:]\n",
    "        left_edges_copy.pop(0)\n",
    "        bin_data = (df.select(col(column))\n",
    "                    .na.drop()\n",
    "                    .select(col(column),\n",
    "                            create_all_conditions(expression_col,\n",
    "                                                  column,\n",
    "                                                  left_edges_copy\n",
    "                                                 ).alias(\"bin_id\")\n",
    "                           )\n",
    "                    .groupBy(\"bin_id\").count()\n",
    "                   ).toPandas()\n",
    "\n",
    "        # If no data goes into one bin, it won't \n",
    "        # appear in bin_data; so we should fill\n",
    "        # in the blanks:\n",
    "        bin_data.index = bin_data[\"bin_id\"]\n",
    "        new_index = list(range(bins))\n",
    "        bin_data = bin_data.reindex(new_index)\n",
    "        bin_data[\"bin_id\"] = bin_data.index\n",
    "        bin_data = bin_data.fillna(0)\n",
    "\n",
    "        bin_data[\"left_edge\"] = left_edges\n",
    "        bin_data[\"width\"] = bin_width\n",
    "        \n",
    "\n",
    "        return bin_data\n",
    "\n",
    "\n",
    "    def describe_integer_1d(df, column, current_result, nrows):\n",
    "        \n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_float_1d(df, column, current_result, nrows):\n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile_approx(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_date_1d(df, column):\n",
    "        stats_df = df.select(column).na.drop().agg(df_min(col(column)).alias(\"min\"),\n",
    "                                                   df_max(col(column)).alias(\"max\")\n",
    "                                                  ).toPandas()\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "\n",
    "        if isinstance(stats[\"max\"], pd.Timestamp):\n",
    "            stats = stats.astype(object)\n",
    "            stats[\"max\"] = str(stats[\"max\"].to_pydatetime())\n",
    "            stats[\"min\"] = str(stats[\"min\"].to_pydatetime())\n",
    "\n",
    "        else:\n",
    "            stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"type\"] = \"DATE\"\n",
    "        return stats\n",
    "\n",
    "    def guess_json_type(string_value):\n",
    "        try:\n",
    "            obj = json.loads(string_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return type(obj)\n",
    "\n",
    "    def describe_categorical_1d(df, column):\n",
    "        value_counts = (df.select(column).na.drop()\n",
    "                        .groupBy(column)\n",
    "                        .agg(count(col(column)))\n",
    "                        .orderBy(\"count({c})\".format(c=column),ascending=False)\n",
    "                       ).cache()\n",
    "\n",
    "        # Get the most frequent class:\n",
    "        stats = (value_counts\n",
    "                 .limit(1)\n",
    "                 .withColumnRenamed(column, \"top\")\n",
    "                 .withColumnRenamed(\"count({c})\".format(c=column), \"freq\")\n",
    "                ).toPandas().iloc[0]\n",
    "\n",
    "        # Get the top 50 classes by value count,\n",
    "        # and put the rest of them grouped at the\n",
    "        # end of the Series:\n",
    "        top_50 = value_counts.limit(50).toPandas().sort_values(\"count({c})\".format(c=column),\n",
    "                                                               ascending=False)\n",
    "        top_50_categories = top_50[column].values.tolist()\n",
    "\n",
    "        others_count = pd.Series([df.select(column).na.drop()\n",
    "                        .where(~(col(column).isin(*top_50_categories)))\n",
    "                        .count()\n",
    "                        ], index=[\"***Other Values***\"])\n",
    "        others_distinct_count = pd.Series([value_counts\n",
    "                                .where(~(col(column).isin(*top_50_categories)))\n",
    "                                .count()\n",
    "                                ], index=[\"***Other Values Distinct Count***\"])\n",
    "\n",
    "        top = top_50.set_index(column)[\"count({c})\".format(c=column)]\n",
    "        top = top.append(others_count)\n",
    "        top = top.append(others_distinct_count)\n",
    "        stats[\"value_counts\"] = top\n",
    "        stats[\"type\"] = \"CAT\"\n",
    "        value_counts.unpersist()\n",
    "        unparsed_valid_jsons = df.select(column).na.drop().rdd.map(\n",
    "            lambda x: guess_json_type(x[column])).filter(\n",
    "            lambda x: x).distinct().collect()\n",
    "        stats[\"unparsed_json_types\"] = unparsed_valid_jsons\n",
    "        return stats\n",
    "\n",
    "    def describe_constant_1d(df, column):\n",
    "        stats = pd.Series(['CONST'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(1)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_unique_1d(df, column):\n",
    "        stats = pd.Series(['UNIQUE'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(50)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_1d(df, column, nrows, lookup_config=None):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "\n",
    "        distinct_count = df.select(column).agg(countDistinct(col(column)).alias(\"distinct_count\")).toPandas()\n",
    "        non_nan_count = df.select(column).na.drop().select(count(col(column)).alias(\"count\")).toPandas()\n",
    "        results_data = pd.concat([distinct_count, non_nan_count],axis=1)\n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / float(results_data[\"count\"])\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / float(nrows)\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        result = results_data.iloc[0].copy()\n",
    "        result[\"memorysize\"] = 0\n",
    "        result.name = column\n",
    "\n",
    "        if result[\"distinct_count\"] <= 1:\n",
    "            result = result.append(describe_constant_1d(df, column))\n",
    "        elif column_type in {\"tinyint\", \"smallint\", \"int\", \"bigint\"}:\n",
    "            result = result.append(describe_integer_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"float\", \"double\", \"decimal\"}:\n",
    "            result = result.append(describe_float_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"date\", \"timestamp\"}:\n",
    "            result = result.append(describe_date_1d(df, column))\n",
    "        elif result[\"is_unique\"] == True:\n",
    "            result = result.append(describe_unique_1d(df, column))\n",
    "        else:\n",
    "            result = result.append(describe_categorical_1d(df, column))\n",
    "            # Fix to also count MISSING value in the distict_count field:\n",
    "            if result[\"n_missing\"] > 0:\n",
    "                result[\"distinct_count\"] = result[\"distinct_count\"] + 1\n",
    "\n",
    "        if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"top\"]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "        else:\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "            # If and IndexError happens,\n",
    "            # it is because all column are NULLs:\n",
    "            except IndexError:\n",
    "                result[\"mode\"] = \"MISSING\"\n",
    "\n",
    "        if lookup_config:\n",
    "            lookup_object = lookup_config['object']\n",
    "            col_name_in_db = lookup_config['col_name_in_db'] if 'col_name_in_db' in lookup_config else None\n",
    "            try:\n",
    "                matched, unmatched = lookup_object.lookup(df.select(column), col_name_in_db)\n",
    "                result['lookedup_values'] = str(matched.count()) + \"/\" + str(df.select(column).count())\n",
    "            except:\n",
    "                result['lookedup_values'] = 'FAILED'\n",
    "        else:\n",
    "            result['lookedup_values'] = ''\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # build final report:\n",
    "    ldesc = {}\n",
    "    for colum in df.columns:\n",
    "        if colum in config:\n",
    "            if 'lookup' in config[colum]:\n",
    "                lookup_config = config[colum]['lookup']\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"], lookup_config=lookup_config)\n",
    "            else:\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        else:\n",
    "            desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        ldesc.update({colum: desc})\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    if corr_reject is not None:\n",
    "        computable_corrs = [colum for colum in ldesc if ldesc[colum][\"type\"] in {\"NUM\"}]\n",
    "\n",
    "        if len(computable_corrs) > 0:\n",
    "            corr = corr_matrix(df, columns=computable_corrs)\n",
    "            for x, corr_x in corr.iterrows():\n",
    "                for y, corr in corr_x.iteritems():\n",
    "                    if x == y:\n",
    "                        break\n",
    "\n",
    "    # Convert ldesc (final report) to a DataFrame\n",
    "    variable_stats = pd.DataFrame(ldesc)\n",
    "\n",
    "    # General statistics\n",
    "    table_stats[\"nvar\"] = len(df.columns)\n",
    "    table_stats[\"total_missing\"] = float(variable_stats.loc[\"n_missing\"].sum()) / (table_stats[\"n\"] * table_stats[\"nvar\"])\n",
    "    memsize = 0\n",
    "    table_stats['memsize'] = fmt_bytesize(memsize)\n",
    "    table_stats['recordsize'] = fmt_bytesize(memsize / table_stats['n'])\n",
    "    table_stats.update({k: 0 for k in (\"NUM\", \"DATE\", \"CONST\", \"CAT\", \"UNIQUE\", \"CORR\")})\n",
    "    table_stats.update(dict(variable_stats.loc['type'].value_counts()))\n",
    "    table_stats['REJECTED'] = table_stats['CONST'] + table_stats['CORR']\n",
    "\n",
    "    freq_dict = {}\n",
    "    for var in variable_stats:\n",
    "        if \"value_counts\" not in variable_stats[var]:\n",
    "            pass\n",
    "        elif not(variable_stats[var][\"value_counts\"] is np.nan):\n",
    "            freq_dict[var] = variable_stats[var][\"value_counts\"]\n",
    "        else:\n",
    "            pass\n",
    "    try:\n",
    "        variable_stats = variable_stats.drop(\"value_counts\")\n",
    "    except (ValueError, KeyError):\n",
    "        pass\n",
    "\n",
    "    return table_stats, variable_stats.T, freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import abs as absou\n",
    "\n",
    "SKEWNESS_CUTOFF = 20\n",
    "DEFAULT_FLOAT_FORMATTER = u'spark_df_profiling.__default_float_formatter'\n",
    "\n",
    "# formmating functions\n",
    "def gradient_format(value, limit1, limit2, c1, c2):\n",
    "    def LerpColour(c1,c2,t):\n",
    "        return (int(c1[0]+(c2[0]-c1[0])*t),int(c1[1]+(c2[1]-c1[1])*t),int(c1[2]+(c2[2]-c1[2])*t))\n",
    "    c = LerpColour(c1, c2, (value-limit1)/(limit2-limit1))\n",
    "    return fmt_color(value,\"rgb{}\".format(str(c)))\n",
    "\n",
    "\n",
    "def fmt_color(text, color):\n",
    "    return(u'<span style=\"color:{color}\">{text}</span>'.format(color=color,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_class(text, cls):\n",
    "    return(u'<span class=\"{cls}\">{text}</span>'.format(cls=cls,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_bytesize(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if num < 0:\n",
    "            num = num*-1\n",
    "            if num < 1024.0:\n",
    "                return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "            num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def fmt_percent(v):\n",
    "    return  \"{:2.1f}%\".format(v*100)\n",
    "\n",
    "def fmt_varname(v):\n",
    "    return u'<code>{0}</code>'.format(v)\n",
    "\n",
    "\n",
    "value_formatters={\n",
    "        u'freq': (lambda v: gradient_format(v, 0, 62000, (30, 198, 244), (99, 200, 72))),\n",
    "        u'p_missing': fmt_percent,\n",
    "        u'p_infinite': fmt_percent,\n",
    "        u'p_unique': fmt_percent,\n",
    "        u'p_zeros': fmt_percent,\n",
    "        u'memorysize': fmt_bytesize,\n",
    "        u'total_missing': fmt_percent,\n",
    "        DEFAULT_FLOAT_FORMATTER: lambda v: str(float('{:.5g}'.format(v))).rstrip('0').rstrip('.'),\n",
    "        u'correlation_var': lambda v: fmt_varname(v),\n",
    "        u'unparsed_json_types': lambda v: ', '.join([s.__name__ for s in v])\n",
    "        }\n",
    "\n",
    "def fmt_row_severity(v):\n",
    "    if np.isnan(v) or v<= 0.01:\n",
    "        return \"ignore\"\n",
    "    else:\n",
    "        return \"alert\"\n",
    "\n",
    "def fmt_skewness(v):\n",
    "    if not np.isnan(v) and (v<-SKEWNESS_CUTOFF or v> SKEWNESS_CUTOFF):\n",
    "        return \"alert\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "row_formatters={\n",
    "    u'p_zeros': fmt_row_severity,\n",
    "    u'p_missing': fmt_row_severity,\n",
    "    u'p_infinite': fmt_row_severity,\n",
    "    u'n_duplicates': fmt_row_severity,\n",
    "    u'skewness': fmt_skewness,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Spark Describe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run([\"/bin/bash\", \"/etc/config/v3io/spark-job-init.sh\"])\n",
    "\n",
    "def describe_spark(context: MLClientCtx, \n",
    "                   dataset: DataItem,\n",
    "                   bins: int=30,\n",
    "                   describe_extended: bool=True)-> None:\n",
    "    \"\"\"\n",
    "    Generates profile reports from an Apache Spark DataFrame. \n",
    "    Based on pandas_profiling, but for Spark's DataFrames instead of pandas.\n",
    "    For each column the following statistics - if relevant for the column type - are presented:\n",
    "    \n",
    "    Essentials: type, unique values, missing values\n",
    "    \n",
    "    Quantile statistics: minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "    \n",
    "    Descriptive statistics: mean, mode, standard deviation, sum, median absolute deviation, \n",
    "                            coefficient of variation, kurtosis, skewness\n",
    "                            \n",
    "    Most frequent values: for categorical data \n",
    "    --------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "                context : MLClientCtx\n",
    "                          MLRun introduces a concept of a runtime \"context\", \n",
    "                          the code can be set up to get parameters and inputs from the context, \n",
    "                          as well as log run outputs, artifacts, tags, and time-series metrics in the context.\n",
    "                                      \n",
    "                dataset : csv_file\n",
    "                          csv file which needs to be local (on our machine)\n",
    "                          the default location will be \"/v3io/projects/<file_name> \n",
    "                          which can be change by using mlrun.mount_v3io later in the function specs\n",
    "                          \n",
    "                bins :    Integer\n",
    "                          Number of bin in histograms\n",
    "                          \n",
    "                describe_extended : Bool \n",
    "                         (True) set to False if the aim is to get a simple \n",
    "                         pandas.DataFrame.describe() like infomration\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Examples: \n",
    "               run mlrun function example, inputs will be part of the function inputs.\n",
    "               artifact_path is part of mlrun function parameters which set the path \n",
    "               for logging artifacts, results, dataset, etc.\n",
    "               \n",
    "               function.run(inputs={\"dataset\": \"iris.csv\",\n",
    "                                    \"bins\": 30,\n",
    "                                    \"describe_extended\": True},\n",
    "                                     artifact_path=artifact_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file location\n",
    "    location = dataset.local()\n",
    "    \n",
    "    # build spark session\n",
    "    spark = SparkSession.builder.appName(\"Spark job\").config(\"spark.executor.memory\",\"6g\").getOrCreate()\n",
    "    \n",
    "    # read csv\n",
    "    df = spark.read.csv(location, header=True, inferSchema= True)\n",
    "\n",
    "    # No use for now\n",
    "    kwargs = []\n",
    "    \n",
    "    # take only numric column\n",
    "    float_cols = [item[0] for item in df.dtypes if item[1].startswith('float') or item[1].startswith('double')]\n",
    "    \n",
    "    if describe_extended == True:\n",
    "        \n",
    "        # run describe function\n",
    "        table, variables, freq = describe(df, bins, float_cols, kwargs)\n",
    "\n",
    "        # get summary table\n",
    "        tbl_1 = variables.reset_index()\n",
    "\n",
    "        # prep report \n",
    "        if len(freq) != 0:\n",
    "            tbl_2 = pd.DataFrame.from_dict(freq, orient = \"index\").sort_index().stack().reset_index()\n",
    "            tbl_2.columns = ['col', 'key', 'val']\n",
    "            tbl_2['Merged'] = [{key: val} for key, val in zip(tbl_2.key, tbl_2.val)]\n",
    "            tbl_2 = tbl_2.groupby('col', as_index=False).agg(lambda x: tuple(x))[['col','Merged']]\n",
    "\n",
    "            # get summary\n",
    "            summary = pd.merge(tbl_1, tbl_2, how='left', left_on='index', right_on='col')\n",
    "\n",
    "        else:\n",
    "            summary = tbl_1\n",
    "\n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "\n",
    "        # log overview\n",
    "        context.log_results(table)\n",
    "    \n",
    "    else:\n",
    "        # run simple describe and save to pandas\n",
    "        tbl_1 = df.describe().toPandas()\n",
    "        \n",
    "        # save final report and transpose \n",
    "        summary = tbl_1.T\n",
    "        \n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "    \n",
    "    # stop spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't remove the # nuclio: end-code cell above\n",
    "### Set MLRun Function Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save spark service name (based on Iguazio services dashboard)\n",
    "spark_service = \"spark\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun will transform the code above (up to nuclio: end-code cell) into serverless function \n",
    "# which will run in k8s pods\n",
    "fn = mlrun.code_to_function(handler=\"describe_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mount_v3io over our function so that our k8s pod which run our function\n",
    "# will be able to access our data (shared data access)\n",
    "fn.apply(mlrun.mount_v3io_extended())\n",
    "fn.apply(mount_v3iod(namespace=\"default-tenant\", v3io_config_configmap=spark_service + \"-submit\"))\n",
    "\n",
    "# skip pulling an image if it already exists. If you would like to always force a pull, \n",
    "# you can set the imagePullPolicy of the container to Always.\n",
    "fn.spec.image_pull_policy = \"IfNotPresent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add build commands to our docker image with required moduls\n",
    "fn.spec.build.commands = ['pip install matplotlib mlrun==0.6.0-rc6 pyspark']\n",
    "\n",
    "# sets environment param in our docker image\n",
    "fn.spec.build.extra = 'ENV PATH $PATH:/igz/.local/bin'\n",
    "\n",
    "# print(fn.to_yamel()) will allow you to see the full function specs\n",
    "# addtional specs available and current specs applied to the current function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and deploy our docker image\n",
    "fn.deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLRun and Run Function\n",
    "Once running the function get be monitored here and our projects dashbaord<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mlrun api path and arrtifact path for logging\n",
    "artifact_path = mlrun.set_environment(api_path = 'http://mlrun-api:8080',\n",
    "                                      artifact_path = os.path.abspath('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-21 07:40:12,571 [info] starting run describe-spark-withui-describe_spark uid=88288fb691df46f9874c0a86eedd1700 DB=http://mlrun-api:8080\n",
      "> 2020-12-21 07:40:12,725 [info] Job is running in the background, pod: describe-spark-withui-describe-spark-bfcvw\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-8dgdj4xf because the default path (/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Matplotlib is building the font cache; this may take a moment.\n",
      "20/12/21 07:41:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "> 2020-12-21 07:42:24,239 [info] run executed, status=completed\n",
      "final state: completed                                                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"88288fb691df46f9874c0a86eedd1700\"><a href=\"https://mlrun-ui.default-tenant.app.hsbctesting3.iguazio-cd0.com/projects/default/jobs/monitor/88288fb691df46f9874c0a86eedd1700/info\" target=\"_blank\" >...eedd1700</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 21 07:41:33</td>\n",
       "      <td>completed</td>\n",
       "      <td>describe-spark-withui-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=describe-spark-withui-describe-spark-bfcvw</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result6c98f502\" title=\"/files/iris.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">n=150</div><div class=\"dictlist\">nvar=5</div><div class=\"dictlist\">total_missing=0.0</div><div class=\"dictlist\">memsize=0.0 YiB</div><div class=\"dictlist\">recordsize=0.0 YiB</div><div class=\"dictlist\">NUM=5</div><div class=\"dictlist\">DATE=0</div><div class=\"dictlist\">CONST=0</div><div class=\"dictlist\">CAT=0</div><div class=\"dictlist\">UNIQUE=0</div><div class=\"dictlist\">CORR=0</div><div class=\"dictlist\">REJECTED=0</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result6c98f502\" title=\"/files/data/summary_stats.csv\">summary_stats</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result6c98f502-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result6c98f502-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result6c98f502\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result6c98f502-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run 88288fb691df46f9874c0a86eedd1700 --project default , !mlrun logs 88288fb691df46f9874c0a86eedd1700 --project default\n",
      "> 2020-12-21 07:42:26,494 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "# run our functions with the relevant params\n",
    "run_res = fn.run(inputs={\"dataset\": \"iris.csv\"},\n",
    "                 artifact_path=artifact_path, watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "403be151df6dbe008b56b624a752ef6ee1649dfc674e5d8bf36595903ae83927"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}