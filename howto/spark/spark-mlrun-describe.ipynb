{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Job with MLRun\n",
    "Using MLRun to run Spark job.\n",
    "The Spark job will run a describe function, which generates profile report<br>\n",
    "from an Apache Spark DataFrame (Based on pandas_profiling).<br>\n",
    "\n",
    "For each column the following statistics - if relevant for the column type - are presented:\n",
    "\n",
    "**Essentials:** `type`, `unique values`, `missing values`,\n",
    "\n",
    "**Quantile statistics:** `minimum value`, `Q1`, `median`, `Q3`, `maximum`, `range`, `interquartile range`.\n",
    "\n",
    "**Descriptive statistics:** `mean`, `mode`, `standard deviation`, `sum`, `median absolute deviation`,<br> \n",
    "                            `coefficient of variation`, `kurtosis`, `skewness`.<br>\n",
    "                        \n",
    "**Most frequent values:** for categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package mlrun installed\n",
      "package matplotlib installed\n"
     ]
    }
   ],
   "source": [
    "# install prerequsits \n",
    "# prerequisits for the notebook is installing 2 packages yfinance yahoo_fin for uploading stocks data \n",
    "import importlib.util\n",
    "import IPython\n",
    "\n",
    "def install_missing_packages(packages):\n",
    "    install_flag = False\n",
    "    for package in packages:\n",
    "        spec = importlib.util.find_spec(package)\n",
    "        if spec is None:\n",
    "            %pip install {package}\n",
    "            install_flag = True\n",
    "        else:     \n",
    "            print(\"package {} installed\".format(package))\n",
    "        if install_flag:            \n",
    "            print (\"restarting kernerl due to package install\")\n",
    "            IPython.Application.instance().kernel.do_shutdown(True)\n",
    "# For illustrative purposes.\n",
    "packages  = ['mlrun', 'matplotlib']\n",
    "install_missing_packages(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.platforms.iguazio import mount_v3io, mount_v3iod\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun.execution import MLClientCtx\n",
    "\n",
    "import os\n",
    "from subprocess import run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Spark Describe Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import base64 as b64\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib\n",
    "\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from pkg_resources import resource_filename\n",
    "import six\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import (abs as df_abs, col, count, countDistinct,\n",
    "                                   max as df_max, mean, min as df_min,\n",
    "                                   sum as df_sum, when\n",
    "                                   )\n",
    "from pyspark.sql.functions import variance, stddev, kurtosis, skewness\n",
    "\n",
    "\n",
    "def describe(df, bins, corr_reject, config, **kwargs):\n",
    "    if not isinstance(df, SparkDataFrame):\n",
    "        raise TypeError(\"df must be of type pyspark.sql.DataFrame\")\n",
    "\n",
    "    # Number of rows:\n",
    "    table_stats = {\"n\": df.count()}\n",
    "    if table_stats[\"n\"] == 0:\n",
    "        raise ValueError(\"df cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        # reset matplotlib style before use\n",
    "        matplotlib.style.use(\"default\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Function to \"pretty name\" floats:\n",
    "    def pretty_name(x):\n",
    "        x *= 100\n",
    "        if x == int(x):\n",
    "            return '%.0f%%' % x\n",
    "        else:\n",
    "            return '%.1f%%' % x\n",
    "\n",
    "    # Function to compute the correlation matrix:\n",
    "    def corr_matrix(df, columns=None):\n",
    "        if columns is None:\n",
    "            columns = df.columns\n",
    "        combinations = list(product(columns,columns))\n",
    "\n",
    "        def separate(l, n):\n",
    "            for i in range(0, len(l), n):\n",
    "                yield l[i:i+n]\n",
    "\n",
    "        grouped = list(separate(combinations,len(columns)))\n",
    "        df_cleaned = df.select(*columns).na.drop(how=\"any\")\n",
    "\n",
    "        for i in grouped:\n",
    "            for j in enumerate(i):\n",
    "                i[j[0]] = i[j[0]] + (df_cleaned.corr(str(j[1][0]), str(j[1][1])),)\n",
    "\n",
    "        df_pandas = pd.DataFrame(grouped).applymap(lambda x: x[2])\n",
    "        df_pandas.columns = columns\n",
    "        df_pandas.index = columns\n",
    "        \n",
    "        return df_pandas\n",
    "\n",
    "    # Compute histogram \n",
    "    def create_hist_data(df, column, minim, maxim, bins=10):\n",
    "\n",
    "        def create_all_conditions(current_col, column, left_edges, count=1):\n",
    "            \"\"\"\n",
    "            Recursive function that exploits the\n",
    "            ability to call the Spark SQL Column method\n",
    "            .when() in a recursive way.\n",
    "            \"\"\"\n",
    "            left_edges = left_edges[:]\n",
    "            if len(left_edges) == 0:\n",
    "                return current_col\n",
    "            if len(left_edges) == 1:\n",
    "                next_col = current_col.when(col(column) >= float(left_edges[0]), count)\n",
    "                left_edges.pop(0)\n",
    "                return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "            next_col = current_col.when((float(left_edges[0]) <= col(column))\n",
    "                                        & (col(column) < float(left_edges[1])), count)\n",
    "            left_edges.pop(0)\n",
    "            return create_all_conditions(next_col, column, left_edges[:], count+1)\n",
    "\n",
    "        num_range = maxim - minim\n",
    "        bin_width = num_range / float(bins)\n",
    "        left_edges = [minim]\n",
    "        for _bin in range(bins):\n",
    "            left_edges = left_edges + [left_edges[-1] + bin_width]\n",
    "        left_edges.pop()\n",
    "        expression_col = when((float(left_edges[0]) <= col(column))\n",
    "                              & (col(column) < float(left_edges[1])), 0)\n",
    "        left_edges_copy = left_edges[:]\n",
    "        left_edges_copy.pop(0)\n",
    "        bin_data = (df.select(col(column))\n",
    "                    .na.drop()\n",
    "                    .select(col(column),\n",
    "                            create_all_conditions(expression_col,\n",
    "                                                  column,\n",
    "                                                  left_edges_copy\n",
    "                                                 ).alias(\"bin_id\")\n",
    "                           )\n",
    "                    .groupBy(\"bin_id\").count()\n",
    "                   ).toPandas()\n",
    "\n",
    "        # If no data goes into one bin, it won't \n",
    "        # appear in bin_data; so we should fill\n",
    "        # in the blanks:\n",
    "        bin_data.index = bin_data[\"bin_id\"]\n",
    "        new_index = list(range(bins))\n",
    "        bin_data = bin_data.reindex(new_index)\n",
    "        bin_data[\"bin_id\"] = bin_data.index\n",
    "        bin_data = bin_data.fillna(0)\n",
    "\n",
    "        bin_data[\"left_edge\"] = left_edges\n",
    "        bin_data[\"width\"] = bin_width\n",
    "        \n",
    "\n",
    "        return bin_data\n",
    "\n",
    "\n",
    "    def describe_integer_1d(df, column, current_result, nrows):\n",
    "        \n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_float_1d(df, column, current_result, nrows):\n",
    "        stats_df = df.select(column).na.drop().agg(mean(col(column)).alias(\"mean\"),\n",
    "                                                       df_min(col(column)).alias(\"min\"),\n",
    "                                                       df_max(col(column)).alias(\"max\"),\n",
    "                                                       variance(col(column)).alias(\"variance\"),\n",
    "                                                       kurtosis(col(column)).alias(\"kurtosis\"),\n",
    "                                                       stddev(col(column)).alias(\"std\"),\n",
    "                                                       skewness(col(column)).alias(\"skewness\"),\n",
    "                                                       df_sum(col(column)).alias(\"sum\")\n",
    "                                                       ).toPandas()\n",
    "\n",
    "        for x in np.array([0.05, 0.25, 0.5, 0.75, 0.95]):\n",
    "            stats_df[pretty_name(x)] = (df.select(column)\n",
    "                                        .na.drop()\n",
    "                                        .selectExpr(\"percentile_approx(`{col}`,CAST({n} AS DOUBLE))\"\n",
    "                                                    .format(col=column, n=x)).toPandas().iloc[:,0]\n",
    "                                        )\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "        stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"iqr\"] = stats[pretty_name(0.75)] - stats[pretty_name(0.25)]\n",
    "        stats[\"cv\"] = stats[\"std\"] / float(stats[\"mean\"])\n",
    "        stats[\"mad\"] = (df.select(column)\n",
    "                        .na.drop()\n",
    "                        .select(df_abs(col(column)-stats[\"mean\"]).alias(\"delta\"))\n",
    "                        .agg(df_sum(col(\"delta\"))).toPandas().iloc[0,0] / float(current_result[\"count\"]))\n",
    "        stats[\"type\"] = \"NUM\"\n",
    "        stats['n_zeros'] = df.select(column).where(col(column)==0.0).count()\n",
    "        stats['p_zeros'] = stats['n_zeros'] / float(nrows)\n",
    "\n",
    "        hist_data = create_hist_data(df, column, stats[\"min\"], stats[\"max\"], bins)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def describe_date_1d(df, column):\n",
    "        stats_df = df.select(column).na.drop().agg(df_min(col(column)).alias(\"min\"),\n",
    "                                                   df_max(col(column)).alias(\"max\")\n",
    "                                                  ).toPandas()\n",
    "        stats = stats_df.iloc[0].copy()\n",
    "        stats.name = column\n",
    "\n",
    "        if isinstance(stats[\"max\"], pd.Timestamp):\n",
    "            stats = stats.astype(object)\n",
    "            stats[\"max\"] = str(stats[\"max\"].to_pydatetime())\n",
    "            stats[\"min\"] = str(stats[\"min\"].to_pydatetime())\n",
    "\n",
    "        else:\n",
    "            stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "        stats[\"type\"] = \"DATE\"\n",
    "        return stats\n",
    "\n",
    "    def guess_json_type(string_value):\n",
    "        try:\n",
    "            obj = json.loads(string_value)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        return type(obj)\n",
    "\n",
    "    def describe_categorical_1d(df, column):\n",
    "        value_counts = (df.select(column).na.drop()\n",
    "                        .groupBy(column)\n",
    "                        .agg(count(col(column)))\n",
    "                        .orderBy(\"count({c})\".format(c=column),ascending=False)\n",
    "                       ).cache()\n",
    "\n",
    "        # Get the most frequent class:\n",
    "        stats = (value_counts\n",
    "                 .limit(1)\n",
    "                 .withColumnRenamed(column, \"top\")\n",
    "                 .withColumnRenamed(\"count({c})\".format(c=column), \"freq\")\n",
    "                ).toPandas().iloc[0]\n",
    "\n",
    "        # Get the top 50 classes by value count,\n",
    "        # and put the rest of them grouped at the\n",
    "        # end of the Series:\n",
    "        top_50 = value_counts.limit(50).toPandas().sort_values(\"count({c})\".format(c=column),\n",
    "                                                               ascending=False)\n",
    "        top_50_categories = top_50[column].values.tolist()\n",
    "\n",
    "        others_count = pd.Series([df.select(column).na.drop()\n",
    "                        .where(~(col(column).isin(*top_50_categories)))\n",
    "                        .count()\n",
    "                        ], index=[\"***Other Values***\"])\n",
    "        others_distinct_count = pd.Series([value_counts\n",
    "                                .where(~(col(column).isin(*top_50_categories)))\n",
    "                                .count()\n",
    "                                ], index=[\"***Other Values Distinct Count***\"])\n",
    "\n",
    "        top = top_50.set_index(column)[\"count({c})\".format(c=column)]\n",
    "        top = top.append(others_count)\n",
    "        top = top.append(others_distinct_count)\n",
    "        stats[\"value_counts\"] = top\n",
    "        stats[\"type\"] = \"CAT\"\n",
    "        value_counts.unpersist()\n",
    "        unparsed_valid_jsons = df.select(column).na.drop().rdd.map(\n",
    "            lambda x: guess_json_type(x[column])).filter(\n",
    "            lambda x: x).distinct().collect()\n",
    "        stats[\"unparsed_json_types\"] = unparsed_valid_jsons\n",
    "        return stats\n",
    "\n",
    "    def describe_constant_1d(df, column):\n",
    "        stats = pd.Series(['CONST'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(1)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_unique_1d(df, column):\n",
    "        stats = pd.Series(['UNIQUE'], index=['type'], name=column)\n",
    "        stats[\"value_counts\"] = (df.select(column)\n",
    "                                 .na.drop()\n",
    "                                 .limit(50)).toPandas().iloc[:,0].value_counts()\n",
    "        return stats\n",
    "\n",
    "    def describe_1d(df, column, nrows, lookup_config=None):\n",
    "        column_type = df.select(column).dtypes[0][1]\n",
    "        if (\"array\" in column_type) or (\"stuct\" in column_type) or (\"map\" in column_type):\n",
    "            raise NotImplementedError(\"Column {c} is of type {t} and cannot be analyzed\".format(c=column, t=column_type))\n",
    "\n",
    "        distinct_count = df.select(column).agg(countDistinct(col(column)).alias(\"distinct_count\")).toPandas()\n",
    "        non_nan_count = df.select(column).na.drop().select(count(col(column)).alias(\"count\")).toPandas()\n",
    "        results_data = pd.concat([distinct_count, non_nan_count],axis=1)\n",
    "        results_data[\"p_unique\"] = results_data[\"distinct_count\"] / float(results_data[\"count\"])\n",
    "        results_data[\"is_unique\"] = results_data[\"distinct_count\"] == nrows\n",
    "        results_data[\"n_missing\"] = nrows - results_data[\"count\"]\n",
    "        results_data[\"p_missing\"] = results_data[\"n_missing\"] / float(nrows)\n",
    "        results_data[\"p_infinite\"] = 0\n",
    "        results_data[\"n_infinite\"] = 0\n",
    "        result = results_data.iloc[0].copy()\n",
    "        result[\"memorysize\"] = 0\n",
    "        result.name = column\n",
    "\n",
    "        if result[\"distinct_count\"] <= 1:\n",
    "            result = result.append(describe_constant_1d(df, column))\n",
    "        elif column_type in {\"tinyint\", \"smallint\", \"int\", \"bigint\"}:\n",
    "            result = result.append(describe_integer_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"float\", \"double\", \"decimal\"}:\n",
    "            result = result.append(describe_float_1d(df, column, result, nrows))\n",
    "        elif column_type in {\"date\", \"timestamp\"}:\n",
    "            result = result.append(describe_date_1d(df, column))\n",
    "        elif result[\"is_unique\"] == True:\n",
    "            result = result.append(describe_unique_1d(df, column))\n",
    "        else:\n",
    "            result = result.append(describe_categorical_1d(df, column))\n",
    "            # Fix to also count MISSING value in the distict_count field:\n",
    "            if result[\"n_missing\"] > 0:\n",
    "                result[\"distinct_count\"] = result[\"distinct_count\"] + 1\n",
    "\n",
    "        if (result[\"count\"] > result[\"distinct_count\"] > 1):\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"top\"]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "        else:\n",
    "            try:\n",
    "                result[\"mode\"] = result[\"value_counts\"].index[0]\n",
    "            except KeyError:\n",
    "                result[\"mode\"] = 0\n",
    "            # If and IndexError happens,\n",
    "            # it is because all column are NULLs:\n",
    "            except IndexError:\n",
    "                result[\"mode\"] = \"MISSING\"\n",
    "\n",
    "        if lookup_config:\n",
    "            lookup_object = lookup_config['object']\n",
    "            col_name_in_db = lookup_config['col_name_in_db'] if 'col_name_in_db' in lookup_config else None\n",
    "            try:\n",
    "                matched, unmatched = lookup_object.lookup(df.select(column), col_name_in_db)\n",
    "                result['lookedup_values'] = str(matched.count()) + \"/\" + str(df.select(column).count())\n",
    "            except:\n",
    "                result['lookedup_values'] = 'FAILED'\n",
    "        else:\n",
    "            result['lookedup_values'] = ''\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    # build final report:\n",
    "    ldesc = {}\n",
    "    for colum in df.columns:\n",
    "        if colum in config:\n",
    "            if 'lookup' in config[colum]:\n",
    "                lookup_config = config[colum]['lookup']\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"], lookup_config=lookup_config)\n",
    "            else:\n",
    "                desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        else:\n",
    "            desc = describe_1d(df, colum, table_stats[\"n\"])\n",
    "        ldesc.update({colum: desc})\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    if corr_reject is not None:\n",
    "        computable_corrs = [colum for colum in ldesc if ldesc[colum][\"type\"] in {\"NUM\"}]\n",
    "\n",
    "        if len(computable_corrs) > 0:\n",
    "            corr = corr_matrix(df, columns=computable_corrs)\n",
    "            for x, corr_x in corr.iterrows():\n",
    "                for y, corr in corr_x.iteritems():\n",
    "                    if x == y:\n",
    "                        break\n",
    "\n",
    "    # Convert ldesc (final report) to a DataFrame\n",
    "    variable_stats = pd.DataFrame(ldesc)\n",
    "\n",
    "    # General statistics\n",
    "    table_stats[\"nvar\"] = len(df.columns)\n",
    "    table_stats[\"total_missing\"] = float(variable_stats.loc[\"n_missing\"].sum()) / (table_stats[\"n\"] * table_stats[\"nvar\"])\n",
    "    memsize = 0\n",
    "    table_stats['memsize'] = fmt_bytesize(memsize)\n",
    "    table_stats['recordsize'] = fmt_bytesize(memsize / table_stats['n'])\n",
    "    table_stats.update({k: 0 for k in (\"NUM\", \"DATE\", \"CONST\", \"CAT\", \"UNIQUE\", \"CORR\")})\n",
    "    table_stats.update(dict(variable_stats.loc['type'].value_counts()))\n",
    "    table_stats['REJECTED'] = table_stats['CONST'] + table_stats['CORR']\n",
    "\n",
    "    freq_dict = {}\n",
    "    for var in variable_stats:\n",
    "        if \"value_counts\" not in variable_stats[var]:\n",
    "            pass\n",
    "        elif not(variable_stats[var][\"value_counts\"] is np.nan):\n",
    "            freq_dict[var] = variable_stats[var][\"value_counts\"]\n",
    "        else:\n",
    "            pass\n",
    "    try:\n",
    "        variable_stats = variable_stats.drop(\"value_counts\")\n",
    "    except (ValueError, KeyError):\n",
    "        pass\n",
    "\n",
    "    return table_stats, variable_stats.T, freq_dict\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import abs as absou\n",
    "\n",
    "SKEWNESS_CUTOFF = 20\n",
    "DEFAULT_FLOAT_FORMATTER = u'spark_df_profiling.__default_float_formatter'\n",
    "\n",
    "# formmating functions\n",
    "def gradient_format(value, limit1, limit2, c1, c2):\n",
    "    def LerpColour(c1,c2,t):\n",
    "        return (int(c1[0]+(c2[0]-c1[0])*t),int(c1[1]+(c2[1]-c1[1])*t),int(c1[2]+(c2[2]-c1[2])*t))\n",
    "    c = LerpColour(c1, c2, (value-limit1)/(limit2-limit1))\n",
    "    return fmt_color(value,\"rgb{}\".format(str(c)))\n",
    "\n",
    "\n",
    "def fmt_color(text, color):\n",
    "    return(u'<span style=\"color:{color}\">{text}</span>'.format(color=color,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_class(text, cls):\n",
    "    return(u'<span class=\"{cls}\">{text}</span>'.format(cls=cls,text=str(text)))\n",
    "\n",
    "\n",
    "def fmt_bytesize(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if num < 0:\n",
    "            num = num*-1\n",
    "            if num < 1024.0:\n",
    "                return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "            num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def fmt_percent(v):\n",
    "    return  \"{:2.1f}%\".format(v*100)\n",
    "\n",
    "def fmt_varname(v):\n",
    "    return u'<code>{0}</code>'.format(v)\n",
    "\n",
    "\n",
    "value_formatters={\n",
    "        u'freq': (lambda v: gradient_format(v, 0, 62000, (30, 198, 244), (99, 200, 72))),\n",
    "        u'p_missing': fmt_percent,\n",
    "        u'p_infinite': fmt_percent,\n",
    "        u'p_unique': fmt_percent,\n",
    "        u'p_zeros': fmt_percent,\n",
    "        u'memorysize': fmt_bytesize,\n",
    "        u'total_missing': fmt_percent,\n",
    "        DEFAULT_FLOAT_FORMATTER: lambda v: str(float('{:.5g}'.format(v))).rstrip('0').rstrip('.'),\n",
    "        u'correlation_var': lambda v: fmt_varname(v),\n",
    "        u'unparsed_json_types': lambda v: ', '.join([s.__name__ for s in v])\n",
    "        }\n",
    "\n",
    "def fmt_row_severity(v):\n",
    "    if np.isnan(v) or v<= 0.01:\n",
    "        return \"ignore\"\n",
    "    else:\n",
    "        return \"alert\"\n",
    "\n",
    "def fmt_skewness(v):\n",
    "    if not np.isnan(v) and (v<-SKEWNESS_CUTOFF or v> SKEWNESS_CUTOFF):\n",
    "        return \"alert\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "row_formatters={\n",
    "    u'p_zeros': fmt_row_severity,\n",
    "    u'p_missing': fmt_row_severity,\n",
    "    u'p_infinite': fmt_row_severity,\n",
    "    u'n_duplicates': fmt_row_severity,\n",
    "    u'skewness': fmt_skewness,\n",
    "}\n",
    "\n",
    "## Set Spark Describe Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_spark(context: MLClientCtx, \n",
    "                   dataset: DataItem,\n",
    "                   bins: int=30,\n",
    "                   describe_extended: bool=True)-> None:\n",
    "    \"\"\"\n",
    "    Generates profile reports from an Apache Spark DataFrame. \n",
    "    Based on pandas_profiling, but for Spark's DataFrames instead of pandas.\n",
    "    For each column the following statistics - if relevant for the column type - are presented:\n",
    "    \n",
    "    Essentials: type, unique values, missing values\n",
    "    \n",
    "    Quantile statistics: minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "    \n",
    "    Descriptive statistics: mean, mode, standard deviation, sum, median absolute deviation, \n",
    "                            coefficient of variation, kurtosis, skewness\n",
    "                            \n",
    "    Most frequent values: for categorical data \n",
    "    --------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "                context : MLClientCtx\n",
    "                          MLRun introduces a concept of a runtime \"context\", \n",
    "                          the code can be set up to get parameters and inputs from the context, \n",
    "                          as well as log run outputs, artifacts, tags, and time-series metrics in the context.\n",
    "                                      \n",
    "                dataset : csv_file\n",
    "                          csv file which needs to be local (on our machine)\n",
    "                          the default location will be \"/v3io/projects/<file_name> \n",
    "                          which can be change by using mlrun.mount_v3io later in the function specs\n",
    "                          \n",
    "                bins :    Integer\n",
    "                          Number of bin in histograms\n",
    "                          \n",
    "                describe_extended : Bool \n",
    "                         (True) set to False if the aim is to get a simple \n",
    "                         pandas.DataFrame.describe() like infomration\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Examples: \n",
    "               run mlrun function example, inputs will be part of the function inputs.\n",
    "               artifact_path is part of mlrun function parameters which set the path \n",
    "               for logging artifacts, results, dataset, etc.\n",
    "               \n",
    "               function.run(inputs={\"dataset\": \"iris.csv\",\n",
    "                                    \"bins\": 30,\n",
    "                                    \"describe_extended\": True},\n",
    "                                     artifact_path=artifact_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file location\n",
    "    location = dataset.local()\n",
    "    \n",
    "    # build spark session\n",
    "    spark = SparkSession.builder.appName(\"Spark job\").config(\"spark.executor.memory\",\"6g\").getOrCreate()\n",
    "    \n",
    "    # read csv\n",
    "    df = spark.read.csv(location, header=True, inferSchema= True)\n",
    "\n",
    "    # No use for now\n",
    "    kwargs = []\n",
    "    \n",
    "    # take only numric column\n",
    "    float_cols = [item[0] for item in df.dtypes if item[1].startswith('float') or item[1].startswith('double')]\n",
    "    \n",
    "    if describe_extended == True:\n",
    "        \n",
    "        # run describe function\n",
    "        table, variables, freq = describe(df, bins, float_cols, kwargs)\n",
    "\n",
    "        # get summary table\n",
    "        tbl_1 = variables.reset_index()\n",
    "\n",
    "        # prep report \n",
    "        if len(freq) != 0:\n",
    "            tbl_2 = pd.DataFrame.from_dict(freq, orient = \"index\").sort_index().stack().reset_index()\n",
    "            tbl_2.columns = ['col', 'key', 'val']\n",
    "            tbl_2['Merged'] = [{key: val} for key, val in zip(tbl_2.key, tbl_2.val)]\n",
    "            tbl_2 = tbl_2.groupby('col', as_index=False).agg(lambda x: tuple(x))[['col','Merged']]\n",
    "\n",
    "            # get summary\n",
    "            summary = pd.merge(tbl_1, tbl_2, how='left', left_on='index', right_on='col')\n",
    "\n",
    "        else:\n",
    "            summary = tbl_1\n",
    "\n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "\n",
    "        # log overview\n",
    "        context.log_results(table)\n",
    "    \n",
    "    else:\n",
    "        # run simple describe and save to pandas\n",
    "        tbl_1 = df.describe().toPandas()\n",
    "        \n",
    "        # save final report and transpose \n",
    "        summary = tbl_1.T\n",
    "        \n",
    "        # log final report\n",
    "        context.log_dataset(\"summary_stats\", \n",
    "                            df=summary,\n",
    "                            format=\"csv\", index=False,\n",
    "                            artifact_path=context.artifact_subpath('data'))\n",
    "    \n",
    "    # stop spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iris_dataset.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "def download_file(url,path):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    \n",
    "    #file_path = path+\"/\"+local_filename\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(\"/v3io/projects/\"+local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "url = \"https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv\"\n",
    "\n",
    "download_file(url,'/v3io/projects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please don't remove the # nuclio: end-code cell above\n",
    "### Set MLRun Function Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark/conf/spark-defaults.conf\n",
      "spark\n"
     ]
    }
   ],
   "source": [
    "#get spark service name\n",
    "from configparser import ConfigParser\n",
    "from itertools import chain\n",
    "\n",
    "parser = ConfigParser()\n",
    "configFilePath = os.environ['SPARK_HOME']+'/conf/spark-defaults.conf'\n",
    "print(configFilePath)\n",
    "with open(configFilePath) as lines:\n",
    "    lines = chain((\"[top]\",), lines)  # This line does the trick.\n",
    "    parser.read_file(lines)\n",
    "    spark_service_name = parser[\"top\"][\"spark.master\"].split(\"://\")[1].split(\"-master\")[0]   \n",
    "print(spark_service_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun will transform the code above (up to nuclio: end-code cell) into serverless function \n",
    "# which will run in k8s pods\n",
    "fn = mlrun.code_to_function(handler=\"describe_spark\", kind=\"remote-spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-24 12:29:23,597 [info] Started building image: .mlrun/func-default-spark-mlrun-describe:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 from registry datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80 \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Building stage 'datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021' [idx: '0', base-idx: '-1'] \n",
      "\u001b[36mINFO\u001b[0m[0000] Unpacking rootfs as cmd RUN pip install matplotlib pyspark requires it. \n",
      "\u001b[36mINFO\u001b[0m[0066] RUN pip install matplotlib pyspark           \n",
      "\u001b[36mINFO\u001b[0m[0066] Initializing snapshotter ...                 \n",
      "\u001b[36mINFO\u001b[0m[0066] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0109] Cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0109] Args: [-c pip install matplotlib pyspark]    \n",
      "\u001b[36mINFO\u001b[0m[0109] Util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0109] Performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0109] Running: [/bin/sh -c pip install matplotlib pyspark] \n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 29.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyspark in ./python (3.2.1)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting numpy>=1.19\n",
      "  Downloading numpy-1.24.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 28.3 MB/s eta 0:00:00\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 43.5 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 kB 51.7 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 35.9 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.7/299.7 kB 55.5 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 57.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 52.4 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 18.5 MB/s eta 0:00:00\n",
      "Collecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.0/199.0 kB 54.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in /conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: py4j, python-dateutil, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.3 numpy-1.24.1 packaging-23.0 pillow-9.4.0 py4j-0.10.9.3 pyparsing-3.0.9 python-dateutil-2.8.2\n",
      "\u001b[36mINFO\u001b[0m[0119] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0128] RUN python -m pip install \"mlrun[complete]==1.3.0-rc8\" \n",
      "\u001b[36mINFO\u001b[0m[0128] Cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0128] Args: [-c python -m pip install \"mlrun[complete]==1.3.0-rc8\"] \n",
      "\u001b[36mINFO\u001b[0m[0128] Util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0128] Performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0128] Running: [/bin/sh -c python -m pip install \"mlrun[complete]==1.3.0-rc8\"] \n",
      "Collecting mlrun[complete]==1.3.0-rc8\n",
      "  Downloading mlrun-1.3.0rc8-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting aiohttp~=3.8\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 51.0 MB/s eta 0:00:00\n",
      "Collecting deepdiff~=5.0\n",
      "  Downloading deepdiff-5.8.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.5/69.5 kB 28.0 MB/s eta 0:00:00\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.10.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 30.1 MB/s eta 0:00:00\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Collecting GitPython~=3.0\n",
      "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 kB 49.5 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.13\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 39.3 MB/s eta 0:00:00\n",
      "Collecting aiohttp-retry~=2.8\n",
      "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting python-dotenv~=0.17.0\n",
      "  Downloading python_dotenv-0.17.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (1.26.13)\n",
      "Collecting pandas<1.5.0,~=1.2\n",
      "  Downloading pandas-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 28.4 MB/s eta 0:00:00\n",
      "Collecting click~=8.0.0\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.5/97.5 kB 34.3 MB/s eta 0:00:00\n",
      "Collecting storey~=1.3.6\n",
      "  Downloading storey-1.3.7-py3-none-any.whl (156 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.1/156.1 kB 42.1 MB/s eta 0:00:00\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.4.46-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 20.3 MB/s eta 0:00:00\n",
      "Collecting orjson~=3.3\n",
      "  Downloading orjson-3.8.5-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 kB 35.4 MB/s eta 0:00:00\n",
      "Collecting inflection~=0.5.0\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting pyarrow<7,>=3\n",
      "  Downloading pyarrow-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 14.8 MB/s eta 0:00:00\n",
      "Collecting fsspec~=2021.8.1\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.3/119.3 kB 35.7 MB/s eta 0:00:00\n",
      "Collecting dask~=2021.11.2\n",
      "  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 29.8 MB/s eta 0:00:00\n",
      "Collecting ipython~=7.0\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 kB 31.6 MB/s eta 0:00:00\n",
      "Collecting chardet<4.0,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 kB 37.9 MB/s eta 0:00:00\n",
      "Collecting v3io-frames~=0.10.2\n",
      "  Downloading v3io_frames-0.10.2-py3-none-any.whl (35 kB)\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting tabulate~=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting v3iofs~=0.1.15\n",
      "  Downloading v3iofs-0.1.15-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cryptography<3.4,~=3.0 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (3.3.2)\n",
      "Requirement already satisfied: requests~=2.22 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (2.28.1)\n",
      "Collecting kubernetes~=12.0\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 27.5 MB/s eta 0:00:00\n",
      "Collecting fastapi~=0.88.0\n",
      "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 23.4 MB/s eta 0:00:00\n",
      "Collecting alembic~=1.9\n",
      "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.6/210.6 kB 36.2 MB/s eta 0:00:00\n",
      "Collecting distributed~=2021.11.2\n",
      "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 802.2/802.2 kB 32.9 MB/s eta 0:00:00\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 630.1/630.1 kB 19.2 MB/s eta 0:00:00\n",
      "Collecting pymysql~=1.0\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 18.3 MB/s eta 0:00:00\n",
      "Collecting kfp<1.8.14,~=1.8.0\n",
      "  Downloading kfp-1.8.13.tar.gz (300 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.1/300.1 kB 36.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting numpy<1.23.0,>=1.16.5\n",
      "  Downloading numpy-1.22.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 22.7 MB/s eta 0:00:00\n",
      "Collecting nuclio-jupyter~=0.9.6\n",
      "  Downloading nuclio_jupyter-0.9.6-py3-none-any.whl (51 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.1/51.1 kB 19.7 MB/s eta 0:00:00\n",
      "Collecting humanfriendly~=8.2\n",
      "  Downloading humanfriendly-8.2-py2.py3-none-any.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.3/86.3 kB 31.6 MB/s eta 0:00:00\n",
      "Collecting v3io~=0.5.20\n",
      "  Downloading v3io-0.5.20-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.2/64.2 kB 23.1 MB/s eta 0:00:00\n",
      "Collecting semver~=2.13\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting azure-storage-blob~=12.13\n",
      "  Downloading azure_storage_blob-12.14.1-py3-none-any.whl (383 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 383.2/383.2 kB 35.1 MB/s eta 0:00:00\n",
      "Collecting s3fs~=2021.8.1\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.2-py2.py3-none-any.whl (215 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.1/215.1 kB 42.8 MB/s eta 0:00:00\n",
      "Collecting msrest~=0.6.21\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.2/85.2 kB 32.5 MB/s eta 0:00:00\n",
      "Collecting azure-core~=1.24\n",
      "  Downloading azure_core-1.26.2-py3-none-any.whl (173 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.8/173.8 kB 44.1 MB/s eta 0:00:00\n",
      "Collecting plotly~=5.4\n",
      "  Downloading plotly-5.13.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 23.9 MB/s eta 0:00:00\n",
      "Collecting adlfs~=2021.8.1\n",
      "  Downloading adlfs-2021.8.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting redis~=4.3\n",
      "  Downloading redis-4.4.2-py3-none-any.whl (237 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 237.8/237.8 kB 40.8 MB/s eta 0:00:00\n",
      "Collecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 9.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gcsfs~=2021.8.1\n",
      "  Downloading gcsfs-2021.8.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting kafka-python~=2.0\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 246.5/246.5 kB 39.6 MB/s eta 0:00:00\n",
      "Collecting azure-keyvault-secrets~=4.2\n",
      "  Downloading azure_keyvault_secrets-4.6.0-py3-none-any.whl (291 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 291.6/291.6 kB 38.8 MB/s eta 0:00:00\n",
      "Collecting botocore<1.20.107,>=1.20.106\n",
      "  Downloading botocore-1.20.106-py2.py3-none-any.whl (7.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 27.3 MB/s eta 0:00:00\n",
      "Collecting boto3<1.17.107,~=1.9\n",
      "  Downloading boto3-1.17.106-py2.py3-none-any.whl (131 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.6/131.6 kB 42.1 MB/s eta 0:00:00\n",
      "Collecting azure-identity~=1.5\n",
      "  Downloading azure_identity-1.12.0-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 kB 44.0 MB/s eta 0:00:00\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46\n",
      "  Downloading azure_datalake_store-0.0.52-py2.py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.7/61.7 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.8/77.8 kB 29.0 MB/s eta 0:00:00\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 23.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 32.4 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 kB 31.3 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 42.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /conda/lib/python3.9/site-packages (from aiohttp~=3.8->mlrun[complete]==1.3.0-rc8) (2.0.4)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 31.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.11.0 in /conda/lib/python3.9/site-packages (from azure-core~=1.24->mlrun[complete]==1.3.0-rc8) (1.16.0)\n",
      "Collecting typing-extensions>=4.0.1\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting msal<2.0.0,>=1.12.0\n",
      "  Downloading msal-1.20.0-py2.py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 34.9 MB/s eta 0:00:00\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting azure-common~=1.1\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting azure-storage-blob~=12.13\n",
      "  Downloading azure_storage_blob-12.14.0-py3-none-any.whl (383 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 383.2/383.2 kB 36.3 MB/s eta 0:00:00\n",
      "  Downloading azure_storage_blob-12.13.1-py3-none-any.whl (377 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 377.4/377.4 kB 35.9 MB/s eta 0:00:00\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.2/79.2 kB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /conda/lib/python3.9/site-packages (from botocore<1.20.107,>=1.20.106->mlrun[complete]==1.3.0-rc8) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /conda/lib/python3.9/site-packages (from cryptography<3.4,~=3.0->mlrun[complete]==1.3.0-rc8) (1.14.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /conda/lib/python3.9/site-packages (from dask~=2021.11.2->mlrun[complete]==1.3.0-rc8) (23.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /conda/lib/python3.9/site-packages (from dask~=2021.11.2->mlrun[complete]==1.3.0-rc8) (0.12.0)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting tornado>=6.0.3\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 424.0/424.0 kB 36.2 MB/s eta 0:00:00\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 322.4/322.4 kB 36.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /conda/lib/python3.9/site-packages (from distributed~=2021.11.2->mlrun[complete]==1.3.0-rc8) (65.6.3)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 42.0 MB/s eta 0:00:00\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 kB 42.7 MB/s eta 0:00:00\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting starlette==0.22.0\n",
      "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 26.8 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 31.9 MB/s eta 0:00:00\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 kB 45.8 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 28.3 MB/s eta 0:00:00\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 40.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 29.1 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 27.6 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 16.7 MB/s eta 0:00:00\n",
      "Collecting db-dtypes<2.0.0dev,>=0.3.0\n",
      "  Downloading db_dtypes-1.0.5-py2.py3-none-any.whl (14 kB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 34.9 MB/s eta 0:00:00\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.4/386.4 kB 30.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib-inline\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.8/116.8 kB 37.5 MB/s eta 0:00:00\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting pexpect>4.3\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.0/59.0 kB 22.4 MB/s eta 0:00:00\n",
      "Collecting jedi>=0.16\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 28.4 MB/s eta 0:00:00\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting absl-py<2,>=0.9\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 43.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 37.1 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 25.3 MB/s eta 0:00:00\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 43.3 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 21.9 MB/s eta 0:00:00\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 17.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 22.7 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 34.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 23.0 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /conda/lib/python3.9/site-packages (from kubernetes~=12.0->mlrun[complete]==1.3.0-rc8) (2022.12.7)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 17.4 MB/s eta 0:00:00\n",
      "Collecting notebook>=5.2.0\n",
      "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.1/439.1 kB 35.5 MB/s eta 0:00:00\n",
      "Collecting nbconvert>=6.4.5\n",
      "  Downloading nbconvert-7.2.8-py3-none-any.whl (274 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.8/274.8 kB 37.8 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 34.9 MB/s eta 0:00:00\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /conda/lib/python3.9/site-packages (from requests~=2.22->mlrun[complete]==1.3.0-rc8) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 535.9/535.9 kB 34.1 MB/s eta 0:00:00\n",
      "Collecting grpcio-tools<1.42,>1.34.0\n",
      "  Downloading grpcio_tools-1.41.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 31.1 MB/s eta 0:00:00\n",
      "Collecting xxhash>=1\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 40.9 MB/s eta 0:00:00\n",
      "Collecting storey~=1.3.6\n",
      "  Downloading storey-1.3.6-py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.9/155.9 kB 40.2 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of sqlalchemy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.4.45-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 28.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of semver to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests~=2.22\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 24.5 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of redis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting redis~=4.3\n",
      "  Downloading redis-4.4.1-py3-none-any.whl (237 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 237.6/237.6 kB 38.2 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.4-cp39-cp39-manylinux1_x86_64.whl (629 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 630.0/630.0 kB 35.0 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of python-dotenv to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dotenv~=0.17.0\n",
      "  Downloading python_dotenv-0.17.0-py2.py3-none-any.whl (18 kB)\n",
      "INFO: pip is looking at multiple versions of pymysql to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymysql~=1.0\n",
      "  Downloading PyMySQL-1.0.1-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 17.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 26.3 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<7,>=3\n",
      "  Downloading pyarrow-6.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 16.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<3.20,>=3.13\n",
      "  Downloading protobuf-3.19.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 35.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of plotly to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting plotly~=5.4\n",
      "  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 23.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas<1.5.0,~=1.2\n",
      "  Downloading pandas-1.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 28.1 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson~=3.3\n",
      "  Downloading orjson-3.8.4-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 kB 42.3 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy<1.23.0,>=1.16.5\n",
      "  Downloading numpy-1.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 25.6 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of nuclio-jupyter to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nest-asyncio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of msrest to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of mergedeep to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.3-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of kubernetes to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kubernetes~=12.0\n",
      "  Downloading kubernetes-12.0.0-py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 29.6 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of kfp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kfp<1.8.14,~=1.8.0\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 38.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of kafka-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kafka-python~=2.0\n",
      "  Downloading kafka_python-2.0.1-py2.py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.2/232.2 kB 27.9 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of ipython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ipython~=7.0\n",
      "  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 kB 36.1 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of inflection to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting inflection~=0.5.0\n",
      "  Downloading inflection-0.5.0-py2.py3-none-any.whl (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of humanfriendly to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery[pandas] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.1/215.1 kB 42.8 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.0-py2.py3-none-any.whl (212 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 46.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.18.1-py2.py3-none-any.whl (189 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.8/189.8 kB 37.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.3.6-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 48.5 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.5-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 40.2 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.3-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 37.2 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.2-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 41.8 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.1-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 38.3 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery[pandas] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_bigquery-3.3.0-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 43.8 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.2.0-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.8/211.8 kB 43.6 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.41.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 28.5 MB/s eta 0:00:00\n",
      "Collecting future>=0.18.2\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 36.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ujson>=3\n",
      "  Downloading ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 221.6 kB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos>=1.5.3\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 kB 41.3 MB/s eta 0:00:00\n",
      "Collecting adal>=0.4.2\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 22.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pycparser in /conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography<3.4,~=3.0->mlrun[complete]==1.3.0-rc8) (2.21)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.6/115.6 kB 39.1 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.8/96.8 kB 33.2 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 45.9 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.18.0-py2.py3-none-any.whl (187 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.7/187.7 kB 46.5 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery_storage-2.17.0-py2.py3-none-any.whl (187 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.7/187.7 kB 38.8 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.4/185.4 kB 34.2 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting parso<0.9.0,>=0.8.0\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.8/100.8 kB 37.8 MB/s eta 0:00:00\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 24.3 MB/s eta 0:00:00\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting portalocker<3,>=1.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting markupsafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting jupyter-core>=4.7\n",
      "  Downloading jupyter_core-5.1.4-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.5/93.5 kB 36.6 MB/s eta 0:00:00\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 31.3 MB/s eta 0:00:00\n",
      "Collecting bleach\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 36.8 MB/s eta 0:00:00\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting importlib-metadata>=3.6\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting nbformat>=5.1\n",
      "  Downloading nbformat-5.7.3-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.1/78.1 kB 30.0 MB/s eta 0:00:00\n",
      "Collecting mistune<3,>=2.0.3\n",
      "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 26.8 MB/s eta 0:00:00\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.20.2-py3-none-any.whl (149 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.2/149.2 kB 40.9 MB/s eta 0:00:00\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.5/122.5 kB 41.1 MB/s eta 0:00:00\n",
      "Collecting nbclassic>=0.4.7\n",
      "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 28.5 MB/s eta 0:00:00\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-25.0.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 37.0 MB/s eta 0:00:00\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.5/133.5 kB 40.8 MB/s eta 0:00:00\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /conda/lib/python3.9/site-packages (from packaging>=20.0->dask~=2021.11.2->mlrun[complete]==1.3.0-rc8) (3.0.9)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 46.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in /conda/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp<1.8.14,~=1.8.0->mlrun[complete]==1.3.0-rc8) (0.38.4)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.7.1-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.7.0-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.5.3-py2.py3-none-any.whl (19 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.47.2-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.47.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.5-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.3-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.1-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.43.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.42.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.41.1-py3-none-any.whl (9.2 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting notebook-shim>=0.1.0\n",
      "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
      "Collecting jupyter-server>=1.8\n",
      "  Downloading jupyter_server-2.1.0-py3-none-any.whl (365 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.2/365.2 kB 34.4 MB/s eta 0:00:00\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 32.3 MB/s eta 0:00:00\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
      "Collecting types-cryptography>=3.3.21\n",
      "  Downloading types_cryptography-3.3.23.2-py3-none-any.whl (30 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 31.2 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting debugpy>=1.0\n",
      "  Downloading debugpy-1.6.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 11.1 MB/s eta 0:00:00\n",
      "Collecting comm>=0.1.1\n",
      "  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
      "Collecting jupyter-server-terminals\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyter-events>=0.4.0\n",
      "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-2.0.4-py3-none-any.whl (7.8 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting jsonschema[format-nongpl]>=3.2.0\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 31.7 MB/s eta 0:00:00\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting jsonschema[format-nongpl]>=3.2.0\n",
      "  Downloading jsonschema-4.17.1-py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.2/90.2 kB 33.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.17.0-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.8/83.8 kB 33.1 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.16.0-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 34.4 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.15.0-py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.7/82.7 kB 32.4 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.14.0-py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.4/82.4 kB 33.1 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.13.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.6/81.6 kB 32.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.12.1-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 30.2 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.12.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 24.2 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.11.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 29.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.3-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 15.8 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.2-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 32.9 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.1-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 34.0 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 30.9 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.9.1-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 31.4 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.9.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 32.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.8.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 32.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.2-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 31.7 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.1-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 34.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 30.4 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.2-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 32.0 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.1-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 32.2 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.4/80.4 kB 32.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.5.1-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 29.8 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.5.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.5.0-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 29.5 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.5.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 30.5 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.4.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.3-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 28.2 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.2-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.8/71.8 kB 31.9 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.2 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.1-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.7/71.7 kB 25.3 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.0-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 kB 13.7 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.4/69.4 kB 13.5 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.2.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.2.0-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.4/69.4 kB 28.2 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.2.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.2-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 26.6 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.2 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 23.7 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 28.6 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.0.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.1/69.1 kB 26.4 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.0.1 does not provide the extra 'format-nongpl'\n",
      "WARNING: jsonschema 3.2.0 does not provide the extra 'format-nongpl'\n",
      "Building wheels for collected packages: adlfs, aiobotocore, kfp, fire, future, kfp-server-api, strip-hints\n",
      "  Building wheel for adlfs (setup.py): started\n",
      "  Building wheel for adlfs (setup.py): finished with status 'done'\n",
      "  Created wheel for adlfs: filename=adlfs-2021.8.2-py3-none-any.whl size=21465 sha256=6e6e4e5c0eb6db75760150ececec7fddb83fa8839697ec4e2d28f0f8eb693762\n",
      "  Stored in directory: /.cache/pip/wheels/00/03/70/7dbccaa3fe1feac9884a8376949e729c73b2497eff79290674\n",
      "  Building wheel for aiobotocore (setup.py): started\n",
      "  Building wheel for aiobotocore (setup.py): finished with status 'done'\n",
      "  Created wheel for aiobotocore: filename=aiobotocore-1.4.2-py3-none-any.whl size=49909 sha256=c0a3f952682df75de81b232d28e424164fc8775d0b29672439bfdacc29a97d5d\n",
      "  Stored in directory: /.cache/pip/wheels/74/dc/76/2e9011d2d0191acf6dcb82a60d8f32c97340bc40c14c383e4a\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.13-py3-none-any.whl size=422423 sha256=5cbe1076614f40a5eb5f96fb262adf9fbe01498f62f9d10db946e9b8f5209888\n",
      "  Stored in directory: /.cache/pip/wheels/ba/0e/01/06569da8d3c237c7759be9f4070cef228d38cfe69fe77036f1\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=28ce2ba22e7fd9f71b380e68b3d3c622c78073de6675ed95b681f1b5f852252c\n",
      "  Stored in directory: /.cache/pip/wheels/34/a9/61/d515d3cd1e8a349fed305bc67a9c7d68fc38d51053b6decad6\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=0662d06044c1299e6e7ba7e003c2ab5bda6819a8061c9db6a1f2126b5736bb5a\n",
      "  Stored in directory: /.cache/pip/wheels/0c/ff/54/efb16da5b1058114a457b3c7167904915d2e5764b637ae8d3d\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=20a8161cf479287895de0b3d37d467d394fe4dfe87f2704b45b546ec4857cefc\n",
      "  Stored in directory: /.cache/pip/wheels/1d/5e/cc/d6c7bfba9ec05cf878694715074232539159539737f01bc680\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=ecb8b068727179845dc3a480df44e30b21a2fd74f87b22e6c83a0f2ea0030c06\n",
      "  Stored in directory: /.cache/pip/wheels/63/7d/5a/867fa96b7d29e9dc3f26ddffe45d960690603b434081da419d\n",
      "Successfully built adlfs aiobotocore kfp fire future kfp-server-api strip-hints\n",
      "Installing collected packages: webencodings, wcwidth, types-cryptography, sortedcontainers, Send2Trash, pytz, python-dotenv, pyasn1, ptyprocess, pickleshare, msgpack, mistune, kafka-python, ipython-genutils, heapdict, fastjsonschema, chardet, backcall, azure-common, zipp, zict, xxhash, wrapt, websocket-client, uritemplate, ujson, typing-extensions, traitlets, tornado, tinycss2, termcolor, tenacity, tblib, tabulate, strip-hints, soupsieve, sniffio, smmap, semver, rsa, rfc3986-validator, rfc3339-validator, pyzmq, pyyaml, python-json-logger, pyrsistent, pymysql, PyJWT, pygments, pyasn1-modules, psutil, protobuf, prompt-toolkit, prometheus-client, portalocker, platformdirs, pexpect, parso, pandocfilters, packaging, orjson, ordered-set, oauthlib, numpy, nest-asyncio, multidict, mergedeep, markupsafe, locket, jupyterlab-pygments, jmespath, isodate, inflection, humanfriendly, httplib2, grpcio, greenlet, google-crc32c, future, fsspec, frozenlist, entrypoints, docstring-parser, defusedxml, decorator, debugpy, cloudpickle, click, cachetools, bleach, attrs, async-timeout, absl-py, yarl, v3io, typer, terminado, sqlalchemy, requests-toolbelt, requests-oauthlib, redis, pydantic, pyarrow, proto-plus, plotly, partd, pandas, matplotlib-inline, Mako, kfp-server-api, kfp-pipeline-spec, jupyter-core, jsonschema, jinja2, jedi, importlib-metadata, grpcio-tools, googleapis-common-protos, google-resumable-media, google-auth, gitdb, fire, Deprecated, deepdiff, comm, botocore, beautifulsoup4, azure-core, argon2-cffi-bindings, anyio, aiosignal, aioitertools, v3iofs, v3io-frames, starlette, s3transfer, nbformat, msrest, kubernetes, jupyter-server-terminals, jupyter-client, ipython, grpcio-status, google-auth-oauthlib, google-auth-httplib2, google-api-core, GitPython, db-dtypes, dask, argon2-cffi, alembic, aiohttp, adal, storey, nbclient, msal, jupyter-events, ipykernel, google-cloud-core, google-api-python-client, gcsfs, fastapi, distributed, boto3, azure-storage-blob, azure-keyvault-secrets, azure-datalake-store, aiohttp-retry, aiobotocore, s3fs, nbconvert, msal-extensions, google-cloud-storage, google-cloud-bigquery-storage, kfp, jupyter-server, google-cloud-bigquery, azure-identity, notebook-shim, adlfs, nbclassic, notebook, nuclio-jupyter, mlrun\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "Successfully installed Deprecated-1.2.13 GitPython-3.1.30 Mako-1.2.4 PyJWT-2.5.0 Send2Trash-1.8.0 absl-py-1.4.0 adal-1.2.7 adlfs-2021.8.2 aiobotocore-1.4.2 aiohttp-3.8.3 aiohttp-retry-2.8.3 aioitertools-0.11.0 aiosignal-1.3.1 alembic-1.9.2 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.2 attrs-22.2.0 azure-common-1.1.28 azure-core-1.26.2 azure-datalake-store-0.0.52 azure-identity-1.12.0 azure-keyvault-secrets-4.6.0 azure-storage-blob-12.13.1 backcall-0.2.0 beautifulsoup4-4.11.1 bleach-6.0.0 boto3-1.17.106 botocore-1.20.106 cachetools-4.2.4 chardet-3.0.4 click-8.0.4 cloudpickle-2.2.1 comm-0.1.2 dask-2021.11.2 db-dtypes-1.0.5 debugpy-1.6.6 decorator-5.1.1 deepdiff-5.8.1 defusedxml-0.7.1 distributed-2021.11.2 docstring-parser-0.15 entrypoints-0.4 fastapi-0.88.0 fastjsonschema-2.16.2 fire-0.5.0 frozenlist-1.3.3 fsspec-2021.8.1 future-0.18.3 gcsfs-2021.8.1 gitdb-4.0.10 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.5.3 google-cloud-bigquery-3.2.0 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-cloud-storage-1.44.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 greenlet-2.0.1 grpcio-1.41.1 grpcio-status-1.41.1 grpcio-tools-1.41.1 heapdict-1.0.1 httplib2-0.21.0 humanfriendly-8.2 importlib-metadata-6.0.0 inflection-0.5.1 ipykernel-6.20.2 ipython-7.34.0 ipython-genutils-0.2.0 isodate-0.6.1 jedi-0.18.2 jinja2-3.1.2 jmespath-0.10.0 jsonschema-3.2.0 jupyter-client-7.4.9 jupyter-core-5.1.4 jupyter-events-0.6.3 jupyter-server-2.1.0 jupyter-server-terminals-0.4.4 jupyterlab-pygments-0.2.2 kafka-python-2.0.2 kfp-1.8.13 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-12.0.1 locket-1.0.0 markupsafe-2.1.2 matplotlib-inline-0.1.6 mergedeep-1.3.4 mistune-2.0.4 mlrun-1.3.0rc8 msal-1.20.0 msal-extensions-1.0.0 msgpack-1.0.4 msrest-0.6.21 multidict-6.0.4 nbclassic-0.4.8 nbclient-0.7.2 nbconvert-7.2.8 nbformat-5.7.3 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 nuclio-jupyter-0.9.6 numpy-1.22.4 oauthlib-3.2.2 ordered-set-4.1.0 orjson-3.8.5 packaging-21.3 pandas-1.4.4 pandocfilters-1.5.0 parso-0.8.3 partd-1.3.0 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-2.6.2 plotly-5.13.0 portalocker-2.7.0 prometheus-client-0.16.0 prompt-toolkit-3.0.36 proto-plus-1.22.2 protobuf-3.19.6 psutil-5.9.4 ptyprocess-0.7.0 pyarrow-6.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.10.4 pygments-2.14.0 pymysql-1.0.2 pyrsistent-0.19.3 python-dotenv-0.17.1 python-json-logger-2.0.4 pytz-2022.7.1 pyyaml-5.4.1 pyzmq-25.0.0 redis-4.4.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rsa-4.9 s3fs-2021.8.1 s3transfer-0.4.2 semver-2.13.0 smmap-5.0.0 sniffio-1.3.0 sortedcontainers-2.4.0 soupsieve-2.3.2.post1 sqlalchemy-1.4.46 starlette-0.22.0 storey-1.3.7 strip-hints-0.1.10 tabulate-0.8.10 tblib-1.7.0 tenacity-8.1.0 termcolor-2.2.0 terminado-0.17.1 tinycss2-1.2.1 tornado-6.2 traitlets-5.8.1 typer-0.7.0 types-cryptography-3.3.23.2 typing-extensions-4.4.0 ujson-5.7.0 uritemplate-3.0.1 v3io-0.5.20 v3io-frames-0.10.2 v3iofs-0.1.15 wcwidth-0.2.6 webencodings-0.5.1 websocket-client-1.4.2 wrapt-1.14.1 xxhash-3.2.0 yarl-1.8.2 zict-2.2.0 zipp-3.11.0\n",
      "\u001b[36mINFO\u001b[0m[0237] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0276] Pushing image to docker-registry.default-tenant.app.vmdev94.lab.iguazeng.com:80/mlrun/func-default-spark-mlrun-describe:latest \n",
      "\u001b[36mINFO\u001b[0m[0285] Pushed docker-registry.default-tenant.app.vmdev94.lab.iguazeng.com:80/mlrun/func-default-spark-mlrun-describe@sha256:11226fcab62ca1c45caa0b8020b7b793a3d49d7e7f1a904f7b0158c6528fd842 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.with_spark_service(spark_service=spark_service_name)\n",
    "\n",
    "fn.spec.build.commands = ['pip install matplotlib pyspark']\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLRun and Run Function\n",
    "Once running the function get be monitored here and our projects dashbaord<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set mlrun api path and arrtifact path for logging\n",
    "artifact_path = mlrun.set_environment(api_path = 'http://mlrun-api:8080',\n",
    "                                      artifact_path = os.path.abspath('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-24 12:34:30,256 [info] starting run spark-mlrun-describe-describe_spark uid=a134c40ac3ce434d8a6231b551d9f5b5 DB=http://mlrun-api:8080\n",
      "> 2023-01-24 12:34:30,435 [info] Job is running in the background, pod: spark-mlrun-describe-describe-spark-jzdzv\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2023-01-24 12:35:18,701 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "package mlrun installed\n",
      "package matplotlib installed\n",
      "> 2023-01-24 12:36:15,957 [info] To track results use the CLI: {'info_cmd': 'mlrun get run a134c40ac3ce434d8a6231b551d9f5b5 -p default', 'logs_cmd': 'mlrun logs a134c40ac3ce434d8a6231b551d9f5b5 -p default'}\n",
      "> 2023-01-24 12:36:15,957 [info] Or click for UI: {'ui_url': 'https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/default/jobs/monitor/a134c40ac3ce434d8a6231b551d9f5b5/overview'}\n",
      "> 2023-01-24 12:36:15,957 [info] run executed, status=completed\n",
      "final state: completed                                                          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"a134c40ac3ce434d8a6231b551d9f5b5\"><a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/default/jobs/monitor/a134c40ac3ce434d8a6231b551d9f5b5/overview\" target=\"_blank\" >...51d9f5b5</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jan 24 12:35:15</td>\n",
       "      <td>completed</td>\n",
       "      <td>spark-mlrun-describe-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=avia</div><div class=\"dictlist\">kind=remote-spark</div><div class=\"dictlist\">owner=avia</div><div class=\"dictlist\">mlrun/client_version=1.3.0-rc8</div><div class=\"dictlist\">host=spark-mlrun-describe-describe-spark-jzdzv</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result2b6cdf1f\" title=\"/files/aviaIguazio/demos/howto/spark/iris_dataset.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">n=150</div><div class=\"dictlist\">nvar=5</div><div class=\"dictlist\">total_missing=0.0</div><div class=\"dictlist\">memsize=0.0 YiB</div><div class=\"dictlist\">recordsize=0.0 YiB</div><div class=\"dictlist\">NUM=5</div><div class=\"dictlist\">DATE=0</div><div class=\"dictlist\">CONST=0</div><div class=\"dictlist\">CAT=0</div><div class=\"dictlist\">UNIQUE=0</div><div class=\"dictlist\">CORR=0</div><div class=\"dictlist\">REJECTED=0</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result2b6cdf1f\" title=\"/files/aviaIguazio/demos/howto/spark/data/spark-mlrun-describe-describe_spark/0/summary_stats.csv\">summary_stats</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result2b6cdf1f-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result2b6cdf1f-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result2b6cdf1f\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result2b6cdf1f-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/default/jobs/monitor/a134c40ac3ce434d8a6231b551d9f5b5/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-24 12:36:20,606 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "# run our functions with the relevant params\n",
    "run_res = fn.run(inputs={\"dataset\": \"iris_dataset.csv\"},\n",
    "                 artifact_path=artifact_path[1], watch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>default</td>\n",
       "      <td><div title=\"a134c40ac3ce434d8a6231b551d9f5b5\"><a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/default/jobs/monitor/a134c40ac3ce434d8a6231b551d9f5b5/overview\" target=\"_blank\" >...51d9f5b5</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jan 24 12:35:15</td>\n",
       "      <td>completed</td>\n",
       "      <td>spark-mlrun-describe-describe_spark</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=avia</div><div class=\"dictlist\">kind=remote-spark</div><div class=\"dictlist\">owner=avia</div><div class=\"dictlist\">mlrun/client_version=1.3.0-rc8</div><div class=\"dictlist\">host=spark-mlrun-describe-describe-spark-jzdzv</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resulte012fdea\" title=\"/files/aviaIguazio/demos/howto/spark/iris_dataset.csv\">dataset</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">n=150</div><div class=\"dictlist\">nvar=5</div><div class=\"dictlist\">total_missing=0.0</div><div class=\"dictlist\">memsize=0.0 YiB</div><div class=\"dictlist\">recordsize=0.0 YiB</div><div class=\"dictlist\">NUM=5</div><div class=\"dictlist\">DATE=0</div><div class=\"dictlist\">CONST=0</div><div class=\"dictlist\">CAT=0</div><div class=\"dictlist\">UNIQUE=0</div><div class=\"dictlist\">CORR=0</div><div class=\"dictlist\">REJECTED=0</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resulte012fdea\" title=\"/files/aviaIguazio/demos/howto/spark/data/spark-mlrun-describe-describe_spark/0/summary_stats.csv\">summary_stats</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resulte012fdea-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resulte012fdea-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resulte012fdea\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resulte012fdea-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
