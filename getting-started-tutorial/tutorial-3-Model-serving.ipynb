{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - Model Serving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mlrun serving can take MLRun models or standard model files and produce managed real-time serverless functions based on Nuclio real-time serverless engine, which can be deployed everywhere\n",
    "\n",
    "Nuclio is a high-performance \"serverless\" framework focused on data, I/O, and compute intensive workloads. More details can be found on https://github.com/nuclio/nuclio \n",
    "\n",
    "Simple model serving classes can be written in Python or be taken from a set of pre-developed ML/DL classes, the code can handle complex data, feature preparation, binary data (images/video). The serving engine supports the full lifecycle including auto generation of micro-services, APIs, load-balancing, logging, model monitoring, configuration management, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/new-tutorials/conf\n",
      "Project name: getting-started-tutorial-admin\n",
      "Artifacts path: /v3io/projects/{{run.project}}/artifacts\n"
     ]
    }
   ],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import load_project\n",
    "from mlrun import mlconf\n",
    "\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(project_path)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project.name}')\n",
    "print(f'Artifacts path: {mlconf.artifact_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing A Simple Serving Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is initialized automatically by the model server , all you need is to implement two mandatory methods:\n",
    "\n",
    "load() - download the model file(s) and load the model into memory, note this can be done synchronously or asynchronously\n",
    "predict() - accept request payload and return prediction/inference results\n",
    "\n",
    "More detailed information on serving classes can be found here : https://github.com/mlrun/mlrun/blob/master/mlrun/serving/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of minimal sklearn serving function example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudpickle import load\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import mlrun\n",
    "\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model('.pkl')\n",
    "        self.model = load(open(model_file, 'rb'))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body['inputs'])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model Serving Function (Service)\n",
    "\n",
    "In order to provision a serving function we need to create an MLRun function of type serving , this can be done by using the code_to_function() call from a notebook. We can also import an existing serving function/template from the marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code converts the ClassifierModel class sbove to a serving function. The name of the class to be used is set using spec.default_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function\n",
    "serving_fn = code_to_function('serving', kind='serving',image='mlrun/mlrun')\n",
    "serving_fn.spec.default_class = 'ClassifierModel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the model created in previous notebook by the training function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskState at 0x7fa4dad7ac90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = f'store://{project.name}/train-iris-train_iris_model'\n",
    "serving_fn.add_model('my_model',model_path=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = serving_fn.apply(mlrun.mount_v3io(remote='projects',mount_path='/v3io/projects'))\n",
    "serving_fn = project.set_function(serving_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the function \n",
    "This will build and deploy a Nuclio serving function. Once function is deployed successfully, http endpoint will be available which can get inference requests, call the predict method and return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-01-04 20:26:07,255 [info] Starting remote function deploy\n",
      "2021-01-04 20:26:07  (info) Deploying function\n",
      "2021-01-04 20:26:07  (info) Building\n",
      "2021-01-04 20:26:07  (info) Staging files and preparing base images\n",
      "2021-01-04 20:26:07  (info) Building processor image\n",
      "2021-01-04 20:26:10  (info) Build complete\n",
      "2021-01-04 20:26:16  (info) Function deploy complete\n",
      "> 2021-01-04 20:26:17,150 [info] function deployed, address=default-tenant.app.product-a.iguazio-cd0.com:30117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://default-tenant.app.product-a.iguazio-cd0.com:30117'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.deploy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serving'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.spec.base_spec.get('metadata').get('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the new Model Serving Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New endpoint was created for the function and can be accessed via http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The address for the function is default-tenant.app.product-a.iguazio-cd0.com:30117 \n",
      "\n",
      "{\"name\": \"ModelRouter\", \"version\": \"v2\", \"extensions\": []}"
     ]
    }
   ],
   "source": [
    "function_address = serving_fn.status.address\n",
    "print (f'The address for the function is {function_address} \\n')\n",
    "\n",
    "!curl $function_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test server by sending data for inference\n",
    "The invoke method enables to programmatically test the function\n",
    "For infer, specify the model name followed by infer =>  /v2/models/{model_name}/infer\n",
    "For complete model service API commands such as list models, get model health and explain see https://github.com/mlrun/mlrun/blob/master/mlrun/serving/README.md#model-server-api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '216ae597-6514-48bd-8dc2-fa6f277cc81a',\n",
       " 'model_name': 'my_model',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''\n",
    "serving_fn.invoke('/v2/models/my_model/infer', my_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Nuclio serving function in UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the projects screen, select the project and then select \"Real-time functions (nuclio)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nuclio-deploy.png\" alt=\"Jobs\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You've completed tutorial 3 of the Iguazio Data Science Platform.\n",
    "Go to [Tutorial 4](tutorial-4-Running-pipeline.ipynb) to learn how to create an automated pipeline for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
