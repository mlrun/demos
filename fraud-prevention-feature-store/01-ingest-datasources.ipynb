{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - This demo works with the online feature store, which is currently not part of the Open Source default deployment.\n",
    "\n",
    "This demo showcases financial fraud prevention and using the MLRun feature store to define complex features that help identify fraud. Fraud prevention specifically is a challenge as it requires processing raw transaction and events in real-time and being able to quickly respond and block transactions before they occur.\n",
    "\n",
    "To address this, we create a development pipeline and a production pipeline. Both pipelines share the same feature engineering and model code, but serve data very differently. Furthermore, we automate the data and model monitoring process, identify drift and trigger retraining in a CI/CD pipeline. This process is described in the diagram below:\n",
    "\n",
    "![Feature store demo diagram - fraud prevention](./images/feature_store_demo_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is described as follows:\n",
    "\n",
    "| TRANSACTIONS                                                                    || &#x2551; |USER EVENTS                                                                           || \n",
    "|-----------------|----------------------------------------------------------------|----------|-----------------|----------------------------------------------------------------|\n",
    "| **age**         | age group value 0-6. Some values are marked as U for unknown   | &#x2551; | **source**      | The party/entity related to the event                          |\n",
    "| **gender**      | A character to define the gender                               | &#x2551; | **event**       | event, such as login or password change                        |\n",
    "| **zipcodeOri**  | ZIP code of the person originating the transaction             | &#x2551; | **timestamp**   | The date and time of the event                                 |\n",
    "| **zipMerchant** | ZIP code of the merchant receiving the transaction             | &#x2551; |                 |                                                                |\n",
    "| **category**    | category of the transaction (e.g., transportation, food, etc.) | &#x2551; |                 |                                                                |\n",
    "| **amount**      | the total amount of the transaction                            | &#x2551; |                 |                                                                |\n",
    "| **fraud**       | whether the transaction is fraudulent                          | &#x2551; |                 |                                                                |\n",
    "| **timestamp**   | the date and time in which the transaction took place          | &#x2551; |                 |                                                                |\n",
    "| **source**      | the ID of the party/entity performing the transaction          | &#x2551; |                 |                                                                |\n",
    "| **target**      | the ID of the party/entity receiving the transaction           | &#x2551; |                 |                                                                |\n",
    "| **device**      | the device ID used to perform the transaction                  | &#x2551; |                 |                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to **Ingest** different data sources to the **Feature Store**.\n",
    "\n",
    "The following FeatureSets will be created:\n",
    "- **Transactions**: Monetary transactions between a source and a target.\n",
    "- **Events**: Account events such as account login or a password change.\n",
    "- **Label**: Fraud label for the data.\n",
    "\n",
    "By the end of this tutorial youâ€™ll learn how to:\n",
    "\n",
    "- Create an ingestion pipeline for each data source.\n",
    "- Define preprocessing, aggregation and validation of the pipeline.\n",
    "- Run the pipeline locally within the notebook.\n",
    "- Launch a real-time function to ingest live data.\n",
    "- Schedule a cron to run the task when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'fraud-demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:03:26,286 [info] loaded project fraud-demo from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Fetch, Process and Ingest our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions to adjust the timestamps of our data\n",
    "# while keeping the order of the selected events and\n",
    "# the relative distance from one event to the other\n",
    "\n",
    "def date_adjustment(sample, data_max, new_max, old_data_period, new_data_period):\n",
    "    '''\n",
    "        Adjust a specific sample's date according to the original and new time periods\n",
    "    '''\n",
    "    sample_dates_scale = ((data_max - sample) / old_data_period)\n",
    "    sample_delta = new_data_period * sample_dates_scale\n",
    "    new_sample_ts = new_max - sample_delta\n",
    "    return new_sample_ts\n",
    "\n",
    "def adjust_data_timespan(dataframe, timestamp_col='timestamp', new_period='2d', new_max_date_str='now'):\n",
    "    '''\n",
    "        Adjust the dataframe timestamps to the new time period\n",
    "    '''\n",
    "    # Calculate old time period\n",
    "    data_min = dataframe.timestamp.min()\n",
    "    data_max = dataframe.timestamp.max()\n",
    "    old_data_period = data_max-data_min\n",
    "    \n",
    "    # Set new time period\n",
    "    new_time_period = pd.Timedelta(new_period)\n",
    "    new_max = pd.Timestamp(new_max_date_str)\n",
    "    new_min = new_max-new_time_period\n",
    "    new_data_period = new_max-new_min\n",
    "    \n",
    "    # Apply the timestamp change\n",
    "    df = dataframe.copy()\n",
    "    df[timestamp_col] = df[timestamp_col].apply(lambda x: date_adjustment(x, data_max, new_max, old_data_period, new_data_period))\n",
    "    df[timestamp_col].astype(\"datetime64[s]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184435</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>72.84</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-31 21:15:54.668558510</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333175</th>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>23.58</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 20:02:48.943938113</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451032</th>\n",
       "      <td>141</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 03:48:04.432990439</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>741ccdad2743422c98939329976a9c06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        step age gender  zipcodeOri  zipMerchant           category  amount  \\\n",
       "184435    63   5      M       28007        28007  es_transportation   72.84   \n",
       "333175   108   5      M       28007        28007  es_transportation   23.58   \n",
       "451032   141   5      M       28007        28007  es_transportation    4.31   \n",
       "\n",
       "        fraud                     timestamp       source       target  \\\n",
       "184435      0 2023-01-31 21:15:54.668558510  C1000148617  M1823072687   \n",
       "333175      0 2023-01-30 20:02:48.943938113  C1000148617  M1823072687   \n",
       "451032      0 2023-02-01 03:48:04.432990439  C1000148617   M348934600   \n",
       "\n",
       "                                  device  \n",
       "184435  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "333175  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "451032  741ccdad2743422c98939329976a9c06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch the transactions dataset from the server\n",
    "transactions_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# use only first 50k\n",
    "transactions_data = transactions_data.sort_values(by='source', axis=0)[:50000]\n",
    "\n",
    "# Adjust the samples timestamp for the past 2 days\n",
    "transactions_data = adjust_data_timespan(transactions_data, new_period='2d')\n",
    "\n",
    "# logging the data\n",
    "project.log_dataset('transactions_data', transactions_data, format='csv')\n",
    "\n",
    "# Preview\n",
    "transactions_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Create a FeatureSet and Preprocessing Pipeline\n",
    "Create the FeatureSet (data pipeline) definition for the **credit transaction processing** which describes the offline/online data transformations and aggregations.<br>\n",
    "The feature store will automatically add an offline `parquet` target and an online `NoSQL` target by using `set_targets()`.\n",
    "\n",
    "The data pipeline consists of:\n",
    "\n",
    "* **Extracting** the data components (hour, day of week)\n",
    "* **Mapping** the age values\n",
    "* **One hot encoding** for the transaction category and the gender\n",
    "* **Aggregating** the amount (avg, sum, count, max over 2/12/24 hour time windows)\n",
    "* **Aggregating** the transactions per category (over 14 days time windows)\n",
    "* **Writing** the results to **offline** (Parquet) and **online** (NoSQL) targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLRun's Feature Store\n",
    "import mlrun.feature_store as fstore\n",
    "from mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transactions FeatureSet\n",
    "transaction_set = fstore.FeatureSet(\"transactions\", \n",
    "                                    entities=[fstore.Entity(\"source\")], \n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"transactions feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"907pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 907.45 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 903.45,-94 903.45,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-27.05 40.7,-27.15 42.83,-27.3 44.92,-27.49 46.98,-27.74 48.99,-28.03 50.95,-28.36 52.84,-28.75 54.66,-29.18 56.4,-29.65 58.06,-30.16 59.63,-30.71 61.11,-31.31 62.49,-31.94 63.76,-32.61 64.93,-33.31 65.99,-34.04 66.93,-34.8 67.77,-35.59 68.48,-36.41 69.09,-37.25 69.58,-38.11 69.95,-38.99 70.21,-39.89 70.36,-40.8 70.4,-41.72 70.33,-42.65 70.16,-43.59 69.89,-44.53 69.53,-45.47 69.07,-46.41 68.52,-47.35 67.89,-48.28 67.18,-49.2 66.4,-50.11 65.55,-51.01 64.63,-51.89 63.65,-52.75 62.62,-53.59 61.53,-54.41 60.4,-55.2 59.23,-55.96 58.02,-56.69 56.78,-57.39 55.5,-58.06 54.2,-58.69 52.88,-59.29 51.53,-59.84 50.17,-60.35 48.79,-60.82 47.4,-61.25 46,-61.64 44.59,-61.97 43.17,-62.26 41.75,-62.51 40.32,-62.7 38.89,-62.85 37.45,-62.95 36.02,-63 34.58,-63 33.15,-62.95 31.71,-62.85 30.28,-62.7 28.85,-62.51 27.43,-62.26 26.01,-61.97 24.6,-61.64 23.2,-61.25 21.81,-60.82 20.43,-60.35 19.07,-59.84 17.72,-59.29 16.4,-58.69 15.1,-58.06 13.82,-57.39 12.58,-56.69 11.37,-55.96 10.2,-55.2 9.07,-54.41 7.98,-53.59 6.95,-52.75 5.97,-51.89 5.05,-51.01 4.2,-50.11 3.42,-49.2 2.71,-48.28 2.08,-47.35 1.53,-46.41 1.07,-45.47 0.71,-44.53 0.44,-43.59 0.27,-42.65 0.2,-41.72 0.24,-40.8 0.39,-39.89 0.65,-38.99 1.02,-38.11 1.51,-37.25 2.11,-36.41 2.83,-35.59 3.66,-34.8 4.61,-34.04 5.67,-33.31 6.84,-32.61 8.11,-31.94 9.49,-31.31 10.97,-30.71 12.54,-30.16 14.2,-29.65 15.94,-29.18 17.76,-28.75 19.65,-28.36 21.61,-28.03 23.62,-27.74 25.68,-27.49 27.77,-27.3 29.9,-27.15 32.05,-27.05 34.22,-27 36.38,-27 38.55,-27.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- DateExtractor -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>DateExtractor</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"183.94\" cy=\"-45\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"183.94\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">DateExtractor</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;DateExtractor -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;DateExtractor</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.94,-45C78.02,-45 87.01,-45 96.29,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.46,-48.5 106.45,-45 96.45,-41.5 96.46,-48.5\"/>\n",
       "</g>\n",
       "<!-- MapValues -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>MapValues</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"359.03\" cy=\"-45\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"359.03\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">MapValues</text>\n",
       "</g>\n",
       "<!-- DateExtractor&#45;&gt;MapValues -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>DateExtractor&#45;&gt;MapValues</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.36,-45C269.86,-45 278.49,-45 286.92,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.08,-48.5 297.08,-45 287.08,-41.5 287.08,-48.5\"/>\n",
       "</g>\n",
       "<!-- OneHotEncoder -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>OneHotEncoder</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"541.92\" cy=\"-45\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"541.92\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- MapValues&#45;&gt;OneHotEncoder -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>MapValues&#45;&gt;OneHotEncoder</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.02,-45C429.27,-45 437.9,-45 446.57,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"446.75,-48.5 456.75,-45 446.75,-41.5 446.75,-48.5\"/>\n",
       "</g>\n",
       "<!-- Aggregates -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Aggregates</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"726.75\" cy=\"-45\" rx=\"63.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"726.75\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Aggregates</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;Aggregates -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;Aggregates</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M627.11,-45C635.71,-45 644.39,-45 652.85,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"653.04,-48.5 663.04,-45 653.04,-41.5 653.04,-48.5\"/>\n",
       "</g>\n",
       "<!-- parquet/parquet -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M899.45,-86.73C899.45,-88.53 883.09,-90 862.95,-90 842.81,-90 826.45,-88.53 826.45,-86.73 826.45,-86.73 826.45,-57.27 826.45,-57.27 826.45,-55.47 842.81,-54 862.95,-54 883.09,-54 899.45,-55.47 899.45,-57.27 899.45,-57.27 899.45,-86.73 899.45,-86.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M899.45,-86.73C899.45,-84.92 883.09,-83.45 862.95,-83.45 842.81,-83.45 826.45,-84.92 826.45,-86.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.95\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">parquet</text>\n",
       "</g>\n",
       "<!-- Aggregates&#45;&gt;parquet/parquet -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Aggregates&#45;&gt;parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.25,-55.36C791.37,-57.8 804.21,-60.38 816.06,-62.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"815.69,-66.26 826.18,-64.8 817.07,-59.4 815.69,-66.26\"/>\n",
       "</g>\n",
       "<!-- nosql/nosql -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.45,-32.73C890.45,-34.53 878.12,-36 862.95,-36 847.77,-36 835.45,-34.53 835.45,-32.73 835.45,-32.73 835.45,-3.27 835.45,-3.27 835.45,-1.47 847.77,0 862.95,0 878.12,0 890.45,-1.47 890.45,-3.27 890.45,-3.27 890.45,-32.73 890.45,-32.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.45,-32.73C890.45,-30.92 878.12,-29.45 862.95,-29.45 847.77,-29.45 835.45,-30.92 835.45,-32.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.95\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">nosql</text>\n",
       "</g>\n",
       "<!-- Aggregates&#45;&gt;nosql/nosql -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Aggregates&#45;&gt;nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.25,-34.64C794.6,-31.55 811.1,-28.23 825.29,-25.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"826.05,-28.79 835.16,-23.39 824.67,-21.93 826.05,-28.79\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8bf56bad30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and add value mapping\n",
    "main_categories = [\"es_transportation\", \"es_health\", \"es_otherservices\",\n",
    "       \"es_food\", \"es_hotelservices\", \"es_barsandrestaurants\",\n",
    "       \"es_tech\", \"es_sportsandtoys\", \"es_wellnessandbeauty\",\n",
    "       \"es_hyper\", \"es_fashion\", \"es_home\", \"es_contents\",\n",
    "       \"es_travel\", \"es_leisure\"]\n",
    "\n",
    "# One Hot Encode the newly defined mappings\n",
    "one_hot_encoder_mapping = {'category': main_categories,\n",
    "                           'gender': list(transactions_data.gender.unique())}\n",
    "\n",
    "# Define the graph steps\n",
    "transaction_set.graph\\\n",
    "    .to(DateExtractor(parts = ['hour','month'], timestamp_col = 'timestamp'))\\\n",
    "    .to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\\\n",
    "    .to(OneHotEncoder(mapping=one_hot_encoder_mapping))\n",
    "\n",
    "\n",
    "# Add aggregation for 2 hour time windows\n",
    "transaction_set.add_aggregation(name='amount',\n",
    "                                column='amount',\n",
    "                                operations=['avg','min','max'],\n",
    "                                windows=['2h'],\n",
    "                                period='1h')\n",
    "\n",
    "\n",
    "# Add the category aggregations over a 1 day window\n",
    "for category in main_categories:\n",
    "    transaction_set.add_aggregation(name=category,column=f'category_{category}',\n",
    "                                    operations=['count'], windows=['1d'], period='12h')\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "transaction_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "transaction_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running with a Spark operator, the MLRun execution details are returned, allowing tracking of the jobâ€™s status and results.\n",
    "Spark operator ingestion is always executed remotely.\n",
    "\n",
    "The cell below should be executed only once to build the spark job image before running the first ingest. It may take a few minutes to prepare the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.runtimes import RemoteSparkRuntime\n",
    "# RemoteSparkRuntime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlrun: start-code\n",
    "\n",
    "from mlrun.feature_store.api import ingest\n",
    "def ingest_handler(context):\n",
    "    ingest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n",
    "        \n",
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = mlrun.code_to_function(name='transactions_func', kind='spark', handler=\"ingest_handler\")\n",
    "\n",
    "function.with_driver_requests(cpu=\"1000m\", mem=\"4G\")\n",
    "function.with_driver_limits(cpu=\"1000\")\n",
    "function.with_executor_requests(cpu=\"500m\", mem=\"2G\")\n",
    "function.with_executor_limits(cpu=\"500m\")\n",
    "function.spec.replicas = 3\n",
    "function.with_igz_spark()\n",
    "function.spec.use_default_image = True\n",
    "\n",
    "run_config = fstore.RunConfig(function=function, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:03:51,245 [info] starting run transactions-ingest uid=71e69146bc1c4656a929e0d50dbe9701 DB=http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.201.8.190 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///etc/config/mlrun/spark-function-code.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2023-02-01 07:04:01,506 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2023-02-01 07:04:04,692 [info] starting ingestion task to store://feature-sets/fraud-demo-dani/transactions:latest.\n",
      "2023-02-01 07:04:05,321 INFO spark.SparkContext: Running Spark version 3.2.1\n",
      "2023-02-01 07:04:05,353 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-02-01 07:04:05,353 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-02-01 07:04:05,354 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-02-01 07:04:05,354 INFO spark.SparkContext: Submitted application: transactions-ingest-71e69146bc1c4656a929e0d50dbe9701\n",
      "2023-02-01 07:04:05,380 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-02-01 07:04:05,397 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-02-01 07:04:05,400 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-02-01 07:04:05,478 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2023-02-01 07:04:05,479 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2023-02-01 07:04:05,479 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-02-01 07:04:05,479 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-02-01 07:04:05,480 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2023-02-01 07:04:05,817 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2023-02-01 07:04:05,849 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-02-01 07:04:05,889 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-02-01 07:04:05,917 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-02-01 07:04:05,917 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-02-01 07:04:05,922 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-02-01 07:04:05,950 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-277ac685-80b2-4958-b39d-06eacf45d28d/blockmgr-e34ca130-8b49-4064-b405-fb3f63c1300b\n",
      "2023-02-01 07:04:05,981 INFO memory.MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "2023-02-01 07:04:06,000 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-02-01 07:04:06,119 INFO util.log: Logging initialized @6681ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-02-01 07:04:06,202 INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.11+9-LTS\n",
      "2023-02-01 07:04:06,224 INFO server.Server: Started @6786ms\n",
      "2023-02-01 07:04:06,263 INFO server.AbstractConnector: Started ServerConnector@c49104b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-02-01 07:04:06,263 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-02-01 07:04:06,290 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@352054a2{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,293 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40b44c60{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,293 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5249cf86{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e5f20c7{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,295 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ee2405{/stages,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,296 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@529468f8{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,297 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@764e917c{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,299 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ef1694f{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,299 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61978ac7{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,300 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e497c53{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,301 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2bf7647e{/storage,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,302 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bb0fe10{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,303 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@357f37c4{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,304 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@215dbea{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,305 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30c18e40{/environment,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,305 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d9db055{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,306 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f787dcb{/executors,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,307 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@617a4bef{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,308 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@249d4353{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,309 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c912df2{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,319 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@555649b{/static,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,320 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33940c5b{/,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,321 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b25fcf4{/api,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,322 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@976d811{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,323 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cb2230e{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:06,325 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:4040\n",
      "2023-02-01 07:04:06,341 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1675235045311\n",
      "2023-02-01 07:04:06,342 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1675235045311\n",
      "2023-02-01 07:04:06,342 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1675235045311\n",
      "2023-02-01 07:04:06,342 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1675235045311\n",
      "2023-02-01 07:04:06,342 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1675235045311\n",
      "2023-02-01 07:04:06,344 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2023-02-01 07:04:06,437 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2023-02-01 07:04:07,520 INFO k8s.ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 3, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
      "2023-02-01 07:04:07,617 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 07:04:07,650 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2023-02-01 07:04:07,650 INFO netty.NettyBlockTransferService: Server created on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079\n",
      "2023-02-01 07:04:07,652 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-02-01 07:04:07,661 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 07:04:07,666 INFO storage.BlockManagerMasterEndpoint: Registering block manager transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 with 2.2 GiB RAM, BlockManagerId(driver, transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 07:04:07,673 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 07:04:07,674 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 07:04:07,708 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 07:04:07,721 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@693d5ab9{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:07,747 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 07:04:24,591 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.140.94:57642) with ID 1,  ResourceProfileId 0\n",
      "2023-02-01 07:04:24,889 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.140.94:41276 with 1007.8 MiB RAM, BlockManagerId(1, 10.200.140.94, 41276, None)\n",
      "2023-02-01 07:04:26,597 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.136.127:35826) with ID 3,  ResourceProfileId 0\n",
      "2023-02-01 07:04:26,791 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.136.126:48696) with ID 2,  ResourceProfileId 0\n",
      "2023-02-01 07:04:26,801 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-02-01 07:04:26,991 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.136.127:43545 with 1007.8 MiB RAM, BlockManagerId(3, 10.200.136.127, 43545, None)\n",
      "2023-02-01 07:04:27,092 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.136.126:44915 with 1007.8 MiB RAM, BlockManagerId(2, 10.200.136.126, 44915, None)\n",
      "2023-02-01 07:04:27,477 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-02-01 07:04:27,504 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2023-02-01 07:04:27,522 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dcc2373{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:27,523 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4797cd52{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:27,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2979d2c9{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:27,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1eae5b8{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:27,546 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cf6fc8c{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-02-01 07:04:29,290 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "2023-02-01 07:04:30,215 INFO datasources.InMemoryFileIndex: It took 211 ms to list leaf files for 1 paths.\n",
      "2023-02-01 07:04:30,392 INFO datasources.InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
      "2023-02-01 07:04:33,103 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:04:33,112 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-02-01 07:04:33,116 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-02-01 07:04:33,669 INFO codegen.CodeGenerator: Code generated in 211.181102 ms\n",
      "2023-02-01 07:04:33,735 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:33,819 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:33,822 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:33,827 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:04:33,838 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:04:33,976 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:04:33,994 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-02-01 07:04:33,994 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:04:33,995 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:04:33,996 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:04:34,001 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:04:34,082 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:34,091 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:34,092 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:34,093 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:04:34,112 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 07:04:34,113 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-02-01 07:04:34,162 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:04:34,886 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.200.140.94:41276 (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:39,271 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.140.94:41276 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:45,982 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11832 ms on 10.200.140.94 (executor 1) (1/1)\n",
      "2023-02-01 07:04:45,985 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:04:45,990 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 11.967 s\n",
      "2023-02-01 07:04:45,994 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:04:45,995 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-02-01 07:04:45,997 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 12.020588 s\n",
      "2023-02-01 07:04:46,023 INFO codegen.CodeGenerator: Code generated in 12.659441 ms\n",
      "2023-02-01 07:04:46,074 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:46,085 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.200.140.94:41276 in memory (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:46,086 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:04:46,086 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 07:04:46,086 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-02-01 07:04:46,096 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:46,124 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:46,125 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:46,126 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:04:46,127 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:04:46,200 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:04:46,201 INFO scheduler.DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:04:46,201 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:04:46,201 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:04:46,201 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:04:46,203 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:04:46,230 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:46,239 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:04:46,245 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 10.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:46,246 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:04:46,247 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:04:46,247 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:04:46,249 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:04:46,249 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:04:46,295 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:04:46,299 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.200.140.94:41276 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:46,901 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.136.126:44915 (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:46,992 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.136.127:43545 (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:55,994 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.136.126:44915 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:04:57,094 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.136.127:43545 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:05,311 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 19062 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:05,701 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 19453 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:05,702 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:05,702 INFO scheduler.DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 19.498 s\n",
      "2023-02-01 07:05:05,703 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:05:05,703 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-02-01 07:05:05,703 INFO scheduler.DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 19.503250 s\n",
      "2023-02-01 07:05:05,796 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:05,796 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 07:05:05,797 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:05,863 INFO codegen.CodeGenerator: Code generated in 35.160777 ms\n",
      "2023-02-01 07:05:05,869 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:05,884 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 10.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:05,889 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.136.127:43545 in memory (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:05,889 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.136.126:44915 in memory (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:05,897 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:05,899 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:05,900 INFO spark.SparkContext: Created broadcast 4 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:05,904 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:06,007 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:06,008 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 07:05:06,008 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:06,064 INFO codegen.CodeGenerator: Code generated in 37.045585 ms\n",
      "2023-02-01 07:05:06,069 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:06,097 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:06,098 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:06,099 INFO spark.SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:06,100 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:06,148 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.200.136.126:44915 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:06,154 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:06,154 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.200.136.127:43545 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:06,262 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:06,262 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 07:05:06,263 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:06,312 INFO codegen.CodeGenerator: Code generated in 34.593508 ms\n",
      "2023-02-01 07:05:06,317 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:06,343 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:06,347 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:06,349 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:06,350 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:07,715 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:11,523 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,524 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:05:11,524 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,529 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,529 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:05:11,529 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,533 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,533 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:05:11,533 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,537 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,538 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:05:11,538 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,541 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,542 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:05:11,542 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,546 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,546 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:05:11,546 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,549 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,549 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:05:11,550 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,553 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,553 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:05:11,553 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,557 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,557 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:05:11,557 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,560 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,560 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:05:11,560 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,564 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,564 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:05:11,564 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,567 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,567 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:05:11,567 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,570 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,570 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:05:11,570 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,573 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,573 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:05:11,574 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,576 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,576 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:05:11,577 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:11,580 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:11,580 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:05:11,580 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:12,036 INFO codegen.CodeGenerator: Code generated in 108.126111 ms\n",
      "2023-02-01 07:05:12,041 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,070 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,071 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,072 INFO spark.SparkContext: Created broadcast 7 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,073 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,165 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,169 INFO scheduler.DAGScheduler: Registering RDD 32 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-02-01 07:05:12,174 INFO scheduler.DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,175 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,175 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,176 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,182 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[32] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,263 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 98.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,266 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,267 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 33.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,268 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,270 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[32] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,270 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:12,272 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:12,273 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:12,340 INFO codegen.CodeGenerator: Code generated in 88.657255 ms\n",
      "2023-02-01 07:05:12,345 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,364 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,365 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,366 INFO spark.SparkContext: Created broadcast 9 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,367 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,388 INFO scheduler.DAGScheduler: Registering RDD 37 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "2023-02-01 07:05:12,389 INFO scheduler.DAGScheduler: Got map stage job 3 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,389 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,389 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,389 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,390 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[37] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,390 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.136.127:43545 (size: 33.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:12,390 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.136.126:44915 (size: 33.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:12,401 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,404 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,405 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,406 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,406 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[37] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,406 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:12,408 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:12,477 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:12,505 INFO codegen.CodeGenerator: Code generated in 59.158036 ms\n",
      "2023-02-01 07:05:12,517 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,534 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,534 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,535 INFO spark.SparkContext: Created broadcast 11 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,536 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,548 INFO scheduler.DAGScheduler: Registering RDD 42 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "2023-02-01 07:05:12,549 INFO scheduler.DAGScheduler: Got map stage job 4 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,549 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,549 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,549 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,550 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[42] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,588 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,591 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,592 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,593 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,593 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[42] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,593 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:12,651 INFO codegen.CodeGenerator: Code generated in 53.808852 ms\n",
      "2023-02-01 07:05:12,655 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,672 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,672 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,673 INFO spark.SparkContext: Created broadcast 13 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,674 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,688 INFO scheduler.DAGScheduler: Registering RDD 47 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "2023-02-01 07:05:12,688 INFO scheduler.DAGScheduler: Got map stage job 5 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,688 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 5 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,688 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,688 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,689 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,715 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,717 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,721 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,722 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,723 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,723 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:12,803 INFO codegen.CodeGenerator: Code generated in 53.467504 ms\n",
      "2023-02-01 07:05:12,806 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,833 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,834 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,834 INFO spark.SparkContext: Created broadcast 15 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,835 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,847 INFO scheduler.DAGScheduler: Registering RDD 52 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "2023-02-01 07:05:12,848 INFO scheduler.DAGScheduler: Got map stage job 6 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,848 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,848 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,848 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,848 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[52] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,863 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,866 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,866 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,867 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,867 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[52] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,867 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:12,933 INFO codegen.CodeGenerator: Code generated in 47.384136 ms\n",
      "2023-02-01 07:05:12,936 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,952 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,953 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,954 INFO spark.SparkContext: Created broadcast 17 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:12,955 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:12,965 INFO scheduler.DAGScheduler: Registering RDD 57 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "2023-02-01 07:05:12,965 INFO scheduler.DAGScheduler: Got map stage job 7 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:12,966 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:12,966 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:12,966 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:12,966 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[57] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:12,973 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,975 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:12,976 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:12,976 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:12,977 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[57] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:12,977 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,047 INFO codegen.CodeGenerator: Code generated in 45.981649 ms\n",
      "2023-02-01 07:05:13,051 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,067 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,068 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,069 INFO spark.SparkContext: Created broadcast 19 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,070 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,079 INFO scheduler.DAGScheduler: Registering RDD 62 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "2023-02-01 07:05:13,079 INFO scheduler.DAGScheduler: Got map stage job 8 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,079 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 8 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,079 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,079 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,080 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[62] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,087 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,090 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,090 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,091 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,091 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[62] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,092 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,159 INFO codegen.CodeGenerator: Code generated in 43.820967 ms\n",
      "2023-02-01 07:05:13,163 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,179 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,180 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,180 INFO spark.SparkContext: Created broadcast 21 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,181 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,193 INFO scheduler.DAGScheduler: Registering RDD 67 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "2023-02-01 07:05:13,193 INFO scheduler.DAGScheduler: Got map stage job 9 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,193 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,193 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,194 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,198 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[67] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,208 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,211 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,213 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,214 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,214 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[67] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,215 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,280 INFO codegen.CodeGenerator: Code generated in 44.266197 ms\n",
      "2023-02-01 07:05:13,284 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,300 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,301 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,302 INFO spark.SparkContext: Created broadcast 23 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,303 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,312 INFO scheduler.DAGScheduler: Registering RDD 72 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "2023-02-01 07:05:13,312 INFO scheduler.DAGScheduler: Got map stage job 10 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,312 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 10 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,312 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,312 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,313 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[72] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,319 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,321 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,321 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,322 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,322 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[72] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,322 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,405 INFO codegen.CodeGenerator: Code generated in 42.876516 ms\n",
      "2023-02-01 07:05:13,409 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,426 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,426 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,427 INFO spark.SparkContext: Created broadcast 25 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,428 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,445 INFO scheduler.DAGScheduler: Registering RDD 77 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
      "2023-02-01 07:05:13,446 INFO scheduler.DAGScheduler: Got map stage job 11 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,446 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,446 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,446 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,446 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[77] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,460 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,462 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,462 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,463 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,463 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[77] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,463 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,522 INFO codegen.CodeGenerator: Code generated in 41.141389 ms\n",
      "2023-02-01 07:05:13,525 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,542 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,543 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,544 INFO spark.SparkContext: Created broadcast 27 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,545 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,552 INFO scheduler.DAGScheduler: Registering RDD 82 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
      "2023-02-01 07:05:13,552 INFO scheduler.DAGScheduler: Got map stage job 12 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,552 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,552 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,552 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,553 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[82] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,559 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,561 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,561 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,562 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,562 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[82] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,562 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,625 INFO codegen.CodeGenerator: Code generated in 40.366728 ms\n",
      "2023-02-01 07:05:13,629 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,645 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,645 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,647 INFO spark.SparkContext: Created broadcast 29 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,647 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,655 INFO scheduler.DAGScheduler: Registering RDD 87 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
      "2023-02-01 07:05:13,655 INFO scheduler.DAGScheduler: Got map stage job 13 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,656 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,656 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,656 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,656 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[87] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,662 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,664 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,665 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,665 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,666 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[87] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,666 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,726 INFO codegen.CodeGenerator: Code generated in 38.526756 ms\n",
      "2023-02-01 07:05:13,730 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,746 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,748 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,749 INFO spark.SparkContext: Created broadcast 31 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,750 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,757 INFO scheduler.DAGScheduler: Registering RDD 92 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
      "2023-02-01 07:05:13,757 INFO scheduler.DAGScheduler: Got map stage job 14 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,758 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 14 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,758 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,758 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,758 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[92] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,770 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,792 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,805 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,806 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,806 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[92] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,806 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:13,877 INFO codegen.CodeGenerator: Code generated in 52.382187 ms\n",
      "2023-02-01 07:05:13,881 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,911 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,912 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,912 INFO spark.SparkContext: Created broadcast 33 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:13,913 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Registering RDD 97 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Got map stage job 15 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:13,922 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[97] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:13,932 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,934 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:13,934 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:13,935 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:13,936 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[97] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:13,936 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:14,019 INFO codegen.CodeGenerator: Code generated in 48.661726 ms\n",
      "2023-02-01 07:05:14,023 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,041 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,041 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:14,042 INFO spark.SparkContext: Created broadcast 35 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:14,043 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:14,052 INFO scheduler.DAGScheduler: Registering RDD 102 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "2023-02-01 07:05:14,052 INFO scheduler.DAGScheduler: Got map stage job 16 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:14,052 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:14,052 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:14,053 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:14,053 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[102] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:14,059 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,061 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,062 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:14,062 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:14,063 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[102] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:14,063 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:14,134 INFO codegen.CodeGenerator: Code generated in 37.662963 ms\n",
      "2023-02-01 07:05:14,137 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,153 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,154 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:14,155 INFO spark.SparkContext: Created broadcast 37 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:14,155 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:14,163 INFO scheduler.DAGScheduler: Registering RDD 107 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "2023-02-01 07:05:14,163 INFO scheduler.DAGScheduler: Got map stage job 17 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:14,163 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 17 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:14,163 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:14,163 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:14,164 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[107] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:14,168 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,170 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:14,171 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:14,171 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:14,171 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[107] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:14,171 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:14,899 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:15,168 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:15,194 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 07:05:22,285 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:22,288 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 10016 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:22,299 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:23,071 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:23,670 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:23,671 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 11264 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:23,684 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:23,796 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:23,797 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 11525 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:23,797 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:23,798 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 11.612 s\n",
      "2023-02-01 07:05:23,798 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:23,799 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 3, ShuffleMapStage 10, ShuffleMapStage 4, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:23,799 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:23,799 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:23,906 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:24,179 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:24,489 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 07:05:26,587 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:26,588 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 4303 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:26,588 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:26,589 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 14.198 s\n",
      "2023-02-01 07:05:26,589 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:26,589 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 4, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:26,589 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:26,589 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:26,604 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.200.136.126:44915 (size: 34.2 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:26,995 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:27,779 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 10) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:27,779 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 4109 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:27,994 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.200.140.94:41276 (size: 34.2 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:28,303 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 11) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:28,304 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 4507 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:28,304 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:28,304 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 15.753 s\n",
      "2023-02-01 07:05:28,304 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:28,305 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:28,305 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:28,305 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:28,388 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:28,398 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:28,793 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:30,378 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 12) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:30,378 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 10) in 2599 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:30,384 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 13) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:30,384 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 3797 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:30,385 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:30,385 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 17.695 s\n",
      "2023-02-01 07:05:30,385 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:30,385 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:30,385 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:30,385 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:30,396 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.200.136.126:44915 (size: 34.2 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:30,469 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:30,782 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:30,790 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:31,600 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 14) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:31,600 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 11) in 3297 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:31,695 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.200.136.127:43545 (size: 34.2 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 07:05:32,086 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:32,970 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:32,971 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 12) in 2594 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:32,971 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:32,972 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 20.123 s\n",
      "2023-02-01 07:05:32,972 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:32,972 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:32,972 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:32,972 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:32,983 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:33,284 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:33,700 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:33,700 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 13) in 3316 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:33,793 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:34,191 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:34,891 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:34,891 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 14) in 3291 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:34,892 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:34,893 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 21.926 s\n",
      "2023-02-01 07:05:34,893 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:34,893 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-02-01 07:05:34,893 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:34,893 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:34,910 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:34,910 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:34,913 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:34,985 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:34,989 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:34,994 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.200.136.127:43545 in memory (size: 34.2 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:35,088 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.200.136.126:44915 in memory (size: 34.2 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:35,384 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 07:05:35,985 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:35,985 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 3015 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:36,194 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:36,307 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 19) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:36,307 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 2607 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:36,307 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:36,308 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 23.227 s\n",
      "2023-02-01 07:05:36,308 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:36,308 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:36,308 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:36,308 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:36,394 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:36,482 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:36,691 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:37,891 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 20) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:37,891 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 3000 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:37,988 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:38,198 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:38,295 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 21) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:38,295 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 2311 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:38,295 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:38,296 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 25.098 s\n",
      "2023-02-01 07:05:38,296 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:38,296 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:38,296 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:38,296 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:38,375 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:38,669 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:39,208 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 22) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:39,208 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 19) in 2901 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:39,293 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:39,594 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:40,593 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 23) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:40,594 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 20) in 2702 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:40,594 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:40,594 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 27.280 s\n",
      "2023-02-01 07:05:40,594 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:40,594 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:40,594 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:40,594 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:40,604 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.4 MiB)\n",
      "2023-02-01 07:05:40,882 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 24) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:40,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 21) in 2589 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:40,893 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:40,968 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:41,187 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:41,801 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 25) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:41,801 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 22) in 2593 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:41,801 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:41,802 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 28.355 s\n",
      "2023-02-01 07:05:41,802 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:41,802 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:41,802 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:41,802 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:41,886 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:42,102 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:42,974 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 26) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:42,974 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 24) in 2092 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:42,983 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:43,269 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:43,892 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 27) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:43,892 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 23) in 3299 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:43,892 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:43,893 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 30.340 s\n",
      "2023-02-01 07:05:43,893 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:43,893 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:43,893 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:43,893 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:43,901 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:05:44,193 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:44,594 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 28) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:44,594 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 25) in 2794 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:44,603 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:44,887 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 29) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:44,888 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 26) in 1915 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:44,888 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:44,888 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 31.231 s\n",
      "2023-02-01 07:05:44,889 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:44,889 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 14)\n",
      "2023-02-01 07:05:44,889 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:44,889 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:44,906 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:44,973 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.200.140.94:41276 (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:45,276 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:46,798 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 30) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:46,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 27) in 2908 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:46,809 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.200.136.127:43545 (size: 34.2 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:46,907 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 31) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:46,907 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 28) in 2314 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:46,907 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:46,908 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 33.149 s\n",
      "2023-02-01 07:05:46,908 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:46,908 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17)\n",
      "2023-02-01 07:05:46,908 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:46,908 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:46,990 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:47,097 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:47,200 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:47,581 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 32) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:47,582 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 29) in 2695 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:47,591 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:47,868 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:48,986 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 33) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:48,986 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 30) in 2188 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:05:48,986 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:48,987 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 35.064 s\n",
      "2023-02-01 07:05:48,987 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:48,987 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 16, ShuffleMapStage 17)\n",
      "2023-02-01 07:05:48,987 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:48,987 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:48,995 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.200.136.127:43545 (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:49,290 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:49,772 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 34) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:49,772 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 32) in 2191 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:05:49,785 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.200.140.94:41276 (size: 34.2 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:50,086 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:05:50,492 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 31) in 3585 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:50,492 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:50,493 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 36.439 s\n",
      "2023-02-01 07:05:50,493 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:50,493 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 17)\n",
      "2023-02-01 07:05:50,493 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:50,493 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:51,309 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 33) in 2324 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:51,686 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 34) in 1914 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:51,686 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:51,686 INFO scheduler.DAGScheduler: ShuffleMapStage 17 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 37.522 s\n",
      "2023-02-01 07:05:51,687 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:51,687 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:05:51,687 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:51,687 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:51,714 INFO adaptive.ShufflePartitionsUtil: For shuffle(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), advisory target size: 67108864, actual target size 3159149, minimum partition size: 1048576\n",
      "2023-02-01 07:05:51,772 INFO codegen.CodeGenerator: Code generated in 16.567374 ms\n",
      "2023-02-01 07:05:51,789 INFO codegen.CodeGenerator: Code generated in 11.817617 ms\n",
      "2023-02-01 07:05:51,837 INFO codegen.CodeGenerator: Code generated in 15.127001 ms\n",
      "2023-02-01 07:05:51,851 INFO codegen.CodeGenerator: Code generated in 9.419164 ms\n",
      "2023-02-01 07:05:51,884 INFO codegen.CodeGenerator: Code generated in 15.522687 ms\n",
      "2023-02-01 07:05:51,897 INFO codegen.CodeGenerator: Code generated in 9.169929 ms\n",
      "2023-02-01 07:05:51,928 INFO codegen.CodeGenerator: Code generated in 14.894267 ms\n",
      "2023-02-01 07:05:51,941 INFO codegen.CodeGenerator: Code generated in 9.292282 ms\n",
      "2023-02-01 07:05:51,980 INFO codegen.CodeGenerator: Code generated in 14.6051 ms\n",
      "2023-02-01 07:05:51,994 INFO codegen.CodeGenerator: Code generated in 10.037841 ms\n",
      "2023-02-01 07:05:52,022 INFO codegen.CodeGenerator: Code generated in 14.486472 ms\n",
      "2023-02-01 07:05:52,036 INFO codegen.CodeGenerator: Code generated in 9.747462 ms\n",
      "2023-02-01 07:05:52,056 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,058 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,067 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:52,069 INFO codegen.CodeGenerator: Code generated in 14.445954 ms\n",
      "2023-02-01 07:05:52,072 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,073 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:52,074 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,080 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,081 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:52,081 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:52,083 INFO codegen.CodeGenerator: Code generated in 9.489598 ms\n",
      "2023-02-01 07:05:52,092 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,093 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:52,094 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,106 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,106 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.200.136.127:43545 in memory (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,110 INFO codegen.CodeGenerator: Code generated in 14.540613 ms\n",
      "2023-02-01 07:05:52,123 INFO codegen.CodeGenerator: Code generated in 9.304301 ms\n",
      "2023-02-01 07:05:52,150 INFO codegen.CodeGenerator: Code generated in 13.993925 ms\n",
      "2023-02-01 07:05:52,163 INFO codegen.CodeGenerator: Code generated in 9.123682 ms\n",
      "2023-02-01 07:05:52,168 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.200.140.94:41276 in memory (size: 34.2 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:52,175 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,175 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.200.136.127:43545 in memory (size: 34.2 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:52,175 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.200.140.94:41276 in memory (size: 34.2 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:52,183 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,184 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,184 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,190 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,190 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:52,191 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,191 INFO codegen.CodeGenerator: Code generated in 14.661284 ms\n",
      "2023-02-01 07:05:52,199 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,200 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:52,201 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:52,205 INFO codegen.CodeGenerator: Code generated in 9.774855 ms\n",
      "2023-02-01 07:05:52,237 INFO codegen.CodeGenerator: Code generated in 13.751878 ms\n",
      "2023-02-01 07:05:52,251 INFO codegen.CodeGenerator: Code generated in 9.732792 ms\n",
      "2023-02-01 07:05:52,271 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:52,273 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:52,273 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:52,278 INFO codegen.CodeGenerator: Code generated in 15.221203 ms\n",
      "2023-02-01 07:05:52,291 INFO codegen.CodeGenerator: Code generated in 9.020244 ms\n",
      "2023-02-01 07:05:52,317 INFO codegen.CodeGenerator: Code generated in 14.293842 ms\n",
      "2023-02-01 07:05:52,330 INFO codegen.CodeGenerator: Code generated in 8.626346 ms\n",
      "2023-02-01 07:05:52,357 INFO codegen.CodeGenerator: Code generated in 14.579717 ms\n",
      "2023-02-01 07:05:52,370 INFO codegen.CodeGenerator: Code generated in 8.903308 ms\n",
      "2023-02-01 07:05:52,396 INFO codegen.CodeGenerator: Code generated in 14.466786 ms\n",
      "2023-02-01 07:05:52,409 INFO codegen.CodeGenerator: Code generated in 8.882091 ms\n",
      "2023-02-01 07:05:52,435 INFO codegen.CodeGenerator: Code generated in 13.647842 ms\n",
      "2023-02-01 07:05:52,448 INFO codegen.CodeGenerator: Code generated in 8.778739 ms\n",
      "2023-02-01 07:05:53,696 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,696 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:05:53,696 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,699 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,699 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:05:53,699 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,701 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,701 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:05:53,701 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,704 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,704 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:05:53,704 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,706 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,706 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:05:53,706 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,708 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,708 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:05:53,709 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,711 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,711 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:05:53,711 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,713 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,713 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:05:53,713 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,716 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,716 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:05:53,716 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,718 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,718 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:05:53,718 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,721 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,721 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:05:53,721 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,723 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,723 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:05:53,723 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,725 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,725 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:05:53,726 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,728 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,728 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:05:53,728 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,730 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,730 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:05:53,730 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,732 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:05:53,732 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:05:53,733 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:05:53,944 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:53,959 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:53,960 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:53,960 INFO spark.SparkContext: Created broadcast 39 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:53,961 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Registering RDD 180 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Got map stage job 18 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:53,982 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[180] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:53,997 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 98.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:53,998 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:53,999 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 33.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:53,999 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:53,999 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[180] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:53,999 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,000 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 35) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:54,001 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 36) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:54,011 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.200.140.94:41276 (size: 33.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:54,013 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.200.136.126:44915 (size: 33.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,039 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,055 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,055 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,056 INFO spark.SparkContext: Created broadcast 41 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,056 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,074 INFO scheduler.DAGScheduler: Registering RDD 185 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 17\n",
      "2023-02-01 07:05:54,074 INFO scheduler.DAGScheduler: Got map stage job 19 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,074 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 19 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,074 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,074 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,075 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[185] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,081 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,082 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,083 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,083 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,088 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[185] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,088 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,089 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 37) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:54,094 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:54,100 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,110 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,125 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,126 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,126 INFO spark.SparkContext: Created broadcast 43 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,127 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,136 INFO scheduler.DAGScheduler: Registering RDD 190 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
      "2023-02-01 07:05:54,136 INFO scheduler.DAGScheduler: Got map stage job 20 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,136 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,136 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,137 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,137 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[190] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,142 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,144 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,145 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,145 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,146 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[190] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,146 INFO scheduler.TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,168 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,172 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:54,183 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,183 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,184 INFO spark.SparkContext: Created broadcast 45 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,185 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Registering RDD 195 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Got map stage job 21 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,194 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[195] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,199 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,201 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,201 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,202 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,202 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[195] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,202 INFO scheduler.TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,223 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,239 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,240 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,240 INFO spark.SparkContext: Created broadcast 47 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,241 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,260 INFO scheduler.DAGScheduler: Registering RDD 200 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 20\n",
      "2023-02-01 07:05:54,260 INFO scheduler.DAGScheduler: Got map stage job 22 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,260 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 22 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,260 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,260 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,261 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[200] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,266 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,268 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,268 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,268 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,269 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[200] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,269 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,290 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,296 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,305 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,306 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,306 INFO spark.SparkContext: Created broadcast 49 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,307 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,316 INFO scheduler.DAGScheduler: Registering RDD 205 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
      "2023-02-01 07:05:54,317 INFO scheduler.DAGScheduler: Got map stage job 23 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,317 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 23 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,317 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,317 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,317 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[205] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,324 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,326 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,327 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,327 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,327 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[205] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,327 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,348 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,413 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,414 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,414 INFO spark.SparkContext: Created broadcast 51 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,415 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,422 INFO scheduler.DAGScheduler: Registering RDD 210 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
      "2023-02-01 07:05:54,422 INFO scheduler.DAGScheduler: Got map stage job 24 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,422 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,422 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,422 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,423 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[210] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,441 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,443 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,451 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,452 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,452 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[210] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,452 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,455 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,470 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,471 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,471 INFO spark.SparkContext: Created broadcast 53 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,472 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,486 INFO scheduler.DAGScheduler: Registering RDD 215 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 23\n",
      "2023-02-01 07:05:54,486 INFO scheduler.DAGScheduler: Got map stage job 25 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,486 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,486 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,486 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,487 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[215] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,492 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,494 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,494 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,495 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,495 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[215] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,495 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,515 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,542 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,546 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,546 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,547 INFO spark.SparkContext: Created broadcast 55 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,547 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,554 INFO scheduler.DAGScheduler: Registering RDD 220 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 24\n",
      "2023-02-01 07:05:54,554 INFO scheduler.DAGScheduler: Got map stage job 26 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,554 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 26 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,554 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,555 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,555 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[220] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,559 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,561 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,561 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,562 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,562 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[220] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,562 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,569 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:54,583 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,585 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,602 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,602 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,604 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,605 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,605 INFO spark.SparkContext: Created broadcast 57 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,606 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,643 INFO scheduler.DAGScheduler: Registering RDD 225 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 25\n",
      "2023-02-01 07:05:54,644 INFO scheduler.DAGScheduler: Got map stage job 27 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,644 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 27 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,644 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,644 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,645 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[225] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,657 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,659 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,659 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,660 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,660 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[225] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,660 INFO scheduler.TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,669 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:54,673 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,673 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,675 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.200.140.94:41276 in memory (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:54,685 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.200.136.126:44915 in memory (size: 34.2 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,688 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,689 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,689 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 33.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,689 INFO spark.SparkContext: Created broadcast 59 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,690 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,690 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.136.126:44915 in memory (size: 33.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,690 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.136.127:43545 in memory (size: 33.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:54,696 INFO scheduler.DAGScheduler: Registering RDD 230 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 26\n",
      "2023-02-01 07:05:54,697 INFO scheduler.DAGScheduler: Got map stage job 28 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,697 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 28 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,697 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,697 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,697 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[230] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,701 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,703 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,703 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,704 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,704 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[230] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,704 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,725 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,741 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,741 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,742 INFO spark.SparkContext: Created broadcast 61 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,743 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Registering RDD 235 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 27\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Got map stage job 29 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,749 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[235] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,753 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,767 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,774 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,775 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,775 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[235] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,775 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,791 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,806 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,806 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,807 INFO spark.SparkContext: Created broadcast 63 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,808 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,814 INFO scheduler.DAGScheduler: Registering RDD 240 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 28\n",
      "2023-02-01 07:05:54,814 INFO scheduler.DAGScheduler: Got map stage job 30 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,814 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 30 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,814 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,814 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,815 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[240] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,819 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,820 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,821 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,821 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,821 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[240] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,821 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,843 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,859 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,859 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,860 INFO spark.SparkContext: Created broadcast 65 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,861 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Registering RDD 245 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 29\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Got map stage job 31 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 31 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,867 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[245] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,872 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,873 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,874 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,874 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,874 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[245] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,874 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,910 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,925 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,925 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,926 INFO spark.SparkContext: Created broadcast 67 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,927 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,933 INFO scheduler.DAGScheduler: Registering RDD 250 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 30\n",
      "2023-02-01 07:05:54,933 INFO scheduler.DAGScheduler: Got map stage job 32 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,933 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 32 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,933 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,933 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,934 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[250] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,938 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,940 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,940 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,940 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,941 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[250] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,941 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:54,961 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,977 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,977 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,978 INFO spark.SparkContext: Created broadcast 69 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:05:54,979 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:05:54,985 INFO scheduler.DAGScheduler: Registering RDD 255 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 31\n",
      "2023-02-01 07:05:54,985 INFO scheduler.DAGScheduler: Got map stage job 33 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:05:54,985 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:05:54,985 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:05:54,986 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:05:54,986 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[255] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:05:54,989 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 100.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,991 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:05:54,991 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:05:54,992 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:05:54,992 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[255] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:05:54,992 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:05:56,108 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 38) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:56,109 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 36) in 2107 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:56,191 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:56,286 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:56,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 39) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:56,693 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 37) in 2603 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:56,705 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:05:56,795 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:57,574 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 40) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:57,574 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 35) in 3574 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:57,574 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:57,575 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (summary at NativeMethodAccessorImpl.java:0) finished in 3.592 s\n",
      "2023-02-01 07:05:57,575 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:57,575 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-02-01 07:05:57,575 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:57,575 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:57,587 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:57,678 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:57,999 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 41) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:57,999 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 38) in 1891 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:05:57,999 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:58,000 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (summary at NativeMethodAccessorImpl.java:0) finished in 3.925 s\n",
      "2023-02-01 07:05:58,000 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:58,000 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-02-01 07:05:58,000 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:58,000 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:58,092 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.200.136.126:44915 (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:58,188 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:58,506 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 42) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:58,506 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 39) in 1814 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:05:58,589 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.200.136.127:43545 (size: 34.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:58,800 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:05:59,094 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 43) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:59,095 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 40) in 1521 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:05:59,095 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:05:59,095 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (summary at NativeMethodAccessorImpl.java:0) finished in 4.958 s\n",
      "2023-02-01 07:05:59,095 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:05:59,095 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-02-01 07:05:59,095 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:05:59,095 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:05:59,173 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:59,270 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:05:59,497 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 44) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:05:59,497 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 41) in 1499 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:05:59,506 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:05:59,792 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:00,613 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 45) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:00,613 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 42) in 2107 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:06:00,613 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:00,614 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (summary at NativeMethodAccessorImpl.java:0) finished in 6.419 s\n",
      "2023-02-01 07:06:00,614 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:00,614 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-02-01 07:06:00,614 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:00,614 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:00,687 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:00,703 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:00,985 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 46) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:00,986 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 43) in 1892 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:01,072 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:01,277 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:01,705 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 47) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:01,705 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 44) in 2209 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:06:01,705 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:01,706 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (summary at NativeMethodAccessorImpl.java:0) finished in 7.445 s\n",
      "2023-02-01 07:06:01,706 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:01,706 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-02-01 07:06:01,706 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:01,706 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:01,720 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:01,785 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.200.136.127:43545 in memory (size: 34.2 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:01,786 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.200.136.126:44915 in memory (size: 34.2 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:01,791 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:01,793 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:01,802 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:01,869 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:01,872 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:01,885 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:06:01,890 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:01,896 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:01,988 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:01,991 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:01,991 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:02,487 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 48) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:02,487 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 45) in 1874 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:06:02,498 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:02,875 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 49) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:02,876 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 46) in 1891 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:06:02,876 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:02,876 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (summary at NativeMethodAccessorImpl.java:0) finished in 8.559 s\n",
      "2023-02-01 07:06:02,876 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:02,876 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:02,876 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:02,876 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:02,884 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:02,885 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:06:02,979 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:03,598 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 25.0 (TID 50) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:03,598 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 47) in 1893 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:06:03,685 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:03,893 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:04,894 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 51) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:04,894 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 48) in 2407 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:06:04,894 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:04,895 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (summary at NativeMethodAccessorImpl.java:0) finished in 10.472 s\n",
      "2023-02-01 07:06:04,895 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:04,895 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:04,895 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:04,895 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:04,903 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:04,998 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:05,273 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 52) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:05,273 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 49) in 2398 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:05,368 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:05,686 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:05,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 53) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:05,692 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 25.0 (TID 50) in 2094 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:06:05,692 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:05,693 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (summary at NativeMethodAccessorImpl.java:0) finished in 11.206 s\n",
      "2023-02-01 07:06:05,693 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:05,693 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:05,693 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:05,693 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:05,701 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:05,795 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:07,081 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 54) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:07,082 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 52) in 1809 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:07,168 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:07,271 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:07,299 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 55) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:07,299 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 53) in 1607 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:06:07,301 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 56) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:07,301 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 51) in 2407 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:06:07,301 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:07,304 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (summary at NativeMethodAccessorImpl.java:0) finished in 12.749 s\n",
      "2023-02-01 07:06:07,304 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:07,304 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:07,304 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:07,304 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:07,309 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:07,388 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:06:07,484 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:07,685 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:08,483 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 57) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:08,483 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 54) in 1402 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:06:08,483 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:08,484 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (summary at NativeMethodAccessorImpl.java:0) finished in 13.839 s\n",
      "2023-02-01 07:06:08,484 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:08,484 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:08,484 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:08,484 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:08,492 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:08,580 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:08,992 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 58) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:08,992 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 56) in 1691 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:06:09,004 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:06:09,385 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:10,287 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 59) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:10,288 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 55) in 2989 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:06:10,288 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:10,288 INFO scheduler.DAGScheduler: ShuffleMapStage 28 (summary at NativeMethodAccessorImpl.java:0) finished in 15.590 s\n",
      "2023-02-01 07:06:10,288 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:10,289 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 29)\n",
      "2023-02-01 07:06:10,289 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:10,289 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:10,297 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:10,392 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:10,571 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 60) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:10,571 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 57) in 2088 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:10,675 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.200.140.94:41276 (size: 34.3 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:10,878 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:11,409 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 61) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:11,410 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 58) in 2418 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:06:11,410 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:11,410 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (summary at NativeMethodAccessorImpl.java:0) finished in 16.660 s\n",
      "2023-02-01 07:06:11,410 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:11,410 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-02-01 07:06:11,410 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:11,410 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:11,490 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.200.136.127:43545 (size: 34.2 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:11,588 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:12,280 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 62) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:12,280 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 60) in 1710 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:12,288 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.200.140.94:41276 (size: 34.2 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:12,377 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:06:12,584 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 63) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:12,585 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 59) in 2298 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:06:12,585 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:12,585 INFO scheduler.DAGScheduler: ShuffleMapStage 30 (summary at NativeMethodAccessorImpl.java:0) finished in 17.770 s\n",
      "2023-02-01 07:06:12,585 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:12,585 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-02-01 07:06:12,585 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:12,585 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:12,596 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.200.136.126:44915 (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:12,690 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:13,500 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 64) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:13,501 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 61) in 2092 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:06:13,584 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.200.136.127:43545 (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:13,676 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 65) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:13,677 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 62) in 1396 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:06:13,677 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:13,677 INFO scheduler.DAGScheduler: ShuffleMapStage 31 (summary at NativeMethodAccessorImpl.java:0) finished in 18.809 s\n",
      "2023-02-01 07:06:13,677 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:13,677 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-02-01 07:06:13,677 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:13,677 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:13,684 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.200.140.94:41276 (size: 34.2 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:06:13,774 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:06:13,788 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:14,392 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 66) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:14,393 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 63) in 1809 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:06:14,402 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.200.136.126:44915 (size: 34.2 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:14,602 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:15,482 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 65) in 1806 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:06:15,486 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 64) in 1986 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:06:15,486 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:15,487 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (summary at NativeMethodAccessorImpl.java:0) finished in 20.553 s\n",
      "2023-02-01 07:06:15,487 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:15,487 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 33)\n",
      "2023-02-01 07:06:15,487 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:15,487 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:16,392 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 66) in 2000 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:06:16,392 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:06:16,393 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (summary at NativeMethodAccessorImpl.java:0) finished in 21.407 s\n",
      "2023-02-01 07:06:16,393 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:06:16,393 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:06:16,393 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:06:16,393 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:06:16,421 INFO adaptive.ShufflePartitionsUtil: For shuffle(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31), advisory target size: 67108864, actual target size 3159149, minimum partition size: 1048576\n",
      "2023-02-01 07:06:16,429 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,431 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:06:16,480 INFO codegen.CodeGenerator: Code generated in 13.187965 ms\n",
      "2023-02-01 07:06:16,485 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:16,492 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,495 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:16,495 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,504 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,505 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,505 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,507 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,509 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,509 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:06:16,517 INFO codegen.CodeGenerator: Code generated in 14.449272 ms\n",
      "2023-02-01 07:06:16,521 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:06:16,522 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,522 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,532 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,533 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.200.140.94:41276 in memory (size: 34.2 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:16,534 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.200.136.126:44915 in memory (size: 34.2 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,537 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:16,538 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:16,543 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,549 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:16,550 INFO codegen.CodeGenerator: Code generated in 13.323554 ms\n",
      "2023-02-01 07:06:16,560 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,578 INFO codegen.CodeGenerator: Code generated in 12.536519 ms\n",
      "2023-02-01 07:06:16,591 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.200.136.126:44915 in memory (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:16,609 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,609 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:16,610 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:06:16,616 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:16,617 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.200.136.127:43545 in memory (size: 34.2 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:06:16,618 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.200.140.94:41276 in memory (size: 34.2 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:06:16,624 INFO codegen.CodeGenerator: Code generated in 12.657013 ms\n",
      "2023-02-01 07:06:16,651 INFO codegen.CodeGenerator: Code generated in 11.961913 ms\n",
      "2023-02-01 07:06:16,678 INFO codegen.CodeGenerator: Code generated in 12.417643 ms\n",
      "2023-02-01 07:06:16,707 INFO codegen.CodeGenerator: Code generated in 11.504465 ms\n",
      "2023-02-01 07:06:16,733 INFO codegen.CodeGenerator: Code generated in 11.876062 ms\n",
      "2023-02-01 07:06:16,760 INFO codegen.CodeGenerator: Code generated in 11.732164 ms\n",
      "2023-02-01 07:06:16,790 INFO codegen.CodeGenerator: Code generated in 11.709008 ms\n",
      "2023-02-01 07:06:16,815 INFO codegen.CodeGenerator: Code generated in 11.366793 ms\n",
      "2023-02-01 07:06:16,841 INFO codegen.CodeGenerator: Code generated in 11.221758 ms\n",
      "2023-02-01 07:06:16,866 INFO codegen.CodeGenerator: Code generated in 11.347771 ms\n",
      "2023-02-01 07:06:16,890 INFO codegen.CodeGenerator: Code generated in 11.065651 ms\n",
      "2023-02-01 07:06:16,914 INFO codegen.CodeGenerator: Code generated in 10.505984 ms\n",
      "2023-02-01 07:06:17,000 INFO scheduler.DAGScheduler: Registering RDD 322 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 32\n",
      "2023-02-01 07:06:17,000 INFO scheduler.DAGScheduler: Got map stage job 34 (summary at NativeMethodAccessorImpl.java:0) with 48 output partitions\n",
      "2023-02-01 07:06:17,000 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 50 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:06:17,000 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 47, ShuffleMapStage 41, ShuffleMapStage 48, ShuffleMapStage 34, ShuffleMapStage 49, ShuffleMapStage 35, ShuffleMapStage 42, ShuffleMapStage 36, ShuffleMapStage 43, ShuffleMapStage 44)\n",
      "2023-02-01 07:06:17,001 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:06:17,001 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[322] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:06:17,177 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1669.6 KiB\n",
      "2023-02-01 07:06:17,178 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 1669.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:06:17,183 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 409.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:06:17,183 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 409.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:17,184 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:06:17,186 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[322] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-02-01 07:06:17,186 INFO scheduler.TaskSchedulerImpl: Adding task set 50.0 with 48 tasks resource profile 0\n",
      "2023-02-01 07:06:17,193 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 50.0 (TID 67) (10.200.136.127, executor 3, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:17,193 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 68) (10.200.140.94, executor 1, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:17,193 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 50.0 (TID 69) (10.200.136.126, executor 2, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:17,211 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.200.140.94:41276 (size: 409.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:06:17,212 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.200.136.127:43545 (size: 409.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:06:17,214 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.200.136.126:44915 (size: 409.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:06:17,684 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:17,786 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:17,788 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:25,202 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 50.0 (TID 70) (10.200.136.127, executor 3, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:25,203 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 50.0 (TID 67) in 8013 ms on 10.200.136.127 (executor 3) (1/48)\n",
      "2023-02-01 07:06:28,496 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 50.0 (TID 71) (10.200.136.127, executor 3, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:28,496 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 50.0 (TID 70) in 3294 ms on 10.200.136.127 (executor 3) (2/48)\n",
      "2023-02-01 07:06:29,983 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 50.0 (TID 72) (10.200.140.94, executor 1, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:29,984 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 68) in 12791 ms on 10.200.140.94 (executor 1) (3/48)\n",
      "2023-02-01 07:06:30,396 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 50.0 (TID 73) (10.200.136.126, executor 2, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:30,396 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 50.0 (TID 69) in 13203 ms on 10.200.136.126 (executor 2) (4/48)\n",
      "2023-02-01 07:06:30,787 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:31,287 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 50.0 (TID 74) (10.200.136.127, executor 3, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:31,288 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 50.0 (TID 71) in 2793 ms on 10.200.136.127 (executor 3) (5/48)\n",
      "2023-02-01 07:06:31,500 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:33,888 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 50.0 (TID 75) (10.200.136.126, executor 2, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:33,888 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 50.0 (TID 73) in 3492 ms on 10.200.136.126 (executor 2) (6/48)\n",
      "2023-02-01 07:06:34,189 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 50.0 (TID 76) (10.200.136.127, executor 3, partition 7, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:34,190 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 50.0 (TID 74) in 2903 ms on 10.200.136.127 (executor 3) (7/48)\n",
      "2023-02-01 07:06:36,300 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 50.0 (TID 77) (10.200.136.127, executor 3, partition 8, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:36,301 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 50.0 (TID 76) in 2112 ms on 10.200.136.127 (executor 3) (8/48)\n",
      "2023-02-01 07:06:36,497 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 50.0 (TID 78) (10.200.136.126, executor 2, partition 11, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:36,498 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 50.0 (TID 75) in 2611 ms on 10.200.136.126 (executor 2) (9/48)\n",
      "2023-02-01 07:06:36,780 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 50.0 (TID 79) (10.200.140.94, executor 1, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:36,781 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 50.0 (TID 72) in 6798 ms on 10.200.140.94 (executor 1) (10/48)\n",
      "2023-02-01 07:06:36,984 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:38,201 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 50.0 (TID 80) (10.200.136.127, executor 3, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:38,201 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 50.0 (TID 77) in 1901 ms on 10.200.136.127 (executor 3) (11/48)\n",
      "2023-02-01 07:06:38,400 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:39,102 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 50.0 (TID 81) (10.200.136.126, executor 2, partition 13, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:39,102 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 50.0 (TID 78) in 2605 ms on 10.200.136.126 (executor 2) (12/48)\n",
      "2023-02-01 07:06:39,298 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:39,787 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 50.0 (TID 82) (10.200.140.94, executor 1, partition 14, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:39,787 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 50.0 (TID 79) in 3007 ms on 10.200.140.94 (executor 1) (13/48)\n",
      "2023-02-01 07:06:40,789 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 50.0 (TID 83) (10.200.136.127, executor 3, partition 16, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:40,789 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 50.0 (TID 80) in 2588 ms on 10.200.136.127 (executor 3) (14/48)\n",
      "2023-02-01 07:06:41,691 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 50.0 (TID 84) (10.200.136.126, executor 2, partition 18, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:41,691 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 50.0 (TID 81) in 2589 ms on 10.200.136.126 (executor 2) (15/48)\n",
      "2023-02-01 07:06:41,888 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:42,071 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 50.0 (TID 85) (10.200.140.94, executor 1, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:42,071 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 50.0 (TID 82) in 2285 ms on 10.200.140.94 (executor 1) (16/48)\n",
      "2023-02-01 07:06:42,275 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:42,985 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 50.0 (TID 86) (10.200.136.127, executor 3, partition 19, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:42,986 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 50.0 (TID 83) in 2198 ms on 10.200.136.127 (executor 3) (17/48)\n",
      "2023-02-01 07:06:43,195 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:44,187 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 50.0 (TID 87) (10.200.136.126, executor 2, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:44,188 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 50.0 (TID 84) in 2498 ms on 10.200.136.126 (executor 2) (18/48)\n",
      "2023-02-01 07:06:44,372 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 50.0 (TID 88) (10.200.140.94, executor 1, partition 21, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:44,373 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 50.0 (TID 85) in 2303 ms on 10.200.140.94 (executor 1) (19/48)\n",
      "2023-02-01 07:06:44,573 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:45,311 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 50.0 (TID 89) (10.200.136.127, executor 3, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:45,311 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 50.0 (TID 86) in 2326 ms on 10.200.136.127 (executor 3) (20/48)\n",
      "2023-02-01 07:06:45,506 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:46,402 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 50.0 (TID 90) (10.200.136.126, executor 2, partition 22, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:46,402 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 50.0 (TID 87) in 2215 ms on 10.200.136.126 (executor 2) (21/48)\n",
      "2023-02-01 07:06:46,600 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:47,069 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 50.0 (TID 91) (10.200.140.94, executor 1, partition 23, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:47,069 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 50.0 (TID 88) in 2697 ms on 10.200.140.94 (executor 1) (22/48)\n",
      "2023-02-01 07:06:47,687 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 50.0 (TID 92) (10.200.136.127, executor 3, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:47,687 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 50.0 (TID 89) in 2376 ms on 10.200.136.127 (executor 3) (23/48)\n",
      "2023-02-01 07:06:49,075 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 50.0 (TID 93) (10.200.140.94, executor 1, partition 26, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:49,075 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 50.0 (TID 91) in 2007 ms on 10.200.140.94 (executor 1) (24/48)\n",
      "2023-02-01 07:06:49,275 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:49,305 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 50.0 (TID 94) (10.200.136.126, executor 2, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:49,306 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 50.0 (TID 90) in 2905 ms on 10.200.136.126 (executor 2) (25/48)\n",
      "2023-02-01 07:06:49,500 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:49,694 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 50.0 (TID 95) (10.200.136.127, executor 3, partition 30, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:49,695 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 50.0 (TID 92) in 2009 ms on 10.200.136.127 (executor 3) (26/48)\n",
      "2023-02-01 07:06:49,893 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:51,488 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 50.0 (TID 96) (10.200.140.94, executor 1, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:51,489 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 50.0 (TID 93) in 2415 ms on 10.200.140.94 (executor 1) (27/48)\n",
      "2023-02-01 07:06:51,695 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 50.0 (TID 97) (10.200.136.126, executor 2, partition 29, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:51,695 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 50.0 (TID 94) in 2390 ms on 10.200.136.126 (executor 2) (28/48)\n",
      "2023-02-01 07:06:51,768 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:52,002 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 50.0 (TID 98) (10.200.136.127, executor 3, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:52,002 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 50.0 (TID 95) in 2308 ms on 10.200.136.127 (executor 3) (29/48)\n",
      "2023-02-01 07:06:53,793 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 50.0 (TID 99) (10.200.140.94, executor 1, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:53,793 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 50.0 (TID 96) in 2305 ms on 10.200.140.94 (executor 1) (30/48)\n",
      "2023-02-01 07:06:53,797 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 50.0 (TID 100) (10.200.136.126, executor 2, partition 32, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:53,798 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 50.0 (TID 97) in 2103 ms on 10.200.136.126 (executor 2) (31/48)\n",
      "2023-02-01 07:06:53,987 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:53,990 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:54,399 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 50.0 (TID 101) (10.200.136.127, executor 3, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:54,399 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 50.0 (TID 98) in 2398 ms on 10.200.136.127 (executor 3) (32/48)\n",
      "2023-02-01 07:06:54,598 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:56,098 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 50.0 (TID 102) (10.200.136.126, executor 2, partition 36, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:56,099 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 50.0 (TID 100) in 2302 ms on 10.200.136.126 (executor 2) (33/48)\n",
      "2023-02-01 07:06:56,295 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.200.136.126:48696\n",
      "2023-02-01 07:06:56,380 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 50.0 (TID 103) (10.200.140.94, executor 1, partition 35, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:56,381 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 50.0 (TID 99) in 2588 ms on 10.200.140.94 (executor 1) (34/48)\n",
      "2023-02-01 07:06:56,433 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 33.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:06:56,467 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.200.140.94:41276 in memory (size: 33.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:06:56,486 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.200.136.126:44915 in memory (size: 33.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:06:56,700 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 50.0 (TID 104) (10.200.136.127, executor 3, partition 39, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:56,701 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 50.0 (TID 101) in 2302 ms on 10.200.136.127 (executor 3) (35/48)\n",
      "2023-02-01 07:06:56,897 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.200.136.127:35826\n",
      "2023-02-01 07:06:58,389 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 50.0 (TID 105) (10.200.140.94, executor 1, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:58,389 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 50.0 (TID 103) in 2009 ms on 10.200.140.94 (executor 1) (36/48)\n",
      "2023-02-01 07:06:58,585 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.200.140.94:57642\n",
      "2023-02-01 07:06:58,694 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 50.0 (TID 106) (10.200.136.126, executor 2, partition 38, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:58,695 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 50.0 (TID 102) in 2597 ms on 10.200.136.126 (executor 2) (37/48)\n",
      "2023-02-01 07:06:59,093 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 50.0 (TID 107) (10.200.136.127, executor 3, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:06:59,094 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 50.0 (TID 104) in 2394 ms on 10.200.136.127 (executor 3) (38/48)\n",
      "2023-02-01 07:07:00,685 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 50.0 (TID 108) (10.200.140.94, executor 1, partition 41, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:00,685 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 50.0 (TID 105) in 2297 ms on 10.200.140.94 (executor 1) (39/48)\n",
      "2023-02-01 07:07:00,808 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 50.0 (TID 109) (10.200.136.126, executor 2, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:00,808 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 50.0 (TID 106) in 2114 ms on 10.200.136.126 (executor 2) (40/48)\n",
      "2023-02-01 07:07:00,876 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.200.140.94:57642\n",
      "2023-02-01 07:07:01,085 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.200.136.126:48696\n",
      "2023-02-01 07:07:01,185 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 50.0 (TID 110) (10.200.136.127, executor 3, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:01,185 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 50.0 (TID 107) in 2092 ms on 10.200.136.127 (executor 3) (41/48)\n",
      "2023-02-01 07:07:01,302 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.200.136.127:35826\n",
      "2023-02-01 07:07:03,088 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 50.0 (TID 111) (10.200.140.94, executor 1, partition 45, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:03,088 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 50.0 (TID 108) in 2404 ms on 10.200.140.94 (executor 1) (42/48)\n",
      "2023-02-01 07:07:03,108 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 50.0 (TID 112) (10.200.136.126, executor 2, partition 44, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:03,108 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 50.0 (TID 109) in 2301 ms on 10.200.136.126 (executor 2) (43/48)\n",
      "2023-02-01 07:07:03,279 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.200.140.94:57642\n",
      "2023-02-01 07:07:03,800 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 50.0 (TID 110) in 2615 ms on 10.200.136.127 (executor 3) (44/48)\n",
      "2023-02-01 07:07:05,187 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 50.0 (TID 113) (10.200.136.126, executor 2, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:05,187 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 50.0 (TID 112) in 2080 ms on 10.200.136.126 (executor 2) (45/48)\n",
      "2023-02-01 07:07:05,386 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.200.136.126:48696\n",
      "2023-02-01 07:07:05,478 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 50.0 (TID 114) (10.200.140.94, executor 1, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:05,478 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 50.0 (TID 111) in 2391 ms on 10.200.140.94 (executor 1) (46/48)\n",
      "2023-02-01 07:07:07,393 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 50.0 (TID 113) in 2205 ms on 10.200.136.126 (executor 2) (47/48)\n",
      "2023-02-01 07:07:07,595 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 50.0 (TID 114) in 2117 ms on 10.200.140.94 (executor 1) (48/48)\n",
      "2023-02-01 07:07:07,595 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:07:07,596 INFO scheduler.DAGScheduler: ShuffleMapStage 50 (summary at NativeMethodAccessorImpl.java:0) finished in 50.539 s\n",
      "2023-02-01 07:07:07,596 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:07:07,596 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:07:07,596 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:07:07,596 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:07:07,756 INFO spark.SparkContext: Starting job: summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:07:07,759 INFO scheduler.DAGScheduler: Got job 35 (summary at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-02-01 07:07:07,759 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:07:07,759 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)\n",
      "2023-02-01 07:07:07,759 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:07:07,760 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (SQLExecutionRDD[325] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:07:07,827 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1826.4 KiB\n",
      "2023-02-01 07:07:07,827 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 1826.5 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:07:07,833 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 417.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:07:07,833 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 417.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:07:07,834 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:07:07,834 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (SQLExecutionRDD[325] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 07:07:07,834 INFO scheduler.TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0\n",
      "2023-02-01 07:07:07,835 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 115) (10.200.140.94, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:07:07,876 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.200.140.94:41276 (size: 417.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:07:08,079 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.200.140.94:57642\n",
      "2023-02-01 07:07:18,573 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 115) in 10738 ms on 10.200.140.94 (executor 1) (1/1)\n",
      "2023-02-01 07:07:18,573 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:07:18,574 INFO scheduler.DAGScheduler: ResultStage 68 (summary at NativeMethodAccessorImpl.java:0) finished in 10.814 s\n",
      "2023-02-01 07:07:18,574 INFO scheduler.DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:07:18,574 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished\n",
      "2023-02-01 07:07:18,574 INFO scheduler.DAGScheduler: Job 35 finished: summary at NativeMethodAccessorImpl.java:0, took 10.818434 s\n",
      "2023-02-01 07:07:18,606 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2023-02-01 07:07:18,628 INFO codegen.CodeGenerator: Code generated in 12.793025 ms\n",
      "2023-02-01 07:07:19,004 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 417.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:07:19,006 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.200.140.94:41276 in memory (size: 417.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:07:19,010 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 34.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:07:19,011 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.200.140.94:41276 in memory (size: 34.3 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:07:19,012 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.200.136.127:43545 in memory (size: 34.3 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:07:19,015 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 409.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:07:19,016 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.200.140.94:41276 in memory (size: 409.9 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:07:19,017 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.200.136.126:44915 in memory (size: 409.9 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:07:19,017 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.200.136.127:43545 in memory (size: 409.9 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,081 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,083 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,083 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:09:23,088 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,088 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,095 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,101 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,102 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,102 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:09:23,107 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,107 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,108 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,117 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,119 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:09:23,119 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,130 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,130 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,132 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,173 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,173 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,173 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,183 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:09:23,183 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,204 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,269 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,270 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,272 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,275 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,278 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,280 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:09:23,281 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:09:23,286 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,286 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:09:23,286 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,375 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,376 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,376 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:09:23,378 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,379 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,380 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,385 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,385 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,385 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.200.140.94:41276 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:23,398 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,400 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:23,401 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:23,474 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:23,474 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.200.136.126:44915 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:23,474 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.200.136.127:43545 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:36,319 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,319 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:09:36,319 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,330 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,330 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:09:36,330 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,340 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,341 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:09:36,341 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,351 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,351 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:09:36,351 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,362 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,362 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:09:36,362 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,372 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,372 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:09:36,372 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,382 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,382 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:09:36,383 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,393 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,393 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:09:36,393 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,403 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,403 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:09:36,404 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,413 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,414 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:09:36,414 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,424 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,424 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:09:36,424 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,434 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,434 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:09:36,434 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,444 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,444 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:09:36,444 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,454 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,454 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:09:36,455 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,465 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,465 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:09:36,465 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:36,475 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:09:36,475 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:09:36,475 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-02-01 07:09:37,543 INFO codegen.CodeGenerator: Code generated in 85.160306 ms\n",
      "2023-02-01 07:09:37,545 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,561 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,561 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:37,562 INFO spark.SparkContext: Created broadcast 73 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:09:37,563 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:09:37,590 INFO scheduler.DAGScheduler: Registering RDD 329 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 33\n",
      "2023-02-01 07:09:37,591 INFO scheduler.DAGScheduler: Got map stage job 36 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-02-01 07:09:37,591 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:09:37,591 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:09:37,591 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:09:37,591 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[329] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:09:37,594 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 123.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,596 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 39.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,596 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 39.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:37,597 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:09:37,597 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[329] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:09:37,597 INFO scheduler.TaskSchedulerImpl: Adding task set 69.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:09:37,598 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 116) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:37,598 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 69.0 (TID 117) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:37,620 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.200.136.127:43545 (size: 39.9 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:37,687 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.200.136.126:44915 (size: 39.9 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:37,689 INFO codegen.CodeGenerator: Code generated in 68.270444 ms\n",
      "2023-02-01 07:09:37,691 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,706 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,706 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:37,707 INFO spark.SparkContext: Created broadcast 75 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:09:37,708 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:09:37,726 INFO scheduler.DAGScheduler: Registering RDD 333 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 34\n",
      "2023-02-01 07:09:37,726 INFO scheduler.DAGScheduler: Got map stage job 37 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-02-01 07:09:37,726 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 70 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:09:37,727 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:09:37,727 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:09:37,727 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 70 (MapPartitionsRDD[333] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:09:37,730 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 124.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,732 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 40.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:37,732 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 40.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:37,732 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:09:37,733 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[333] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:09:37,733 INFO scheduler.TaskSchedulerImpl: Adding task set 70.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:09:37,734 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 70.0 (TID 118) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:37,743 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.200.140.94:41276 (size: 40.1 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:38,286 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:09:38,301 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:38,395 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:40,293 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 70.0 (TID 119) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:40,293 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 69.0 (TID 117) in 2695 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:09:40,384 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.200.136.127:43545 (size: 40.1 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:40,471 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 70.0 (TID 118) in 2738 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:09:40,705 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:09:40,886 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 116) in 3287 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:09:40,886 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:09:40,886 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 3.295 s\n",
      "2023-02-01 07:09:40,886 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:09:40,886 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 70)\n",
      "2023-02-01 07:09:40,886 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:09:40,886 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:09:41,086 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 39.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:41,087 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.200.136.126:44915 in memory (size: 39.9 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:09:41,087 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.200.136.127:43545 in memory (size: 39.9 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:42,295 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 70.0 (TID 119) in 2002 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:09:42,295 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:09:42,296 INFO scheduler.DAGScheduler: ShuffleMapStage 70 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 4.568 s\n",
      "2023-02-01 07:09:42,296 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:09:42,296 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:09:42,296 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:09:42,296 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:09:42,541 INFO adaptive.ShufflePartitionsUtil: For shuffle(33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34), advisory target size: 67108864, actual target size 1507112, minimum partition size: 1048576\n",
      "2023-02-01 07:09:43,013 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,070 INFO codegen.CodeGenerator: Code generated in 34.08573 ms\n",
      "2023-02-01 07:09:43,106 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,154 INFO codegen.CodeGenerator: Code generated in 32.280946 ms\n",
      "2023-02-01 07:09:43,162 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,211 INFO codegen.CodeGenerator: Code generated in 33.283892 ms\n",
      "2023-02-01 07:09:43,222 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,268 INFO codegen.CodeGenerator: Code generated in 31.072071 ms\n",
      "2023-02-01 07:09:43,275 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,322 INFO codegen.CodeGenerator: Code generated in 31.668323 ms\n",
      "2023-02-01 07:09:43,329 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,375 INFO codegen.CodeGenerator: Code generated in 30.82923 ms\n",
      "2023-02-01 07:09:43,382 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,430 INFO codegen.CodeGenerator: Code generated in 32.367241 ms\n",
      "2023-02-01 07:09:43,437 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,482 INFO codegen.CodeGenerator: Code generated in 30.649479 ms\n",
      "2023-02-01 07:09:43,489 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,535 INFO codegen.CodeGenerator: Code generated in 30.369213 ms\n",
      "2023-02-01 07:09:43,542 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,587 INFO codegen.CodeGenerator: Code generated in 30.429906 ms\n",
      "2023-02-01 07:09:43,594 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,632 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 40.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:43,633 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.200.136.127:43545 in memory (size: 40.1 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:43,633 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.200.140.94:41276 in memory (size: 40.1 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:09:43,662 INFO codegen.CodeGenerator: Code generated in 27.568116 ms\n",
      "2023-02-01 07:09:43,668 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,710 INFO codegen.CodeGenerator: Code generated in 26.965026 ms\n",
      "2023-02-01 07:09:43,717 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,759 INFO codegen.CodeGenerator: Code generated in 27.175889 ms\n",
      "2023-02-01 07:09:43,766 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,807 INFO codegen.CodeGenerator: Code generated in 26.349456 ms\n",
      "2023-02-01 07:09:43,813 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,854 INFO codegen.CodeGenerator: Code generated in 26.60551 ms\n",
      "2023-02-01 07:09:43,861 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-02-01 07:09:43,901 INFO codegen.CodeGenerator: Code generated in 25.673147 ms\n",
      "2023-02-01 07:09:44,015 INFO scheduler.DAGScheduler: Registering RDD 384 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 35\n",
      "2023-02-01 07:09:44,015 INFO scheduler.DAGScheduler: Got map stage job 38 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 48 output partitions\n",
      "2023-02-01 07:09:44,015 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 73 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:09:44,015 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 71, ShuffleMapStage 72)\n",
      "2023-02-01 07:09:44,016 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:09:44,016 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 73 (MapPartitionsRDD[384] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:09:44,101 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "2023-02-01 07:09:44,102 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 2.3 MiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:44,109 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 640.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:09:44,110 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 640.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:09:44,110 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:09:44,111 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 73 (MapPartitionsRDD[384] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-02-01 07:09:44,111 INFO scheduler.TaskSchedulerImpl: Adding task set 73.0 with 48 tasks resource profile 0\n",
      "2023-02-01 07:09:44,112 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 120) (10.200.136.126, executor 2, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:44,112 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 73.0 (TID 121) (10.200.136.127, executor 3, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:44,112 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 73.0 (TID 122) (10.200.140.94, executor 1, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:44,127 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.200.140.94:41276 (size: 640.2 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:09:44,130 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.200.136.127:43545 (size: 640.2 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:09:44,131 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.200.136.126:44915 (size: 640.2 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:09:44,485 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.200.136.127:35826\n",
      "2023-02-01 07:09:44,499 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.200.136.126:48696\n",
      "2023-02-01 07:09:44,572 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.200.140.94:57642\n",
      "2023-02-01 07:09:49,382 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 73.0 (TID 123) (10.200.140.94, executor 1, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:49,382 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 73.0 (TID 122) in 5270 ms on 10.200.140.94 (executor 1) (1/48)\n",
      "2023-02-01 07:09:51,605 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 73.0 (TID 124) (10.200.136.127, executor 3, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:51,606 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 73.0 (TID 121) in 7494 ms on 10.200.136.127 (executor 3) (2/48)\n",
      "2023-02-01 07:09:52,104 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 120) in 7992 ms on 10.200.136.126 (executor 2) (3/48)\n",
      "2023-02-01 07:09:52,373 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 73.0 (TID 125) (10.200.140.94, executor 1, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:52,374 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 73.0 (TID 123) in 2992 ms on 10.200.140.94 (executor 1) (4/48)\n",
      "2023-02-01 07:09:54,376 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 73.0 (TID 126) (10.200.140.94, executor 1, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:54,377 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 73.0 (TID 125) in 2004 ms on 10.200.140.94 (executor 1) (5/48)\n",
      "2023-02-01 07:09:55,437 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 73.0 (TID 127) (10.200.136.126, executor 2, partition 7, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:55,501 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.200.136.126:48696\n",
      "2023-02-01 07:09:56,577 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 73.0 (TID 128) (10.200.140.94, executor 1, partition 8, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:56,578 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 73.0 (TID 126) in 2202 ms on 10.200.140.94 (executor 1) (6/48)\n",
      "2023-02-01 07:09:56,704 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 73.0 (TID 129) (10.200.136.127, executor 3, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:56,704 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 73.0 (TID 124) in 5099 ms on 10.200.136.127 (executor 3) (7/48)\n",
      "2023-02-01 07:09:56,901 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.200.136.127:35826\n",
      "2023-02-01 07:09:57,303 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 73.0 (TID 127) in 1867 ms on 10.200.136.126 (executor 2) (8/48)\n",
      "2023-02-01 07:09:58,170 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 73.0 (TID 130) (10.200.140.94, executor 1, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:58,170 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 73.0 (TID 128) in 1593 ms on 10.200.140.94 (executor 1) (9/48)\n",
      "2023-02-01 07:09:58,806 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 73.0 (TID 131) (10.200.136.127, executor 3, partition 11, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:58,806 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 73.0 (TID 129) in 2103 ms on 10.200.136.127 (executor 3) (10/48)\n",
      "2023-02-01 07:09:59,984 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 73.0 (TID 132) (10.200.140.94, executor 1, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:09:59,984 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 73.0 (TID 130) in 1814 ms on 10.200.140.94 (executor 1) (11/48)\n",
      "2023-02-01 07:10:00,437 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 73.0 (TID 133) (10.200.136.126, executor 2, partition 13, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:00,805 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 73.0 (TID 134) (10.200.136.127, executor 3, partition 14, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:00,805 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 73.0 (TID 131) in 1999 ms on 10.200.136.127 (executor 3) (12/48)\n",
      "2023-02-01 07:10:01,868 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 73.0 (TID 135) (10.200.140.94, executor 1, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:01,869 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 73.0 (TID 132) in 1886 ms on 10.200.140.94 (executor 1) (13/48)\n",
      "2023-02-01 07:10:02,206 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 73.0 (TID 133) in 1769 ms on 10.200.136.126 (executor 2) (14/48)\n",
      "2023-02-01 07:10:02,892 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 73.0 (TID 136) (10.200.136.127, executor 3, partition 16, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:02,892 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 73.0 (TID 134) in 2087 ms on 10.200.136.127 (executor 3) (15/48)\n",
      "2023-02-01 07:10:03,686 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 73.0 (TID 137) (10.200.140.94, executor 1, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:03,687 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 73.0 (TID 135) in 1819 ms on 10.200.140.94 (executor 1) (16/48)\n",
      "2023-02-01 07:10:04,886 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 73.0 (TID 138) (10.200.136.127, executor 3, partition 18, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:04,886 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 73.0 (TID 136) in 1995 ms on 10.200.136.127 (executor 3) (17/48)\n",
      "2023-02-01 07:10:05,437 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 73.0 (TID 139) (10.200.136.126, executor 2, partition 19, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:05,780 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 73.0 (TID 140) (10.200.140.94, executor 1, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:05,781 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 73.0 (TID 137) in 2095 ms on 10.200.140.94 (executor 1) (18/48)\n",
      "2023-02-01 07:10:06,895 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 73.0 (TID 141) (10.200.136.127, executor 3, partition 21, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:06,895 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 73.0 (TID 138) in 2009 ms on 10.200.136.127 (executor 3) (19/48)\n",
      "2023-02-01 07:10:07,301 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 73.0 (TID 139) in 1864 ms on 10.200.136.126 (executor 2) (20/48)\n",
      "2023-02-01 07:10:07,780 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 73.0 (TID 142) (10.200.140.94, executor 1, partition 22, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:07,781 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 73.0 (TID 140) in 2000 ms on 10.200.140.94 (executor 1) (21/48)\n",
      "2023-02-01 07:10:08,601 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 73.0 (TID 143) (10.200.136.127, executor 3, partition 23, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:08,601 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 73.0 (TID 141) in 1706 ms on 10.200.136.127 (executor 3) (22/48)\n",
      "2023-02-01 07:10:09,669 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 73.0 (TID 144) (10.200.140.94, executor 1, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:09,670 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 73.0 (TID 142) in 1890 ms on 10.200.140.94 (executor 1) (23/48)\n",
      "2023-02-01 07:10:10,309 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 73.0 (TID 145) (10.200.136.127, executor 3, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:10,310 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 73.0 (TID 143) in 1709 ms on 10.200.136.127 (executor 3) (24/48)\n",
      "2023-02-01 07:10:10,437 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 73.0 (TID 146) (10.200.136.126, executor 2, partition 26, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:11,490 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 73.0 (TID 147) (10.200.140.94, executor 1, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:11,490 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 73.0 (TID 144) in 1821 ms on 10.200.140.94 (executor 1) (25/48)\n",
      "2023-02-01 07:10:12,100 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 73.0 (TID 146) in 1663 ms on 10.200.136.126 (executor 2) (26/48)\n",
      "2023-02-01 07:10:12,405 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 73.0 (TID 148) (10.200.136.127, executor 3, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:12,406 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 73.0 (TID 145) in 2097 ms on 10.200.136.127 (executor 3) (27/48)\n",
      "2023-02-01 07:10:13,271 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 73.0 (TID 149) (10.200.140.94, executor 1, partition 29, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:13,272 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 73.0 (TID 147) in 1782 ms on 10.200.140.94 (executor 1) (28/48)\n",
      "2023-02-01 07:10:14,501 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 73.0 (TID 150) (10.200.136.127, executor 3, partition 30, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:14,501 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 73.0 (TID 148) in 2096 ms on 10.200.136.127 (executor 3) (29/48)\n",
      "2023-02-01 07:10:14,877 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 73.0 (TID 151) (10.200.140.94, executor 1, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:14,878 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 73.0 (TID 149) in 1607 ms on 10.200.140.94 (executor 1) (30/48)\n",
      "2023-02-01 07:10:15,437 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 73.0 (TID 152) (10.200.136.126, executor 2, partition 32, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:16,604 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 73.0 (TID 153) (10.200.136.127, executor 3, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:16,604 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 73.0 (TID 150) in 2103 ms on 10.200.136.127 (executor 3) (31/48)\n",
      "2023-02-01 07:10:16,880 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 73.0 (TID 154) (10.200.140.94, executor 1, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:16,880 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 73.0 (TID 151) in 2003 ms on 10.200.140.94 (executor 1) (32/48)\n",
      "2023-02-01 07:10:17,097 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 73.0 (TID 152) in 1661 ms on 10.200.136.126 (executor 2) (33/48)\n",
      "2023-02-01 07:10:18,300 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 73.0 (TID 155) (10.200.136.127, executor 3, partition 35, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:18,300 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 73.0 (TID 153) in 1696 ms on 10.200.136.127 (executor 3) (34/48)\n",
      "2023-02-01 07:10:18,690 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 73.0 (TID 156) (10.200.140.94, executor 1, partition 36, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:18,690 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 73.0 (TID 154) in 1810 ms on 10.200.140.94 (executor 1) (35/48)\n",
      "2023-02-01 07:10:19,789 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 73.0 (TID 157) (10.200.136.127, executor 3, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:19,790 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 73.0 (TID 155) in 1490 ms on 10.200.136.127 (executor 3) (36/48)\n",
      "2023-02-01 07:10:20,437 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 73.0 (TID 158) (10.200.136.126, executor 2, partition 38, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:20,488 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 73.0 (TID 159) (10.200.140.94, executor 1, partition 39, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:20,488 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 73.0 (TID 156) in 1799 ms on 10.200.140.94 (executor 1) (37/48)\n",
      "2023-02-01 07:10:21,603 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 73.0 (TID 160) (10.200.136.127, executor 3, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:21,603 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 73.0 (TID 157) in 1814 ms on 10.200.136.127 (executor 3) (38/48)\n",
      "2023-02-01 07:10:22,193 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 73.0 (TID 158) in 1756 ms on 10.200.136.126 (executor 2) (39/48)\n",
      "2023-02-01 07:10:22,270 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 73.0 (TID 161) (10.200.140.94, executor 1, partition 41, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:22,270 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 73.0 (TID 159) in 1782 ms on 10.200.140.94 (executor 1) (40/48)\n",
      "2023-02-01 07:10:23,507 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 73.0 (TID 162) (10.200.136.127, executor 3, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:23,508 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 73.0 (TID 160) in 1906 ms on 10.200.136.127 (executor 3) (41/48)\n",
      "2023-02-01 07:10:23,785 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 73.0 (TID 163) (10.200.140.94, executor 1, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:23,786 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 73.0 (TID 161) in 1515 ms on 10.200.140.94 (executor 1) (42/48)\n",
      "2023-02-01 07:10:25,385 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 73.0 (TID 164) (10.200.136.127, executor 3, partition 44, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:25,385 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 73.0 (TID 162) in 1878 ms on 10.200.136.127 (executor 3) (43/48)\n",
      "2023-02-01 07:10:25,437 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 73.0 (TID 165) (10.200.136.126, executor 2, partition 45, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:25,874 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 73.0 (TID 166) (10.200.140.94, executor 1, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:25,875 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 73.0 (TID 163) in 2090 ms on 10.200.140.94 (executor 1) (44/48)\n",
      "2023-02-01 07:10:26,593 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 73.0 (TID 167) (10.200.136.127, executor 3, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:26,593 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 73.0 (TID 164) in 1209 ms on 10.200.136.127 (executor 3) (45/48)\n",
      "2023-02-01 07:10:26,997 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 73.0 (TID 165) in 1561 ms on 10.200.136.126 (executor 2) (46/48)\n",
      "2023-02-01 07:10:27,777 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 73.0 (TID 166) in 1903 ms on 10.200.140.94 (executor 1) (47/48)\n",
      "2023-02-01 07:10:27,894 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 73.0 (TID 167) in 1301 ms on 10.200.136.127 (executor 3) (48/48)\n",
      "2023-02-01 07:10:27,894 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:10:27,895 INFO scheduler.DAGScheduler: ShuffleMapStage 73 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 43.851 s\n",
      "2023-02-01 07:10:27,895 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:10:27,895 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:10:27,895 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:10:27,895 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:10:28,555 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:10:28,556 INFO scheduler.DAGScheduler: Got job 39 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 1 output partitions\n",
      "2023-02-01 07:10:28,556 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:10:28,556 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)\n",
      "2023-02-01 07:10:28,556 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:10:28,557 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[387] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:10:28,664 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "2023-02-01 07:10:28,665 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 2.7 MiB, free 2.2 GiB)\n",
      "2023-02-01 07:10:28,672 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 585.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:10:28,672 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 585.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:10:28,672 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:10:28,673 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[387] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 07:10:28,673 INFO scheduler.TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "2023-02-01 07:10:28,674 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 168) (10.200.136.127, executor 3, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:10:28,687 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.200.136.127:43545 (size: 585.3 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:10:28,815 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.200.136.127:35826\n",
      "2023-02-01 07:10:31,612 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 168) in 2939 ms on 10.200.136.127 (executor 3) (1/1)\n",
      "2023-02-01 07:10:31,612 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:10:31,613 INFO scheduler.DAGScheduler: ResultStage 77 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 3.056 s\n",
      "2023-02-01 07:10:31,613 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:10:31,613 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "2023-02-01 07:10:31,613 INFO scheduler.DAGScheduler: Job 39 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104, took 3.058304 s\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2023-02-01 07:10:32,540 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 640.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:10:32,541 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.200.140.94:41276 in memory (size: 640.2 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:10:32,541 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.200.136.127:43545 in memory (size: 640.2 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:10:32,542 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.200.136.126:44915 in memory (size: 640.2 KiB, free: 1007.3 MiB)\n",
      "2023-02-01 07:10:32,548 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 585.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:10:32,550 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.200.136.127:43545 in memory (size: 585.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:11:37,286 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,286 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:11:37,286 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,290 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,290 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:11:37,290 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,294 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,294 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:11:37,295 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,300 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,300 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:11:37,300 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,305 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,305 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:11:37,306 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,312 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,312 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:11:37,312 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,318 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,318 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:11:37,318 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,325 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,325 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:11:37,326 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,333 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,333 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:11:37,334 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,342 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,342 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:11:37,342 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,350 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,350 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:11:37,350 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,359 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,359 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:11:37,359 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,369 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,369 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:11:37,369 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,379 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,379 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:11:37,379 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,390 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,390 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:11:37,390 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:37,401 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:11:37,401 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:11:37,402 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-02-01 07:11:38,205 INFO codegen.CodeGenerator: Code generated in 39.478501 ms\n",
      "2023-02-01 07:11:38,207 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,222 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,223 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:38,223 INFO spark.SparkContext: Created broadcast 79 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:11:38,224 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:11:38,243 INFO scheduler.DAGScheduler: Registering RDD 391 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 36\n",
      "2023-02-01 07:11:38,244 INFO scheduler.DAGScheduler: Got map stage job 40 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-02-01 07:11:38,244 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:11:38,244 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:11:38,244 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:11:38,244 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[391] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:11:38,246 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 76.5 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,248 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 30.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,249 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 30.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:38,250 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:11:38,251 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[391] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:11:38,251 INFO scheduler.TaskSchedulerImpl: Adding task set 78.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:11:38,252 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 169) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:38,252 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 78.0 (TID 170) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:38,260 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.200.140.94:41276 (size: 30.3 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:11:38,260 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.200.136.126:44915 (size: 30.3 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:11:38,301 INFO codegen.CodeGenerator: Code generated in 36.986932 ms\n",
      "2023-02-01 07:11:38,312 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,328 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,329 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:38,329 INFO spark.SparkContext: Created broadcast 81 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:11:38,330 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:11:38,343 INFO scheduler.DAGScheduler: Registering RDD 395 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 37\n",
      "2023-02-01 07:11:38,343 INFO scheduler.DAGScheduler: Got map stage job 41 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-02-01 07:11:38,343 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 79 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:11:38,343 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:11:38,343 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:11:38,344 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[395] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:11:38,346 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 78.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,348 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 30.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:38,348 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 30.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:38,348 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:11:38,348 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[395] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:11:38,348 INFO scheduler.TaskSchedulerImpl: Adding task set 79.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:11:38,349 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 79.0 (TID 171) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:38,357 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.200.136.127:43545 (size: 30.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:11:38,486 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:11:38,500 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:11:38,592 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:11:39,586 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 79.0 (TID 172) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:39,586 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 78.0 (TID 170) in 1334 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:11:39,593 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.200.136.126:44915 (size: 30.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:11:39,787 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 169) in 1535 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:11:39,787 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:11:39,787 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 1.543 s\n",
      "2023-02-01 07:11:39,787 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:11:39,787 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 79)\n",
      "2023-02-01 07:11:39,787 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:11:39,787 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:11:39,887 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:11:39,908 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 79.0 (TID 171) in 1559 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:11:41,090 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 79.0 (TID 172) in 1504 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:11:41,091 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:11:41,091 INFO scheduler.DAGScheduler: ShuffleMapStage 79 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 2.747 s\n",
      "2023-02-01 07:11:41,091 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:11:41,091 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:11:41,091 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:11:41,091 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:11:41,139 INFO adaptive.ShufflePartitionsUtil: For shuffle(36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37), advisory target size: 67108864, actual target size 1082985, minimum partition size: 1048576\n",
      "2023-02-01 07:11:41,755 INFO scheduler.DAGScheduler: Registering RDD 430 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 38\n",
      "2023-02-01 07:11:41,755 INFO scheduler.DAGScheduler: Got map stage job 42 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 48 output partitions\n",
      "2023-02-01 07:11:41,755 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 82 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:11:41,755 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81, ShuffleMapStage 80)\n",
      "2023-02-01 07:11:41,756 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:11:41,756 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 82 (MapPartitionsRDD[430] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:11:41,830 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1094.7 KiB\n",
      "2023-02-01 07:11:41,830 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 1094.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:41,834 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 268.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:11:41,834 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 268.4 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:41,835 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:11:41,835 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 82 (MapPartitionsRDD[430] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-02-01 07:11:41,836 INFO scheduler.TaskSchedulerImpl: Adding task set 82.0 with 48 tasks resource profile 0\n",
      "2023-02-01 07:11:41,837 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 173) (10.200.140.94, executor 1, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:41,837 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 82.0 (TID 174) (10.200.136.126, executor 2, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:41,837 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 82.0 (TID 175) (10.200.136.127, executor 3, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:41,847 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.200.140.94:41276 (size: 268.4 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:11:41,848 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.200.136.127:43545 (size: 268.4 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:11:41,848 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.200.136.126:44915 (size: 268.4 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:11:41,880 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.200.140.94:57642\n",
      "2023-02-01 07:11:41,897 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.200.136.127:35826\n",
      "2023-02-01 07:11:41,897 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.200.136.126:48696\n",
      "2023-02-01 07:11:44,701 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 82.0 (TID 176) (10.200.136.127, executor 3, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:44,701 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 82.0 (TID 175) in 2864 ms on 10.200.136.127 (executor 3) (1/48)\n",
      "2023-02-01 07:11:45,529 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 30.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:45,585 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.200.136.126:44915 in memory (size: 30.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:11:45,585 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.200.136.127:43545 in memory (size: 30.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:11:45,590 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.200.136.126:44915 in memory (size: 30.3 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:11:45,591 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 30.3 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:11:45,668 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.200.140.94:41276 in memory (size: 30.3 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:11:46,194 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 82.0 (TID 177) (10.200.136.127, executor 3, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:46,195 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 82.0 (TID 176) in 1494 ms on 10.200.136.127 (executor 3) (2/48)\n",
      "2023-02-01 07:11:46,284 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 82.0 (TID 178) (10.200.140.94, executor 1, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:46,284 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 173) in 4448 ms on 10.200.140.94 (executor 1) (3/48)\n",
      "2023-02-01 07:11:46,413 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 82.0 (TID 179) (10.200.136.126, executor 2, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:46,414 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 82.0 (TID 174) in 4577 ms on 10.200.136.126 (executor 2) (4/48)\n",
      "2023-02-01 07:11:46,590 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.200.136.126:48696\n",
      "2023-02-01 07:11:47,205 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 82.0 (TID 180) (10.200.136.127, executor 3, partition 7, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:47,205 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 82.0 (TID 177) in 1011 ms on 10.200.136.127 (executor 3) (5/48)\n",
      "2023-02-01 07:11:48,287 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 82.0 (TID 181) (10.200.136.126, executor 2, partition 8, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:48,287 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 82.0 (TID 179) in 1874 ms on 10.200.136.126 (executor 2) (6/48)\n",
      "2023-02-01 07:11:48,894 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 82.0 (TID 182) (10.200.136.127, executor 3, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:48,895 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 82.0 (TID 180) in 1691 ms on 10.200.136.127 (executor 3) (7/48)\n",
      "2023-02-01 07:11:49,184 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 82.0 (TID 178) in 2900 ms on 10.200.140.94 (executor 1) (8/48)\n",
      "2023-02-01 07:11:49,495 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 82.0 (TID 183) (10.200.136.126, executor 2, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:49,495 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 82.0 (TID 181) in 1209 ms on 10.200.136.126 (executor 2) (9/48)\n",
      "2023-02-01 07:11:50,286 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 82.0 (TID 184) (10.200.136.127, executor 3, partition 11, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:50,286 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 82.0 (TID 182) in 1392 ms on 10.200.136.127 (executor 3) (10/48)\n",
      "2023-02-01 07:11:51,185 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 82.0 (TID 185) (10.200.136.126, executor 2, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:51,185 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 82.0 (TID 183) in 1690 ms on 10.200.136.126 (executor 2) (11/48)\n",
      "2023-02-01 07:11:51,304 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 82.0 (TID 186) (10.200.136.127, executor 3, partition 13, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:51,304 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 82.0 (TID 184) in 1018 ms on 10.200.136.127 (executor 3) (12/48)\n",
      "2023-02-01 07:11:52,437 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 82.0 (TID 187) (10.200.140.94, executor 1, partition 14, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:52,473 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.200.140.94:57642\n",
      "2023-02-01 07:11:52,687 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 82.0 (TID 188) (10.200.136.127, executor 3, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:52,687 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 82.0 (TID 186) in 1383 ms on 10.200.136.127 (executor 3) (13/48)\n",
      "2023-02-01 07:11:52,895 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 82.0 (TID 189) (10.200.136.126, executor 2, partition 16, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:52,896 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 82.0 (TID 185) in 1711 ms on 10.200.136.126 (executor 2) (14/48)\n",
      "2023-02-01 07:11:53,981 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 82.0 (TID 187) in 1544 ms on 10.200.140.94 (executor 1) (15/48)\n",
      "2023-02-01 07:11:53,995 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 82.0 (TID 190) (10.200.136.127, executor 3, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:53,996 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 82.0 (TID 188) in 1310 ms on 10.200.136.127 (executor 3) (16/48)\n",
      "2023-02-01 07:11:54,398 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 82.0 (TID 191) (10.200.136.126, executor 2, partition 18, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:54,398 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 82.0 (TID 189) in 1503 ms on 10.200.136.126 (executor 2) (17/48)\n",
      "2023-02-01 07:11:54,995 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 82.0 (TID 192) (10.200.136.127, executor 3, partition 19, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:54,995 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 82.0 (TID 190) in 1000 ms on 10.200.136.127 (executor 3) (18/48)\n",
      "2023-02-01 07:11:56,090 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 82.0 (TID 193) (10.200.136.126, executor 2, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:56,090 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 82.0 (TID 191) in 1692 ms on 10.200.136.126 (executor 2) (19/48)\n",
      "2023-02-01 07:11:56,385 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 82.0 (TID 194) (10.200.136.127, executor 3, partition 21, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:56,386 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 82.0 (TID 192) in 1391 ms on 10.200.136.127 (executor 3) (20/48)\n",
      "2023-02-01 07:11:56,437 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 82.0 (TID 195) (10.200.140.94, executor 1, partition 22, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:57,604 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 82.0 (TID 196) (10.200.136.127, executor 3, partition 23, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:57,604 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 82.0 (TID 194) in 1219 ms on 10.200.136.127 (executor 3) (21/48)\n",
      "2023-02-01 07:11:57,687 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 82.0 (TID 197) (10.200.136.126, executor 2, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:57,687 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 82.0 (TID 193) in 1597 ms on 10.200.136.126 (executor 2) (22/48)\n",
      "2023-02-01 07:11:57,880 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 82.0 (TID 195) in 1443 ms on 10.200.140.94 (executor 1) (23/48)\n",
      "2023-02-01 07:11:58,884 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 82.0 (TID 198) (10.200.136.127, executor 3, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:58,885 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 82.0 (TID 196) in 1282 ms on 10.200.136.127 (executor 3) (24/48)\n",
      "2023-02-01 07:11:59,300 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 82.0 (TID 199) (10.200.136.126, executor 2, partition 26, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:11:59,300 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 82.0 (TID 197) in 1613 ms on 10.200.136.126 (executor 2) (25/48)\n",
      "2023-02-01 07:12:00,185 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 82.0 (TID 200) (10.200.136.127, executor 3, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:00,186 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 82.0 (TID 198) in 1302 ms on 10.200.136.127 (executor 3) (26/48)\n",
      "2023-02-01 07:12:00,593 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 82.0 (TID 201) (10.200.136.126, executor 2, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:00,593 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 82.0 (TID 199) in 1293 ms on 10.200.136.126 (executor 2) (27/48)\n",
      "2023-02-01 07:12:01,398 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 82.0 (TID 202) (10.200.136.127, executor 3, partition 29, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:01,399 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 82.0 (TID 200) in 1214 ms on 10.200.136.127 (executor 3) (28/48)\n",
      "2023-02-01 07:12:01,437 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 82.0 (TID 203) (10.200.140.94, executor 1, partition 30, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:02,192 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 82.0 (TID 204) (10.200.136.126, executor 2, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:02,193 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 82.0 (TID 201) in 1601 ms on 10.200.136.126 (executor 2) (29/48)\n",
      "2023-02-01 07:12:02,388 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 82.0 (TID 205) (10.200.136.127, executor 3, partition 32, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:02,389 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 82.0 (TID 202) in 991 ms on 10.200.136.127 (executor 3) (30/48)\n",
      "2023-02-01 07:12:02,692 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 82.0 (TID 203) in 1256 ms on 10.200.140.94 (executor 1) (31/48)\n",
      "2023-02-01 07:12:03,885 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 82.0 (TID 206) (10.200.136.126, executor 2, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:03,885 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 82.0 (TID 204) in 1693 ms on 10.200.136.126 (executor 2) (32/48)\n",
      "2023-02-01 07:12:04,002 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 82.0 (TID 207) (10.200.136.127, executor 3, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:04,002 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 82.0 (TID 205) in 1614 ms on 10.200.136.127 (executor 3) (33/48)\n",
      "2023-02-01 07:12:05,191 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 82.0 (TID 208) (10.200.136.126, executor 2, partition 35, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:05,192 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 82.0 (TID 206) in 1307 ms on 10.200.136.126 (executor 2) (34/48)\n",
      "2023-02-01 07:12:05,437 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 82.0 (TID 209) (10.200.140.94, executor 1, partition 36, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:05,487 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 82.0 (TID 210) (10.200.136.127, executor 3, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:05,488 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 82.0 (TID 207) in 1486 ms on 10.200.136.127 (executor 3) (35/48)\n",
      "2023-02-01 07:12:06,305 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 82.0 (TID 211) (10.200.136.126, executor 2, partition 38, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:06,305 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 82.0 (TID 208) in 1114 ms on 10.200.136.126 (executor 2) (36/48)\n",
      "2023-02-01 07:12:06,586 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 82.0 (TID 212) (10.200.136.127, executor 3, partition 39, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:06,586 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 82.0 (TID 210) in 1099 ms on 10.200.136.127 (executor 3) (37/48)\n",
      "2023-02-01 07:12:06,880 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 82.0 (TID 209) in 1444 ms on 10.200.140.94 (executor 1) (38/48)\n",
      "2023-02-01 07:12:07,696 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 82.0 (TID 213) (10.200.136.127, executor 3, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:07,697 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 82.0 (TID 212) in 1111 ms on 10.200.136.127 (executor 3) (39/48)\n",
      "2023-02-01 07:12:08,301 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 82.0 (TID 214) (10.200.136.126, executor 2, partition 41, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:08,301 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 82.0 (TID 211) in 1996 ms on 10.200.136.126 (executor 2) (40/48)\n",
      "2023-02-01 07:12:08,699 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 82.0 (TID 215) (10.200.136.127, executor 3, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:08,699 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 82.0 (TID 213) in 1003 ms on 10.200.136.127 (executor 3) (41/48)\n",
      "2023-02-01 07:12:09,601 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 82.0 (TID 216) (10.200.136.126, executor 2, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:09,602 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 82.0 (TID 214) in 1302 ms on 10.200.136.126 (executor 2) (42/48)\n",
      "2023-02-01 07:12:10,091 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 82.0 (TID 217) (10.200.136.127, executor 3, partition 44, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:10,091 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 82.0 (TID 215) in 1392 ms on 10.200.136.127 (executor 3) (43/48)\n",
      "2023-02-01 07:12:10,437 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 82.0 (TID 218) (10.200.140.94, executor 1, partition 45, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:10,987 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 82.0 (TID 219) (10.200.136.127, executor 3, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:10,987 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 82.0 (TID 217) in 896 ms on 10.200.136.127 (executor 3) (44/48)\n",
      "2023-02-01 07:12:11,097 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 82.0 (TID 220) (10.200.136.126, executor 2, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:11,097 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 82.0 (TID 216) in 1496 ms on 10.200.136.126 (executor 2) (45/48)\n",
      "2023-02-01 07:12:11,770 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 82.0 (TID 218) in 1334 ms on 10.200.140.94 (executor 1) (46/48)\n",
      "2023-02-01 07:12:12,094 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 82.0 (TID 219) in 1107 ms on 10.200.136.127 (executor 3) (47/48)\n",
      "2023-02-01 07:12:12,794 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 82.0 (TID 220) in 1697 ms on 10.200.136.126 (executor 2) (48/48)\n",
      "2023-02-01 07:12:12,795 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:12,795 INFO scheduler.DAGScheduler: ShuffleMapStage 82 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 31.013 s\n",
      "2023-02-01 07:12:12,795 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:12,795 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:12:12,795 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:12,795 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:13,332 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 07:12:13,333 INFO scheduler.DAGScheduler: Got job 43 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 1 output partitions\n",
      "2023-02-01 07:12:13,333 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 07:12:13,333 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)\n",
      "2023-02-01 07:12:13,333 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:13,334 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[433] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 07:12:13,441 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "2023-02-01 07:12:13,441 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 2.3 MiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:13,447 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 472.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:13,447 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 472.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:13,448 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:13,448 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[433] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 07:12:13,448 INFO scheduler.TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "2023-02-01 07:12:13,449 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 221) (10.200.136.127, executor 3, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:13,462 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.200.136.127:43545 (size: 472.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:13,600 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 10.200.136.127:35826\n",
      "2023-02-01 07:12:15,322 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 221) in 1873 ms on 10.200.136.127 (executor 3) (1/1)\n",
      "2023-02-01 07:12:15,322 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:15,322 INFO scheduler.DAGScheduler: ResultStage 86 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 1.988 s\n",
      "2023-02-01 07:12:15,322 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:12:15,322 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished\n",
      "2023-02-01 07:12:15,323 INFO scheduler.DAGScheduler: Job 43 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104, took 1.990371 s\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2023-02-01 07:12:16,287 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 472.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:16,288 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.200.136.127:43545 in memory (size: 472.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:16,291 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 268.4 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:16,292 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.200.136.127:43545 in memory (size: 268.4 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:12:16,292 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.200.140.94:41276 in memory (size: 268.4 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:16,292 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.200.136.126:44915 in memory (size: 268.4 KiB, free: 1007.2 MiB)\n",
      "2023-02-01 07:12:16,957 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,958 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:12:16,958 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,960 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,960 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:12:16,960 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,962 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,962 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:12:16,962 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,964 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,964 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:12:16,965 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,967 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,967 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:12:16,967 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,969 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,969 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:12:16,969 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,971 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,971 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:12:16,972 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,974 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,974 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:12:16,974 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,976 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,976 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:12:16,976 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,979 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,979 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:12:16,979 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,981 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,981 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:12:16,981 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,983 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,983 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:12:16,984 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,986 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,986 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:12:16,986 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,988 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,988 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:12:16,988 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,991 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,991 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:12:16,991 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:16,993 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:16,993 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:12:16,993 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:17,504 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,520 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,520 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,520 INFO spark.SparkContext: Created broadcast 85 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,521 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,536 INFO scheduler.DAGScheduler: Registering RDD 438 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 39\n",
      "2023-02-01 07:12:17,537 INFO scheduler.DAGScheduler: Got map stage job 44 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,537 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,537 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,537 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,537 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[438] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,550 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 104.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,552 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,555 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,555 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,555 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[438] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,555 INFO scheduler.TaskSchedulerImpl: Adding task set 87.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,556 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 222) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:17,556 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 87.0 (TID 223) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:17,564 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.200.140.94:41276 (size: 36.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:17,571 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,586 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,586 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,587 INFO spark.SparkContext: Created broadcast 87 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,588 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,590 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.200.136.126:44915 (size: 36.2 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Registering RDD 443 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 40\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Got map stage job 45 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 88 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,604 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[443] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,609 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 106.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,611 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,611 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,611 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,612 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[443] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,612 INFO scheduler.TaskSchedulerImpl: Adding task set 88.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,612 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 88.0 (TID 224) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:17,619 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.200.136.127:43545 (size: 36.7 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:17,630 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,644 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,645 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,645 INFO spark.SparkContext: Created broadcast 89 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,646 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Registering RDD 448 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 41\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Got map stage job 46 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 89 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,653 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 89 (MapPartitionsRDD[448] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,657 INFO memory.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,659 INFO memory.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,659 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,662 INFO spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,663 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 89 (MapPartitionsRDD[448] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,663 INFO scheduler.TaskSchedulerImpl: Adding task set 89.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,678 INFO memory.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,679 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:17,689 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:17,693 INFO memory.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,694 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,694 INFO spark.SparkContext: Created broadcast 91 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,695 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Registering RDD 453 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 42\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Got map stage job 47 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 90 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,702 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[453] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,706 INFO memory.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,708 INFO memory.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,708 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,708 INFO spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,709 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[453] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,709 INFO scheduler.TaskSchedulerImpl: Adding task set 90.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,728 INFO memory.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,743 INFO memory.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,743 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,744 INFO spark.SparkContext: Created broadcast 93 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,745 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Registering RDD 458 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 43\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Got map stage job 48 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 91 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,758 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 91 (MapPartitionsRDD[458] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,767 INFO memory.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,768 INFO memory.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,771 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,772 INFO spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,773 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 91 (MapPartitionsRDD[458] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,773 INFO scheduler.TaskSchedulerImpl: Adding task set 91.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,787 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-02-01 07:12:17,789 INFO memory.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,804 INFO memory.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,805 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,805 INFO spark.SparkContext: Created broadcast 95 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,806 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Registering RDD 463 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 44\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Got map stage job 49 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,812 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[463] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,818 INFO memory.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,819 INFO memory.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,820 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,820 INFO spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,821 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[463] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,821 INFO scheduler.TaskSchedulerImpl: Adding task set 92.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,839 INFO memory.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,853 INFO memory.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,854 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,854 INFO spark.SparkContext: Created broadcast 97 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,855 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,860 INFO scheduler.DAGScheduler: Registering RDD 468 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 45\n",
      "2023-02-01 07:12:17,861 INFO scheduler.DAGScheduler: Got map stage job 50 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,861 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 93 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,861 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,861 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,861 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 93 (MapPartitionsRDD[468] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,868 INFO memory.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,870 INFO memory.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,877 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,878 INFO spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,878 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 93 (MapPartitionsRDD[468] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,878 INFO scheduler.TaskSchedulerImpl: Adding task set 93.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,886 INFO memory.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,901 INFO memory.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,901 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,902 INFO spark.SparkContext: Created broadcast 99 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,902 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,910 INFO scheduler.DAGScheduler: Registering RDD 473 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 46\n",
      "2023-02-01 07:12:17,910 INFO scheduler.DAGScheduler: Got map stage job 51 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,910 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 94 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,910 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,910 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,911 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 94 (MapPartitionsRDD[473] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,913 INFO memory.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,915 INFO memory.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,915 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,916 INFO spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,916 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 94 (MapPartitionsRDD[473] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,916 INFO scheduler.TaskSchedulerImpl: Adding task set 94.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,934 INFO memory.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,950 INFO memory.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,951 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,951 INFO spark.SparkContext: Created broadcast 101 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,952 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Registering RDD 478 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 47\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Got map stage job 52 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 95 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:17,958 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[478] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:17,961 INFO memory.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,963 INFO memory.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,963 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,964 INFO spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:17,964 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[478] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:17,964 INFO scheduler.TaskSchedulerImpl: Adding task set 95.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:17,982 INFO memory.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,997 INFO memory.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:17,997 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:17,998 INFO spark.SparkContext: Created broadcast 103 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:17,998 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Registering RDD 483 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 48\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Got map stage job 53 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,009 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[483] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,012 INFO memory.MemoryStore: Block broadcast_104 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,013 INFO memory.MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,013 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,014 INFO spark.SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,014 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[483] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,014 INFO scheduler.TaskSchedulerImpl: Adding task set 96.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,031 INFO memory.MemoryStore: Block broadcast_105 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,046 INFO memory.MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,047 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,047 INFO spark.SparkContext: Created broadcast 105 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,048 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Registering RDD 488 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 49\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Got map stage job 54 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 97 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,053 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 97 (MapPartitionsRDD[488] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,056 INFO memory.MemoryStore: Block broadcast_106 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,094 INFO memory.MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,094 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,094 INFO spark.SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,098 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 97 (MapPartitionsRDD[488] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,098 INFO scheduler.TaskSchedulerImpl: Adding task set 97.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,118 INFO memory.MemoryStore: Block broadcast_107 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,133 INFO memory.MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,134 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,134 INFO spark.SparkContext: Created broadcast 107 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,135 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,142 INFO scheduler.DAGScheduler: Registering RDD 493 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 50\n",
      "2023-02-01 07:12:18,142 INFO scheduler.DAGScheduler: Got map stage job 55 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,142 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 98 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,143 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,143 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,143 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 98 (MapPartitionsRDD[493] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,146 INFO memory.MemoryStore: Block broadcast_108 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,148 INFO memory.MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,150 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,152 INFO spark.SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,153 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 98 (MapPartitionsRDD[493] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,153 INFO scheduler.TaskSchedulerImpl: Adding task set 98.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,167 INFO memory.MemoryStore: Block broadcast_109 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,182 INFO memory.MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,183 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,183 INFO spark.SparkContext: Created broadcast 109 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,184 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,189 INFO scheduler.DAGScheduler: Registering RDD 498 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 51\n",
      "2023-02-01 07:12:18,190 INFO scheduler.DAGScheduler: Got map stage job 56 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,190 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 99 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,190 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,190 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,190 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[498] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,193 INFO memory.MemoryStore: Block broadcast_110 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,195 INFO memory.MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,195 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,195 INFO spark.SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,196 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[498] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,196 INFO scheduler.TaskSchedulerImpl: Adding task set 99.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,214 INFO memory.MemoryStore: Block broadcast_111 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,230 INFO memory.MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,231 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,231 INFO spark.SparkContext: Created broadcast 111 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,232 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,237 INFO scheduler.DAGScheduler: Registering RDD 503 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 52\n",
      "2023-02-01 07:12:18,238 INFO scheduler.DAGScheduler: Got map stage job 57 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,238 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 100 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,238 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,238 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,238 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 100 (MapPartitionsRDD[503] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,241 INFO memory.MemoryStore: Block broadcast_112 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,243 INFO memory.MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,244 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,246 INFO spark.SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,246 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 100 (MapPartitionsRDD[503] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,246 INFO scheduler.TaskSchedulerImpl: Adding task set 100.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,262 INFO memory.MemoryStore: Block broadcast_113 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,279 INFO memory.MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,279 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,280 INFO spark.SparkContext: Created broadcast 113 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,280 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Registering RDD 508 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 53\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Got map stage job 58 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,286 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[508] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,289 INFO memory.MemoryStore: Block broadcast_114 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,291 INFO memory.MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,293 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,294 INFO spark.SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,295 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[508] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,295 INFO scheduler.TaskSchedulerImpl: Adding task set 101.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:18,316 INFO memory.MemoryStore: Block broadcast_115 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,342 INFO memory.MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,343 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,344 INFO spark.SparkContext: Created broadcast 115 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:18,345 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:18,354 INFO scheduler.DAGScheduler: Registering RDD 513 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 54\n",
      "2023-02-01 07:12:18,355 INFO scheduler.DAGScheduler: Got map stage job 59 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-02-01 07:12:18,355 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 102 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:18,355 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:18,355 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:18,355 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 102 (MapPartitionsRDD[513] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:18,358 INFO memory.MemoryStore: Block broadcast_116 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,360 INFO memory.MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:18,360 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:18,360 INFO spark.SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:18,361 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 102 (MapPartitionsRDD[513] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:18,361 INFO scheduler.TaskSchedulerImpl: Adding task set 102.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:20,697 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 88.0 (TID 225) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:20,698 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 87.0 (TID 223) in 3142 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:20,704 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.200.136.126:44915 (size: 36.7 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:20,885 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 226) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:20,885 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 88.0 (TID 224) in 3273 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:20,892 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:20,904 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:20,904 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:21,881 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 89.0 (TID 227) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:21,882 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 222) in 4326 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:21,882 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:21,882 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 4.345 s\n",
      "2023-02-01 07:12:21,882 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:21,882 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 88, ShuffleMapStage 89, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:21,883 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:21,883 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:21,888 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:21,986 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:22,897 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 228) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:22,897 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 88.0 (TID 225) in 2200 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:22,897 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:22,898 INFO scheduler.DAGScheduler: ShuffleMapStage 88 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 5.292 s\n",
      "2023-02-01 07:12:22,898 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:22,898 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 89, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:22,898 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:22,898 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:22,904 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,086 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,389 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 90.0 (TID 229) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:23,389 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 89.0 (TID 227) in 1508 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:23,430 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 230) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:23,430 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 226) in 2546 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:23,430 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:23,431 INFO scheduler.DAGScheduler: ShuffleMapStage 89 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 5.776 s\n",
      "2023-02-01 07:12:23,431 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:23,431 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:23,431 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:23,431 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:23,472 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,487 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,497 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,549 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:23,550 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,568 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,612 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:23,614 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.200.136.127:43545 in memory (size: 36.7 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:23,672 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,695 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.200.136.126:44915 in memory (size: 36.7 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:23,699 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:23,701 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.200.136.126:44915 in memory (size: 36.2 KiB, free: 1007.0 MiB)\n",
      "2023-02-01 07:12:23,767 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.200.140.94:41276 in memory (size: 36.2 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:25,193 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 91.0 (TID 231) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:25,193 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 228) in 2296 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:25,200 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:25,211 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:25,576 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 232) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:25,576 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 90.0 (TID 229) in 2188 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:25,576 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:25,577 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 7.874 s\n",
      "2023-02-01 07:12:25,577 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:25,577 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:25,577 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:25,577 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:25,583 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:25,598 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 92.0 (TID 233) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:25,599 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 230) in 2169 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:25,605 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:25,779 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:25,789 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-02-01 07:12:26,791 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 93.0 (TID 234) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:26,791 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 91.0 (TID 231) in 1598 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:26,791 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:26,791 INFO scheduler.DAGScheduler: ShuffleMapStage 91 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 9.032 s\n",
      "2023-02-01 07:12:26,792 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:26,792 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:26,792 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:26,792 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:26,798 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:26,809 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:27,398 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 93.0 (TID 235) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:27,398 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 92.0 (TID 233) in 1800 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:27,407 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:27,497 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:27,981 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 236) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:27,981 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 232) in 2405 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:27,981 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:27,982 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 10.169 s\n",
      "2023-02-01 07:12:27,982 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:27,982 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:27,982 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:27,982 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:27,987 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.200.140.94:41276 (size: 35.8 KiB, free: 1006.8 MiB)\n",
      "2023-02-01 07:12:28,178 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:29,190 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 94.0 (TID 237) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:29,190 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 93.0 (TID 235) in 1792 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:29,273 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.200.136.127:43545 (size: 35.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:29,312 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 238) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:29,313 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 93.0 (TID 234) in 2522 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:29,313 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:29,313 INFO scheduler.DAGScheduler: ShuffleMapStage 93 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 11.452 s\n",
      "2023-02-01 07:12:29,313 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:29,313 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:29,313 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:29,313 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:29,321 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:29,394 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:29,479 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:30,371 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 95.0 (TID 239) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:30,371 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 236) in 2390 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:30,386 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:30,471 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:30,906 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 240) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:30,906 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 94.0 (TID 237) in 1716 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:30,906 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:30,907 INFO scheduler.DAGScheduler: ShuffleMapStage 94 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 12.996 s\n",
      "2023-02-01 07:12:30,907 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:30,907 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 95)\n",
      "2023-02-01 07:12:30,907 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:30,907 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:30,986 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.200.136.127:43545 (size: 35.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:31,186 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:31,489 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 96.0 (TID 241) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:31,489 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 238) in 2177 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:31,496 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.200.136.126:44915 (size: 35.8 KiB, free: 1006.7 MiB)\n",
      "2023-02-01 07:12:31,687 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:31,872 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 97.0 (TID 242) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:31,872 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 95.0 (TID 239) in 1501 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:31,872 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:31,873 INFO scheduler.DAGScheduler: ShuffleMapStage 95 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 13.914 s\n",
      "2023-02-01 07:12:31,873 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:31,873 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:31,873 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:31,873 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:31,878 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:32,069 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:33,400 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 97.0 (TID 243) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:33,400 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 96.0 (TID 241) in 1911 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:33,484 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:33,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 244) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:33,497 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 240) in 2591 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:33,498 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:33,498 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 15.489 s\n",
      "2023-02-01 07:12:33,498 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:33,498 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:33,498 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:33,498 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:33,504 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.6 MiB)\n",
      "2023-02-01 07:12:33,588 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:33,597 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:33,980 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 98.0 (TID 245) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:33,981 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 97.0 (TID 242) in 2109 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:33,989 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:34,269 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:35,208 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 246) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:35,208 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 97.0 (TID 243) in 1808 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:35,208 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:35,209 INFO scheduler.DAGScheduler: ShuffleMapStage 97 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 17.154 s\n",
      "2023-02-01 07:12:35,209 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:35,209 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:35,209 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:35,209 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:35,292 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:35,306 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:35,803 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 99.0 (TID 247) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:35,803 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 244) in 2306 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:35,885 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.5 MiB)\n",
      "2023-02-01 07:12:35,899 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:36,179 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 248) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:36,180 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 98.0 (TID 245) in 2200 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:36,180 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:36,180 INFO scheduler.DAGScheduler: ShuffleMapStage 98 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 18.037 s\n",
      "2023-02-01 07:12:36,180 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:36,180 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:36,180 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:36,180 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:36,186 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:36,370 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:37,388 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 100.0 (TID 249) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:37,388 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 99.0 (TID 247) in 1585 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:37,469 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:37,585 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 250) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:37,586 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 246) in 2378 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:37,586 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:37,586 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 19.396 s\n",
      "2023-02-01 07:12:37,586 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:37,586 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:37,586 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:37,586 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:37,593 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:37,600 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:37,892 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:38,570 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 101.0 (TID 251) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:38,570 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 248) in 2391 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:38,576 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:38,687 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:38,997 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 102.0 (TID 252) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:38,998 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 100.0 (TID 249) in 1610 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:38,998 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:38,998 INFO scheduler.DAGScheduler: ShuffleMapStage 100 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 20.760 s\n",
      "2023-02-01 07:12:38,998 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:38,998 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 101)\n",
      "2023-02-01 07:12:38,998 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:38,998 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:39,004 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:39,109 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:40,002 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 102.0 (TID 253) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:40,003 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 250) in 2418 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:40,088 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:40,192 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 101.0 (TID 251) in 1623 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:40,192 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:40,193 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 21.905 s\n",
      "2023-02-01 07:12:40,193 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:40,193 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102)\n",
      "2023-02-01 07:12:40,193 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:40,193 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:40,286 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:40,336 INFO storage.BlockManagerInfo: Removed broadcast_110_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:40,385 INFO storage.BlockManagerInfo: Removed broadcast_110_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:40,385 INFO storage.BlockManagerInfo: Removed broadcast_110_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:40,389 INFO storage.BlockManagerInfo: Removed broadcast_112_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:40,390 INFO storage.BlockManagerInfo: Removed broadcast_112_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:40,390 INFO storage.BlockManagerInfo: Removed broadcast_112_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:40,393 INFO storage.BlockManagerInfo: Removed broadcast_108_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:40,394 INFO storage.BlockManagerInfo: Removed broadcast_108_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:40,395 INFO storage.BlockManagerInfo: Removed broadcast_108_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:41,007 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 102.0 (TID 252) in 2010 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:42,199 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 102.0 (TID 253) in 2197 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:42,199 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:42,200 INFO scheduler.DAGScheduler: ShuffleMapStage 102 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 23.845 s\n",
      "2023-02-01 07:12:42,200 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:42,200 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:12:42,200 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:42,200 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:42,379 INFO adaptive.ShufflePartitionsUtil: For shuffle(39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54), advisory target size: 67108864, actual target size 3159149, minimum partition size: 1048576\n",
      "2023-02-01 07:12:42,673 INFO codegen.CodeGenerator: Code generated in 15.439825 ms\n",
      "2023-02-01 07:12:42,696 INFO codegen.CodeGenerator: Code generated in 12.278567 ms\n",
      "2023-02-01 07:12:42,718 INFO codegen.CodeGenerator: Code generated in 12.128167 ms\n",
      "2023-02-01 07:12:42,740 INFO codegen.CodeGenerator: Code generated in 12.502297 ms\n",
      "2023-02-01 07:12:42,760 INFO codegen.CodeGenerator: Code generated in 11.652891 ms\n",
      "2023-02-01 07:12:42,780 INFO codegen.CodeGenerator: Code generated in 11.582687 ms\n",
      "2023-02-01 07:12:42,800 INFO codegen.CodeGenerator: Code generated in 11.761037 ms\n",
      "2023-02-01 07:12:42,819 INFO codegen.CodeGenerator: Code generated in 11.324924 ms\n",
      "2023-02-01 07:12:42,838 INFO codegen.CodeGenerator: Code generated in 12.483845 ms\n",
      "2023-02-01 07:12:42,856 INFO codegen.CodeGenerator: Code generated in 11.470388 ms\n",
      "2023-02-01 07:12:42,873 INFO codegen.CodeGenerator: Code generated in 11.459946 ms\n",
      "2023-02-01 07:12:42,890 INFO codegen.CodeGenerator: Code generated in 11.820588 ms\n",
      "2023-02-01 07:12:42,906 INFO codegen.CodeGenerator: Code generated in 11.685498 ms\n",
      "2023-02-01 07:12:42,921 INFO codegen.CodeGenerator: Code generated in 11.325891 ms\n",
      "2023-02-01 07:12:42,937 INFO codegen.CodeGenerator: Code generated in 11.052396 ms\n",
      "2023-02-01 07:12:42,979 INFO codegen.CodeGenerator: Code generated in 10.418346 ms\n",
      "2023-02-01 07:12:43,018 INFO codegen.CodeGenerator: Code generated in 11.260418 ms\n",
      "2023-02-01 07:12:43,030 INFO codegen.CodeGenerator: Code generated in 9.31271 ms\n",
      "2023-02-01 07:12:43,063 INFO codegen.CodeGenerator: Code generated in 11.735808 ms\n",
      "2023-02-01 07:12:43,094 INFO codegen.CodeGenerator: Code generated in 11.675485 ms\n",
      "2023-02-01 07:12:43,106 INFO codegen.CodeGenerator: Code generated in 9.139618 ms\n",
      "2023-02-01 07:12:43,135 INFO codegen.CodeGenerator: Code generated in 12.320886 ms\n",
      "2023-02-01 07:12:43,167 INFO codegen.CodeGenerator: Code generated in 12.712478 ms\n",
      "2023-02-01 07:12:43,178 INFO codegen.CodeGenerator: Code generated in 9.04114 ms\n",
      "2023-02-01 07:12:43,207 INFO codegen.CodeGenerator: Code generated in 12.745933 ms\n",
      "2023-02-01 07:12:43,243 INFO codegen.CodeGenerator: Code generated in 11.333153 ms\n",
      "2023-02-01 07:12:43,254 INFO codegen.CodeGenerator: Code generated in 8.60359 ms\n",
      "2023-02-01 07:12:43,281 INFO codegen.CodeGenerator: Code generated in 11.309616 ms\n",
      "2023-02-01 07:12:43,310 INFO codegen.CodeGenerator: Code generated in 11.900309 ms\n",
      "2023-02-01 07:12:43,322 INFO codegen.CodeGenerator: Code generated in 9.245704 ms\n",
      "2023-02-01 07:12:43,348 INFO codegen.CodeGenerator: Code generated in 11.27807 ms\n",
      "2023-02-01 07:12:43,360 INFO codegen.CodeGenerator: Code generated in 9.535521 ms\n",
      "2023-02-01 07:12:43,386 INFO codegen.CodeGenerator: Code generated in 11.616968 ms\n",
      "2023-02-01 07:12:43,398 INFO codegen.CodeGenerator: Code generated in 9.07623 ms\n",
      "2023-02-01 07:12:43,441 INFO codegen.CodeGenerator: Code generated in 26.055156 ms\n",
      "2023-02-01 07:12:43,466 INFO codegen.CodeGenerator: Code generated in 20.087819 ms\n",
      "2023-02-01 07:12:43,501 INFO codegen.CodeGenerator: Code generated in 10.236342 ms\n",
      "2023-02-01 07:12:43,512 INFO codegen.CodeGenerator: Code generated in 8.304852 ms\n",
      "2023-02-01 07:12:43,534 INFO codegen.CodeGenerator: Code generated in 9.219561 ms\n",
      "2023-02-01 07:12:43,543 INFO codegen.CodeGenerator: Code generated in 7.196222 ms\n",
      "2023-02-01 07:12:43,554 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 07:12:43,557 INFO scheduler.DAGScheduler: Got job 60 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 1 output partitions\n",
      "2023-02-01 07:12:43,557 INFO scheduler.DAGScheduler: Final stage: ResultStage 119 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 07:12:43,557 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 117, ShuffleMapStage 103, ShuffleMapStage 118, ShuffleMapStage 104, ShuffleMapStage 111, ShuffleMapStage 105, ShuffleMapStage 112, ShuffleMapStage 106, ShuffleMapStage 113, ShuffleMapStage 114, ShuffleMapStage 115, ShuffleMapStage 107, ShuffleMapStage 108, ShuffleMapStage 109, ShuffleMapStage 116, ShuffleMapStage 110)\n",
      "2023-02-01 07:12:43,557 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:43,558 INFO scheduler.DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[607] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 07:12:43,602 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1618.0 KiB\n",
      "2023-02-01 07:12:43,602 INFO memory.MemoryStore: Block broadcast_117 stored as values in memory (estimated size 1618.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:43,607 INFO memory.MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 394.0 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:43,608 INFO storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 394.0 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:43,608 INFO spark.SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:43,608 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[607] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 07:12:43,608 INFO scheduler.TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0\n",
      "2023-02-01 07:12:43,609 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 119.0 (TID 254) (10.200.136.126, executor 2, partition 0, NODE_LOCAL, 4791 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:43,622 INFO storage.BlockManagerInfo: Added broadcast_117_piece0 in memory on 10.200.136.126:44915 (size: 394.0 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:12:43,700 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to 10.200.136.126:48696\n",
      "2023-02-01 07:12:44,692 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 119.0 (TID 254) in 1082 ms on 10.200.136.126 (executor 2) (1/1)\n",
      "2023-02-01 07:12:44,692 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:44,692 INFO scheduler.DAGScheduler: ResultStage 119 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 1.133 s\n",
      "2023-02-01 07:12:44,693 INFO scheduler.DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:12:44,693 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished\n",
      "2023-02-01 07:12:44,693 INFO scheduler.DAGScheduler: Job 60 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67, took 1.138531 s\n",
      "> 2023-02-01 07:12:44,750 [info] writing to target parquet, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/transactions/parquet/sets/transactions/1675235564750_621/', 'format': 'parquet', 'partitionBy': ['year', 'month', 'day', 'hour']}\n",
      "2023-02-01 07:12:45,881 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,881 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:12:45,881 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,883 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,883 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:12:45,883 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,885 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,885 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:12:45,886 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,887 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,887 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:12:45,888 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,889 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,890 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:12:45,890 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,892 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,892 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:12:45,892 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,894 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,894 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:12:45,894 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,896 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,896 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:12:45,896 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,898 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,898 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:12:45,898 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,900 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,900 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:12:45,900 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,902 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,902 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:12:45,902 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,904 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,904 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:12:45,904 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,906 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,906 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:12:45,906 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,908 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,908 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:12:45,908 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,910 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,910 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:12:45,910 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,912 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:12:45,912 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:12:45,913 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:12:45,999 INFO storage.BlockManagerInfo: Removed broadcast_117_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 394.0 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,000 INFO storage.BlockManagerInfo: Removed broadcast_117_piece0 on 10.200.136.126:44915 in memory (size: 394.0 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:46,219 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 07:12:46,234 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-02-01 07:12:46,235 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-02-01 07:12:46,235 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 07:12:46,236 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-02-01 07:12:46,236 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-02-01 07:12:46,236 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 07:12:46,586 INFO memory.MemoryStore: Block broadcast_118 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,601 INFO memory.MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,602 INFO storage.BlockManagerInfo: Added broadcast_118_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,602 INFO spark.SparkContext: Created broadcast 118 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,603 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,621 INFO scheduler.DAGScheduler: Registering RDD 612 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 55\n",
      "2023-02-01 07:12:46,621 INFO scheduler.DAGScheduler: Got map stage job 61 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,621 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 120 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,621 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,621 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,622 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 120 (MapPartitionsRDD[612] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,642 INFO memory.MemoryStore: Block broadcast_119 stored as values in memory (estimated size 104.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,644 INFO memory.MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,644 INFO storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,644 INFO spark.SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,645 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 120 (MapPartitionsRDD[612] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,645 INFO scheduler.TaskSchedulerImpl: Adding task set 120.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,646 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 120.0 (TID 255) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:46,646 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 120.0 (TID 256) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:46,655 INFO storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on 10.200.140.94:41276 (size: 36.2 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:46,656 INFO storage.BlockManagerInfo: Added broadcast_119_piece0 in memory on 10.200.136.126:44915 (size: 36.2 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:46,659 INFO memory.MemoryStore: Block broadcast_120 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,666 INFO storage.BlockManagerInfo: Added broadcast_118_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:46,667 INFO storage.BlockManagerInfo: Added broadcast_118_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:46,674 INFO memory.MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,674 INFO storage.BlockManagerInfo: Added broadcast_120_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,675 INFO spark.SparkContext: Created broadcast 120 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,676 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,691 INFO scheduler.DAGScheduler: Registering RDD 617 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 56\n",
      "2023-02-01 07:12:46,692 INFO scheduler.DAGScheduler: Got map stage job 62 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,692 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 121 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,692 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,692 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,692 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 121 (MapPartitionsRDD[617] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,696 INFO memory.MemoryStore: Block broadcast_121 stored as values in memory (estimated size 106.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,698 INFO memory.MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,698 INFO storage.BlockManagerInfo: Added broadcast_121_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,698 INFO spark.SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,699 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 121 (MapPartitionsRDD[617] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,699 INFO scheduler.TaskSchedulerImpl: Adding task set 121.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,699 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 121.0 (TID 257) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:46,707 INFO storage.BlockManagerInfo: Added broadcast_121_piece0 in memory on 10.200.136.127:43545 (size: 36.7 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:46,716 INFO memory.MemoryStore: Block broadcast_122 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,717 INFO storage.BlockManagerInfo: Added broadcast_120_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:46,731 INFO memory.MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,731 INFO storage.BlockManagerInfo: Added broadcast_122_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,732 INFO spark.SparkContext: Created broadcast 122 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,732 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,740 INFO scheduler.DAGScheduler: Registering RDD 622 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 57\n",
      "2023-02-01 07:12:46,740 INFO scheduler.DAGScheduler: Got map stage job 63 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,740 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 122 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,740 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,740 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,741 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 122 (MapPartitionsRDD[622] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,745 INFO memory.MemoryStore: Block broadcast_123 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,746 INFO memory.MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,746 INFO storage.BlockManagerInfo: Added broadcast_123_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,747 INFO spark.SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,747 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 122 (MapPartitionsRDD[622] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,747 INFO scheduler.TaskSchedulerImpl: Adding task set 122.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,765 INFO memory.MemoryStore: Block broadcast_124 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,780 INFO memory.MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,780 INFO storage.BlockManagerInfo: Added broadcast_124_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,780 INFO spark.SparkContext: Created broadcast 124 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,781 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Registering RDD 627 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 58\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Got map stage job 64 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 123 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,789 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 123 (MapPartitionsRDD[627] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,793 INFO memory.MemoryStore: Block broadcast_125 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,795 INFO memory.MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,795 INFO storage.BlockManagerInfo: Added broadcast_125_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,795 INFO spark.SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,796 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[627] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,796 INFO scheduler.TaskSchedulerImpl: Adding task set 123.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,814 INFO memory.MemoryStore: Block broadcast_126 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,829 INFO memory.MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,829 INFO storage.BlockManagerInfo: Added broadcast_126_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,829 INFO spark.SparkContext: Created broadcast 126 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,830 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,846 INFO scheduler.DAGScheduler: Registering RDD 632 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 59\n",
      "2023-02-01 07:12:46,847 INFO scheduler.DAGScheduler: Got map stage job 65 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,847 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 124 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,847 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,847 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,847 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 124 (MapPartitionsRDD[632] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,851 INFO memory.MemoryStore: Block broadcast_127 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,853 INFO memory.MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,853 INFO storage.BlockManagerInfo: Added broadcast_127_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,853 INFO spark.SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,854 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 124 (MapPartitionsRDD[632] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,854 INFO scheduler.TaskSchedulerImpl: Adding task set 124.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,871 INFO memory.MemoryStore: Block broadcast_128 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,886 INFO memory.MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,886 INFO storage.BlockManagerInfo: Added broadcast_128_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,887 INFO spark.SparkContext: Created broadcast 128 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,887 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,893 INFO scheduler.DAGScheduler: Registering RDD 637 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 60\n",
      "2023-02-01 07:12:46,893 INFO scheduler.DAGScheduler: Got map stage job 66 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,893 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 125 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,893 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,894 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,894 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 125 (MapPartitionsRDD[637] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,898 INFO memory.MemoryStore: Block broadcast_129 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,899 INFO memory.MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,899 INFO storage.BlockManagerInfo: Added broadcast_129_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,900 INFO spark.SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,900 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 125 (MapPartitionsRDD[637] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,900 INFO scheduler.TaskSchedulerImpl: Adding task set 125.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,918 INFO memory.MemoryStore: Block broadcast_130 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,933 INFO memory.MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,933 INFO storage.BlockManagerInfo: Added broadcast_130_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,934 INFO spark.SparkContext: Created broadcast 130 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,934 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Registering RDD 642 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 61\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Got map stage job 67 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 126 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,941 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[642] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,945 INFO memory.MemoryStore: Block broadcast_131 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,947 INFO memory.MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,947 INFO storage.BlockManagerInfo: Added broadcast_131_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,947 INFO spark.SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:46,948 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[642] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:46,948 INFO scheduler.TaskSchedulerImpl: Adding task set 126.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:46,966 INFO memory.MemoryStore: Block broadcast_132 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,981 INFO memory.MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:46,981 INFO storage.BlockManagerInfo: Added broadcast_132_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:46,981 INFO spark.SparkContext: Created broadcast 132 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:46,982 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:46,990 INFO scheduler.DAGScheduler: Registering RDD 647 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 62\n",
      "2023-02-01 07:12:46,991 INFO scheduler.DAGScheduler: Got map stage job 68 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:46,991 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 127 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:46,991 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:46,991 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:46,991 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 127 (MapPartitionsRDD[647] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:46,994 INFO memory.MemoryStore: Block broadcast_133 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,036 INFO memory.MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,037 INFO storage.BlockManagerInfo: Added broadcast_133_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,040 INFO spark.SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,041 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 127 (MapPartitionsRDD[647] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,041 INFO scheduler.TaskSchedulerImpl: Adding task set 127.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,060 INFO memory.MemoryStore: Block broadcast_134 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,076 INFO memory.MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,077 INFO storage.BlockManagerInfo: Added broadcast_134_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,078 INFO spark.SparkContext: Created broadcast 134 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,078 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Registering RDD 652 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 63\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Got map stage job 69 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 128 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,085 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 128 (MapPartitionsRDD[652] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,089 INFO memory.MemoryStore: Block broadcast_135 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,091 INFO memory.MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,091 INFO storage.BlockManagerInfo: Added broadcast_135_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,092 INFO spark.SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,092 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 128 (MapPartitionsRDD[652] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,092 INFO scheduler.TaskSchedulerImpl: Adding task set 128.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,113 INFO memory.MemoryStore: Block broadcast_136 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,128 INFO memory.MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,129 INFO storage.BlockManagerInfo: Added broadcast_136_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,130 INFO spark.SparkContext: Created broadcast 136 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,130 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Registering RDD 657 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 64\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Got map stage job 70 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 129 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,144 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 129 (MapPartitionsRDD[657] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,148 INFO memory.MemoryStore: Block broadcast_137 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,150 INFO memory.MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,150 INFO storage.BlockManagerInfo: Added broadcast_137_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,150 INFO spark.SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,150 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 129 (MapPartitionsRDD[657] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,150 INFO scheduler.TaskSchedulerImpl: Adding task set 129.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,169 INFO memory.MemoryStore: Block broadcast_138 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,184 INFO memory.MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,185 INFO storage.BlockManagerInfo: Added broadcast_138_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,185 INFO spark.SparkContext: Created broadcast 138 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,186 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Registering RDD 662 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 65\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Got map stage job 71 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 130 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,192 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 130 (MapPartitionsRDD[662] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,196 INFO memory.MemoryStore: Block broadcast_139 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,197 INFO memory.MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,198 INFO storage.BlockManagerInfo: Added broadcast_139_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,198 INFO spark.SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,198 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 130 (MapPartitionsRDD[662] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,198 INFO scheduler.TaskSchedulerImpl: Adding task set 130.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,217 INFO memory.MemoryStore: Block broadcast_140 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,232 INFO memory.MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,232 INFO storage.BlockManagerInfo: Added broadcast_140_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,233 INFO spark.SparkContext: Created broadcast 140 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,234 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Registering RDD 667 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 66\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Got map stage job 72 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 131 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,240 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 131 (MapPartitionsRDD[667] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,244 INFO memory.MemoryStore: Block broadcast_141 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,245 INFO memory.MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,245 INFO storage.BlockManagerInfo: Added broadcast_141_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,246 INFO spark.SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,246 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 131 (MapPartitionsRDD[667] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,246 INFO scheduler.TaskSchedulerImpl: Adding task set 131.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,265 INFO memory.MemoryStore: Block broadcast_142 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,280 INFO memory.MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,280 INFO storage.BlockManagerInfo: Added broadcast_142_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,281 INFO spark.SparkContext: Created broadcast 142 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,281 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,287 INFO scheduler.DAGScheduler: Registering RDD 672 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 67\n",
      "2023-02-01 07:12:47,287 INFO scheduler.DAGScheduler: Got map stage job 73 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,287 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 132 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,287 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,287 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,288 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 132 (MapPartitionsRDD[672] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,291 INFO memory.MemoryStore: Block broadcast_143 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,293 INFO memory.MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,293 INFO storage.BlockManagerInfo: Added broadcast_143_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,293 INFO spark.SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,294 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 132 (MapPartitionsRDD[672] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,294 INFO scheduler.TaskSchedulerImpl: Adding task set 132.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,312 INFO memory.MemoryStore: Block broadcast_144 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,349 INFO memory.MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,350 INFO storage.BlockManagerInfo: Added broadcast_144_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,350 INFO spark.SparkContext: Created broadcast 144 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,351 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,354 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Registering RDD 677 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 68\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Got map stage job 74 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 133 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,358 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 133 (MapPartitionsRDD[677] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,363 INFO memory.MemoryStore: Block broadcast_145 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,364 INFO memory.MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,364 INFO storage.BlockManagerInfo: Added broadcast_145_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,365 INFO spark.SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,365 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 133 (MapPartitionsRDD[677] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,365 INFO scheduler.TaskSchedulerImpl: Adding task set 133.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,384 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,385 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,385 INFO memory.MemoryStore: Block broadcast_146 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,400 INFO memory.MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,400 INFO storage.BlockManagerInfo: Added broadcast_146_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,401 INFO spark.SparkContext: Created broadcast 146 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,401 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Registering RDD 682 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 69\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Got map stage job 75 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 134 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,407 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 134 (MapPartitionsRDD[682] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,411 INFO memory.MemoryStore: Block broadcast_147 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,412 INFO memory.MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,413 INFO storage.BlockManagerInfo: Added broadcast_147_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,413 INFO spark.SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,413 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 134 (MapPartitionsRDD[682] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,413 INFO scheduler.TaskSchedulerImpl: Adding task set 134.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,432 INFO memory.MemoryStore: Block broadcast_148 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,447 INFO memory.MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,447 INFO storage.BlockManagerInfo: Added broadcast_148_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,447 INFO spark.SparkContext: Created broadcast 148 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:12:47,448 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Registering RDD 687 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 70\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Got map stage job 76 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 135 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:12:47,454 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 135 (MapPartitionsRDD[687] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:12:47,458 INFO memory.MemoryStore: Block broadcast_149 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,459 INFO memory.MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:12:47,459 INFO storage.BlockManagerInfo: Added broadcast_149_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,460 INFO spark.SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:12:47,460 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 135 (MapPartitionsRDD[687] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:12:47,460 INFO scheduler.TaskSchedulerImpl: Adding task set 135.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:12:47,470 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,485 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,590 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:47,596 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,669 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,689 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,694 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,695 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,696 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:47,785 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,787 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on 10.200.136.127:43545 in memory (size: 35.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:47,868 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on 10.200.140.94:41276 in memory (size: 35.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:47,871 INFO storage.BlockManagerInfo: Removed broadcast_104_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,884 INFO storage.BlockManagerInfo: Removed broadcast_104_piece0 on 10.200.136.126:44915 in memory (size: 35.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:47,889 INFO storage.BlockManagerInfo: Removed broadcast_104_piece0 on 10.200.136.127:43545 in memory (size: 35.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:47,898 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:12:47,969 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:48,183 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:48,308 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 121.0 (TID 258) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:48,308 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 121.0 (TID 257) in 1609 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:48,591 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 122.0 (TID 259) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:48,591 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 120.0 (TID 256) in 1945 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:48,601 INFO storage.BlockManagerInfo: Added broadcast_123_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:48,673 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 122.0 (TID 260) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:48,674 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 120.0 (TID 255) in 2029 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:48,674 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:48,674 INFO scheduler.DAGScheduler: ShuffleMapStage 120 (save at NativeMethodAccessorImpl.java:0) finished in 2.052 s\n",
      "2023-02-01 07:12:48,674 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:48,674 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 125, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 121, ShuffleMapStage 122, ShuffleMapStage 123, ShuffleMapStage 130, ShuffleMapStage 124, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:48,674 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:48,674 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:48,686 INFO storage.BlockManagerInfo: Added broadcast_123_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:48,696 INFO storage.BlockManagerInfo: Added broadcast_122_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:48,769 INFO storage.BlockManagerInfo: Added broadcast_122_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:49,977 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 123.0 (TID 261) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:49,978 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 122.0 (TID 260) in 1305 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:49,985 INFO storage.BlockManagerInfo: Added broadcast_125_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:49,998 INFO storage.BlockManagerInfo: Added broadcast_124_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:50,106 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 123.0 (TID 262) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:50,106 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 121.0 (TID 258) in 1799 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:50,106 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:50,107 INFO scheduler.DAGScheduler: ShuffleMapStage 121 (save at NativeMethodAccessorImpl.java:0) finished in 3.415 s\n",
      "2023-02-01 07:12:50,107 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:50,107 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 125, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 122, ShuffleMapStage 123, ShuffleMapStage 130, ShuffleMapStage 124, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:50,107 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:50,107 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:50,189 INFO storage.BlockManagerInfo: Added broadcast_125_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:50,304 INFO storage.BlockManagerInfo: Added broadcast_124_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-02-01 07:12:50,595 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 124.0 (TID 263) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:50,595 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 122.0 (TID 259) in 2004 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:50,595 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:50,596 INFO scheduler.DAGScheduler: ShuffleMapStage 122 (save at NativeMethodAccessorImpl.java:0) finished in 3.854 s\n",
      "2023-02-01 07:12:50,596 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:50,596 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 125, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 123, ShuffleMapStage 130, ShuffleMapStage 124, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:50,596 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:50,596 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:50,603 INFO storage.BlockManagerInfo: Added broadcast_127_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:50,689 INFO storage.BlockManagerInfo: Added broadcast_126_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:51,584 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 124.0 (TID 264) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:51,584 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 123.0 (TID 261) in 1607 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:51,591 INFO storage.BlockManagerInfo: Added broadcast_127_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:51,676 INFO storage.BlockManagerInfo: Added broadcast_126_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:51,885 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 125.0 (TID 265) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:51,885 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 123.0 (TID 262) in 1779 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:51,885 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:51,886 INFO scheduler.DAGScheduler: ShuffleMapStage 123 (save at NativeMethodAccessorImpl.java:0) finished in 5.095 s\n",
      "2023-02-01 07:12:51,886 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:51,886 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 125, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 124, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:51,886 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:51,886 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:51,893 INFO storage.BlockManagerInfo: Added broadcast_129_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:51,905 INFO storage.BlockManagerInfo: Added broadcast_128_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-02-01 07:12:52,290 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 125.0 (TID 266) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:52,290 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 124.0 (TID 263) in 1695 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:52,298 INFO storage.BlockManagerInfo: Added broadcast_129_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:52,491 INFO storage.BlockManagerInfo: Added broadcast_128_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:52,992 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 126.0 (TID 267) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:52,992 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 124.0 (TID 264) in 1408 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:52,992 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:52,992 INFO scheduler.DAGScheduler: ShuffleMapStage 124 (save at NativeMethodAccessorImpl.java:0) finished in 6.145 s\n",
      "2023-02-01 07:12:52,992 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:52,992 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 125, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:52,992 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:52,992 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:53,071 INFO storage.BlockManagerInfo: Added broadcast_131_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:53,180 INFO storage.BlockManagerInfo: Added broadcast_130_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:53,707 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 126.0 (TID 268) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:53,707 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 125.0 (TID 265) in 1822 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:53,869 INFO storage.BlockManagerInfo: Added broadcast_131_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:53,894 INFO storage.BlockManagerInfo: Added broadcast_130_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-02-01 07:12:54,090 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 127.0 (TID 269) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:54,090 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 125.0 (TID 266) in 1800 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:54,090 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:54,091 INFO scheduler.DAGScheduler: ShuffleMapStage 125 (save at NativeMethodAccessorImpl.java:0) finished in 7.196 s\n",
      "2023-02-01 07:12:54,091 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:54,091 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 126, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:54,091 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:54,091 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:54,096 INFO storage.BlockManagerInfo: Added broadcast_133_piece0 in memory on 10.200.136.126:44915 (size: 35.8 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:54,302 INFO storage.BlockManagerInfo: Added broadcast_132_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:55,275 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 127.0 (TID 270) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:55,276 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 126.0 (TID 267) in 2285 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:55,300 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 128.0 (TID 271) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:55,301 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 126.0 (TID 268) in 1593 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:55,301 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:55,301 INFO scheduler.DAGScheduler: ShuffleMapStage 126 (save at NativeMethodAccessorImpl.java:0) finished in 8.360 s\n",
      "2023-02-01 07:12:55,301 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:55,301 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 127, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:55,301 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:55,301 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:55,311 INFO storage.BlockManagerInfo: Added broadcast_135_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:55,369 INFO storage.BlockManagerInfo: Added broadcast_133_piece0 in memory on 10.200.140.94:41276 (size: 35.8 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:55,380 INFO storage.BlockManagerInfo: Added broadcast_132_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:55,396 INFO storage.BlockManagerInfo: Added broadcast_134_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:56,292 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 128.0 (TID 272) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:56,292 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 127.0 (TID 269) in 2203 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:12:56,298 INFO storage.BlockManagerInfo: Added broadcast_135_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:56,384 INFO storage.BlockManagerInfo: Added broadcast_134_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:12:56,879 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 129.0 (TID 273) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:56,879 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 127.0 (TID 270) in 1604 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:12:56,879 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:56,879 INFO scheduler.DAGScheduler: ShuffleMapStage 127 (save at NativeMethodAccessorImpl.java:0) finished in 9.888 s\n",
      "2023-02-01 07:12:56,879 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:56,879 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 128, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:56,879 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:56,879 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:56,885 INFO storage.BlockManagerInfo: Added broadcast_137_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:12:57,074 INFO storage.BlockManagerInfo: Added broadcast_136_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:12:57,787 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 129.0 (TID 274) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:57,787 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 128.0 (TID 271) in 2487 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:12:57,876 INFO storage.BlockManagerInfo: Added broadcast_137_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.1 MiB)\n",
      "2023-02-01 07:12:57,892 INFO storage.BlockManagerInfo: Added broadcast_136_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:58,190 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 130.0 (TID 275) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:58,191 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 128.0 (TID 272) in 1900 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:12:58,191 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:58,191 INFO scheduler.DAGScheduler: ShuffleMapStage 128 (save at NativeMethodAccessorImpl.java:0) finished in 11.105 s\n",
      "2023-02-01 07:12:58,191 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:58,191 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 135, ShuffleMapStage 129, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:58,191 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:58,191 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:58,197 INFO storage.BlockManagerInfo: Added broadcast_139_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:12:58,285 INFO storage.BlockManagerInfo: Added broadcast_138_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:12:59,088 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 130.0 (TID 276) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:59,089 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 129.0 (TID 273) in 2211 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:12:59,184 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 131.0 (TID 277) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:12:59,184 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 129.0 (TID 274) in 1397 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:12:59,184 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:12:59,185 INFO scheduler.DAGScheduler: ShuffleMapStage 129 (save at NativeMethodAccessorImpl.java:0) finished in 12.040 s\n",
      "2023-02-01 07:12:59,185 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:12:59,185 INFO storage.BlockManagerInfo: Added broadcast_139_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:12:59,185 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 135, ShuffleMapStage 130, ShuffleMapStage 131)\n",
      "2023-02-01 07:12:59,185 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:12:59,185 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:12:59,191 INFO storage.BlockManagerInfo: Added broadcast_141_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:12:59,196 INFO storage.BlockManagerInfo: Added broadcast_138_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:12:59,203 INFO storage.BlockManagerInfo: Added broadcast_140_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:00,102 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 131.0 (TID 278) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:00,102 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 130.0 (TID 275) in 1912 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:13:00,184 INFO storage.BlockManagerInfo: Added broadcast_141_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:00,299 INFO storage.BlockManagerInfo: Added broadcast_140_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:00,589 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 132.0 (TID 279) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:00,589 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 130.0 (TID 276) in 1501 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:13:00,589 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:00,590 INFO scheduler.DAGScheduler: ShuffleMapStage 130 (save at NativeMethodAccessorImpl.java:0) finished in 13.397 s\n",
      "2023-02-01 07:13:00,590 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:00,590 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 135, ShuffleMapStage 131)\n",
      "2023-02-01 07:13:00,590 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:00,590 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:00,671 INFO storage.BlockManagerInfo: Added broadcast_143_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:00,681 INFO storage.BlockManagerInfo: Added broadcast_142_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:00,990 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 132.0 (TID 280) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:00,990 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 131.0 (TID 277) in 1806 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:13:00,996 INFO storage.BlockManagerInfo: Added broadcast_143_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:01,007 INFO storage.BlockManagerInfo: Added broadcast_142_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:01,990 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 133.0 (TID 281) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:01,990 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 131.0 (TID 278) in 1889 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:13:01,990 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:01,991 INFO scheduler.DAGScheduler: ShuffleMapStage 131 (save at NativeMethodAccessorImpl.java:0) finished in 14.750 s\n",
      "2023-02-01 07:13:01,991 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:01,991 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 132, ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 135)\n",
      "2023-02-01 07:13:01,991 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:01,991 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:01,996 INFO storage.BlockManagerInfo: Added broadcast_145_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:02,107 INFO storage.BlockManagerInfo: Added broadcast_144_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:02,178 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 133.0 (TID 282) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:02,178 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 132.0 (TID 279) in 1589 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:13:02,184 INFO storage.BlockManagerInfo: Added broadcast_145_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:02,271 INFO storage.BlockManagerInfo: Added broadcast_144_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:02,408 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 134.0 (TID 283) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:02,409 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 132.0 (TID 280) in 1420 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:13:02,409 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:02,409 INFO scheduler.DAGScheduler: ShuffleMapStage 132 (save at NativeMethodAccessorImpl.java:0) finished in 15.121 s\n",
      "2023-02-01 07:13:02,409 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:02,409 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 133, ShuffleMapStage 134, ShuffleMapStage 135)\n",
      "2023-02-01 07:13:02,409 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:02,409 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:02,491 INFO storage.BlockManagerInfo: Added broadcast_147_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:02,692 INFO storage.BlockManagerInfo: Added broadcast_146_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:03,589 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 134.0 (TID 284) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:03,589 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 133.0 (TID 282) in 1411 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:13:03,676 INFO storage.BlockManagerInfo: Added broadcast_147_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:03,688 INFO storage.BlockManagerInfo: Added broadcast_146_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:13:03,902 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 135.0 (TID 285) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:03,902 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 133.0 (TID 281) in 1912 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:13:03,902 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:03,903 INFO scheduler.DAGScheduler: ShuffleMapStage 133 (save at NativeMethodAccessorImpl.java:0) finished in 16.544 s\n",
      "2023-02-01 07:13:03,903 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:03,903 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 134, ShuffleMapStage 135)\n",
      "2023-02-01 07:13:03,903 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:03,903 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:03,910 INFO storage.BlockManagerInfo: Added broadcast_149_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:03,995 INFO storage.BlockManagerInfo: Added broadcast_148_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:05,100 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 135.0 (TID 286) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:05,101 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 134.0 (TID 283) in 2693 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:13:05,107 INFO storage.BlockManagerInfo: Added broadcast_149_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:05,185 INFO storage.BlockManagerInfo: Added broadcast_148_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:05,497 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 135.0 (TID 285) in 1595 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:13:05,570 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 134.0 (TID 284) in 1981 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:13:05,570 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:05,571 INFO scheduler.DAGScheduler: ShuffleMapStage 134 (save at NativeMethodAccessorImpl.java:0) finished in 18.163 s\n",
      "2023-02-01 07:13:05,571 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:05,571 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 135)\n",
      "2023-02-01 07:13:05,571 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:05,571 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:06,699 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 135.0 (TID 286) in 1599 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:13:06,699 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:13:06,699 INFO scheduler.DAGScheduler: ShuffleMapStage 135 (save at NativeMethodAccessorImpl.java:0) finished in 19.245 s\n",
      "2023-02-01 07:13:06,699 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:13:06,699 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:13:06,699 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:13:06,699 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:13:06,718 INFO adaptive.ShufflePartitionsUtil: For shuffle(55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70), advisory target size: 67108864, actual target size 3159149, minimum partition size: 1048576\n",
      "2023-02-01 07:13:06,924 INFO codegen.CodeGenerator: Code generated in 15.612175 ms\n",
      "2023-02-01 07:13:06,959 INFO codegen.CodeGenerator: Code generated in 13.69507 ms\n",
      "2023-02-01 07:13:06,979 INFO storage.BlockManagerInfo: Removed broadcast_114_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:06,981 INFO storage.BlockManagerInfo: Removed broadcast_114_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:06,982 INFO storage.BlockManagerInfo: Removed broadcast_114_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:06,987 INFO storage.BlockManagerInfo: Removed broadcast_149_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:06,988 INFO storage.BlockManagerInfo: Removed broadcast_149_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:06,988 INFO storage.BlockManagerInfo: Removed broadcast_149_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:06,990 INFO storage.BlockManagerInfo: Removed broadcast_119_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:06,991 INFO storage.BlockManagerInfo: Removed broadcast_119_piece0 on 10.200.136.126:44915 in memory (size: 36.2 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:06,991 INFO storage.BlockManagerInfo: Removed broadcast_119_piece0 on 10.200.140.94:41276 in memory (size: 36.2 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:06,998 INFO storage.BlockManagerInfo: Removed broadcast_123_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:06,999 INFO storage.BlockManagerInfo: Removed broadcast_123_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:06,999 INFO storage.BlockManagerInfo: Removed broadcast_123_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:13:07,001 INFO storage.BlockManagerInfo: Removed broadcast_127_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,002 INFO storage.BlockManagerInfo: Removed broadcast_127_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,002 INFO storage.BlockManagerInfo: Removed broadcast_127_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,005 INFO storage.BlockManagerInfo: Removed broadcast_137_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,006 INFO storage.BlockManagerInfo: Removed broadcast_137_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,006 INFO storage.BlockManagerInfo: Removed broadcast_137_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,007 INFO codegen.CodeGenerator: Code generated in 15.459021 ms\n",
      "2023-02-01 07:13:07,016 INFO storage.BlockManagerInfo: Removed broadcast_116_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,018 INFO storage.BlockManagerInfo: Removed broadcast_116_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,019 INFO storage.BlockManagerInfo: Removed broadcast_116_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,022 INFO storage.BlockManagerInfo: Removed broadcast_131_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,022 INFO storage.BlockManagerInfo: Removed broadcast_131_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,022 INFO storage.BlockManagerInfo: Removed broadcast_131_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:13:07,028 INFO storage.BlockManagerInfo: Removed broadcast_147_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,031 INFO storage.BlockManagerInfo: Removed broadcast_147_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,031 INFO storage.BlockManagerInfo: Removed broadcast_147_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,069 INFO codegen.CodeGenerator: Code generated in 43.746877 ms\n",
      "2023-02-01 07:13:07,084 INFO storage.BlockManagerInfo: Removed broadcast_143_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,086 INFO storage.BlockManagerInfo: Removed broadcast_143_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,086 INFO storage.BlockManagerInfo: Removed broadcast_143_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,090 INFO storage.BlockManagerInfo: Removed broadcast_125_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,090 INFO storage.BlockManagerInfo: Removed broadcast_125_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,090 INFO storage.BlockManagerInfo: Removed broadcast_125_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,094 INFO storage.BlockManagerInfo: Removed broadcast_133_piece0 on 10.200.140.94:41276 in memory (size: 35.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,094 INFO storage.BlockManagerInfo: Removed broadcast_133_piece0 on 10.200.136.126:44915 in memory (size: 35.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:13:07,102 INFO storage.BlockManagerInfo: Removed broadcast_133_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,105 INFO storage.BlockManagerInfo: Removed broadcast_145_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,105 INFO storage.BlockManagerInfo: Removed broadcast_145_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,105 INFO storage.BlockManagerInfo: Removed broadcast_145_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,110 INFO codegen.CodeGenerator: Code generated in 15.976969 ms\n",
      "2023-02-01 07:13:07,112 INFO storage.BlockManagerInfo: Removed broadcast_141_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,112 INFO storage.BlockManagerInfo: Removed broadcast_141_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,115 INFO storage.BlockManagerInfo: Removed broadcast_141_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,118 INFO storage.BlockManagerInfo: Removed broadcast_129_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,119 INFO storage.BlockManagerInfo: Removed broadcast_129_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,119 INFO storage.BlockManagerInfo: Removed broadcast_129_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:13:07,128 INFO storage.BlockManagerInfo: Removed broadcast_121_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,128 INFO storage.BlockManagerInfo: Removed broadcast_121_piece0 on 10.200.136.127:43545 in memory (size: 36.7 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:13:07,139 INFO storage.BlockManagerInfo: Removed broadcast_139_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:13:07,140 INFO storage.BlockManagerInfo: Removed broadcast_139_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:13:07,140 INFO storage.BlockManagerInfo: Removed broadcast_139_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,140 INFO codegen.CodeGenerator: Code generated in 15.221144 ms\n",
      "2023-02-01 07:13:07,144 INFO storage.BlockManagerInfo: Removed broadcast_135_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,144 INFO storage.BlockManagerInfo: Removed broadcast_135_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:13:07,147 INFO storage.BlockManagerInfo: Removed broadcast_135_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:13:07,153 INFO codegen.CodeGenerator: Code generated in 9.26927 ms\n",
      "2023-02-01 07:13:07,181 INFO codegen.CodeGenerator: Code generated in 15.801517 ms\n",
      "2023-02-01 07:13:07,193 INFO codegen.CodeGenerator: Code generated in 8.812002 ms\n",
      "2023-02-01 07:13:07,223 INFO codegen.CodeGenerator: Code generated in 14.894717 ms\n",
      "2023-02-01 07:13:07,252 INFO codegen.CodeGenerator: Code generated in 14.751655 ms\n",
      "2023-02-01 07:13:07,263 INFO codegen.CodeGenerator: Code generated in 7.887899 ms\n",
      "2023-02-01 07:13:07,289 INFO codegen.CodeGenerator: Code generated in 14.745398 ms\n",
      "2023-02-01 07:13:07,319 INFO codegen.CodeGenerator: Code generated in 15.576996 ms\n",
      "2023-02-01 07:13:07,349 INFO codegen.CodeGenerator: Code generated in 15.663934 ms\n",
      "2023-02-01 07:13:07,360 INFO codegen.CodeGenerator: Code generated in 8.324099 ms\n",
      "2023-02-01 07:13:07,386 INFO codegen.CodeGenerator: Code generated in 14.704878 ms\n",
      "2023-02-01 07:13:07,397 INFO codegen.CodeGenerator: Code generated in 8.107782 ms\n",
      "2023-02-01 07:13:07,424 INFO codegen.CodeGenerator: Code generated in 15.24478 ms\n",
      "2023-02-01 07:13:07,453 INFO codegen.CodeGenerator: Code generated in 15.405581 ms\n",
      "2023-02-01 07:13:07,465 INFO codegen.CodeGenerator: Code generated in 8.668726 ms\n",
      "2023-02-01 07:13:07,493 INFO codegen.CodeGenerator: Code generated in 15.799067 ms\n",
      "2023-02-01 07:13:07,505 INFO codegen.CodeGenerator: Code generated in 8.923197 ms\n",
      "2023-02-01 07:13:07,565 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 07:13:07,568 INFO scheduler.DAGScheduler: Got job 77 (save at NativeMethodAccessorImpl.java:0) with 48 output partitions\n",
      "2023-02-01 07:13:07,568 INFO scheduler.DAGScheduler: Final stage: ResultStage 152 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 07:13:07,568 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 147, ShuffleMapStage 139, ShuffleMapStage 140, ShuffleMapStage 141, ShuffleMapStage 148, ShuffleMapStage 142, ShuffleMapStage 149, ShuffleMapStage 143, ShuffleMapStage 150, ShuffleMapStage 136, ShuffleMapStage 151, ShuffleMapStage 137, ShuffleMapStage 144, ShuffleMapStage 138, ShuffleMapStage 145, ShuffleMapStage 146)\n",
      "2023-02-01 07:13:07,568 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:13:07,568 INFO scheduler.DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[753] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 07:13:07,695 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1767.9 KiB\n",
      "2023-02-01 07:13:07,696 INFO memory.MemoryStore: Block broadcast_150 stored as values in memory (estimated size 1768.0 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:13:07,702 INFO memory.MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 496.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:13:07,702 INFO storage.BlockManagerInfo: Added broadcast_150_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 496.4 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:13:07,702 INFO spark.SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:13:07,703 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ResultStage 152 (MapPartitionsRDD[753] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-02-01 07:13:07,703 INFO scheduler.TaskSchedulerImpl: Adding task set 152.0 with 48 tasks resource profile 0\n",
      "2023-02-01 07:13:07,704 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 152.0 (TID 287) (10.200.136.127, executor 3, partition 3, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:07,704 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 152.0 (TID 288) (10.200.136.126, executor 2, partition 0, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:07,704 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 152.0 (TID 289) (10.200.140.94, executor 1, partition 1, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:07,718 INFO storage.BlockManagerInfo: Added broadcast_150_piece0 in memory on 10.200.136.127:43545 (size: 496.4 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:13:07,719 INFO storage.BlockManagerInfo: Added broadcast_150_piece0 in memory on 10.200.136.126:44915 (size: 496.4 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:13:07,720 INFO storage.BlockManagerInfo: Added broadcast_150_piece0 in memory on 10.200.140.94:41276 (size: 496.4 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:13:08,089 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 56 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:08,199 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:08,267 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 55 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:17,790 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 152.0 (TID 290) (10.200.136.127, executor 3, partition 4, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:17,796 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 152.0 (TID 287) in 10092 ms on 10.200.136.127 (executor 3) (1/48)\n",
      "2023-02-01 07:13:24,463 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 152.0 (TID 291) (10.200.136.127, executor 3, partition 5, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:24,464 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 152.0 (TID 290) in 6674 ms on 10.200.136.127 (executor 3) (2/48)\n",
      "2023-02-01 07:13:27,326 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 152.0 (TID 292) (10.200.136.127, executor 3, partition 9, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:27,327 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 152.0 (TID 291) in 2864 ms on 10.200.136.127 (executor 3) (3/48)\n",
      "2023-02-01 07:13:27,499 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 58 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:29,809 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 152.0 (TID 293) (10.200.136.127, executor 3, partition 10, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:29,809 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 152.0 (TID 292) in 2483 ms on 10.200.136.127 (executor 3) (4/48)\n",
      "2023-02-01 07:13:31,897 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 152.0 (TID 294) (10.200.136.126, executor 2, partition 2, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:31,898 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 152.0 (TID 288) in 24194 ms on 10.200.136.126 (executor 2) (5/48)\n",
      "2023-02-01 07:13:32,023 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 152.0 (TID 295) (10.200.136.127, executor 3, partition 11, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:32,023 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 152.0 (TID 293) in 2215 ms on 10.200.136.127 (executor 3) (6/48)\n",
      "2023-02-01 07:13:32,669 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 152.0 (TID 296) (10.200.140.94, executor 1, partition 6, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:32,670 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 152.0 (TID 289) in 24966 ms on 10.200.140.94 (executor 1) (7/48)\n",
      "2023-02-01 07:13:32,872 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 57 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:34,879 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 152.0 (TID 297) (10.200.136.127, executor 3, partition 15, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:34,880 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 152.0 (TID 295) in 2857 ms on 10.200.136.127 (executor 3) (8/48)\n",
      "2023-02-01 07:13:34,935 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 60 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:35,710 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 152.0 (TID 298) (10.200.140.94, executor 1, partition 7, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:35,711 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 152.0 (TID 296) in 3042 ms on 10.200.140.94 (executor 1) (9/48)\n",
      "2023-02-01 07:13:37,370 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 152.0 (TID 299) (10.200.140.94, executor 1, partition 8, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:37,371 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 152.0 (TID 298) in 1661 ms on 10.200.140.94 (executor 1) (10/48)\n",
      "2023-02-01 07:13:38,020 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 152.0 (TID 300) (10.200.136.127, executor 3, partition 16, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:38,020 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 152.0 (TID 297) in 3141 ms on 10.200.136.127 (executor 3) (11/48)\n",
      "2023-02-01 07:13:39,816 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 152.0 (TID 301) (10.200.140.94, executor 1, partition 12, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:39,816 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 152.0 (TID 299) in 2446 ms on 10.200.140.94 (executor 1) (12/48)\n",
      "2023-02-01 07:13:39,912 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 152.0 (TID 302) (10.200.136.127, executor 3, partition 17, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:39,913 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 152.0 (TID 300) in 1894 ms on 10.200.136.127 (executor 3) (13/48)\n",
      "2023-02-01 07:13:39,987 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:41,939 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 152.0 (TID 303) (10.200.136.127, executor 3, partition 18, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:41,939 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 152.0 (TID 302) in 2027 ms on 10.200.136.127 (executor 3) (14/48)\n",
      "2023-02-01 07:13:42,093 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:42,265 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 152.0 (TID 304) (10.200.140.94, executor 1, partition 13, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:42,266 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 152.0 (TID 301) in 2451 ms on 10.200.140.94 (executor 1) (15/48)\n",
      "2023-02-01 07:13:42,312 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 152.0 (TID 305) (10.200.136.126, executor 2, partition 14, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:42,314 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 152.0 (TID 294) in 10417 ms on 10.200.136.126 (executor 2) (16/48)\n",
      "2023-02-01 07:13:42,594 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 59 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:44,487 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 152.0 (TID 306) (10.200.136.127, executor 3, partition 19, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:44,487 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 152.0 (TID 303) in 2549 ms on 10.200.136.127 (executor 3) (17/48)\n",
      "2023-02-01 07:13:44,626 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 152.0 (TID 307) (10.200.140.94, executor 1, partition 20, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:44,627 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 152.0 (TID 304) in 2362 ms on 10.200.140.94 (executor 1) (18/48)\n",
      "2023-02-01 07:13:44,775 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 61 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:45,075 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 152.0 (TID 308) (10.200.136.126, executor 2, partition 21, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:45,076 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 152.0 (TID 305) in 2764 ms on 10.200.136.126 (executor 2) (19/48)\n",
      "2023-02-01 07:13:45,201 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:46,226 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 152.0 (TID 309) (10.200.136.127, executor 3, partition 24, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:46,227 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 152.0 (TID 306) in 1741 ms on 10.200.136.127 (executor 3) (20/48)\n",
      "2023-02-01 07:13:46,393 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:47,189 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 152.0 (TID 310) (10.200.140.94, executor 1, partition 22, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:47,190 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 152.0 (TID 307) in 2564 ms on 10.200.140.94 (executor 1) (21/48)\n",
      "2023-02-01 07:13:47,288 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 62 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:47,319 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 152.0 (TID 311) (10.200.136.126, executor 2, partition 23, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:47,319 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 152.0 (TID 308) in 2244 ms on 10.200.136.126 (executor 2) (22/48)\n",
      "2023-02-01 07:13:49,044 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 152.0 (TID 312) (10.200.136.126, executor 2, partition 25, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:49,045 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 152.0 (TID 311) in 1726 ms on 10.200.136.126 (executor 2) (23/48)\n",
      "2023-02-01 07:13:49,124 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 63 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:49,853 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 152.0 (TID 313) (10.200.136.127, executor 3, partition 26, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:49,854 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 152.0 (TID 309) in 3628 ms on 10.200.136.127 (executor 3) (24/48)\n",
      "2023-02-01 07:13:49,989 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 152.0 (TID 314) (10.200.140.94, executor 1, partition 27, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:49,990 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 152.0 (TID 310) in 2801 ms on 10.200.140.94 (executor 1) (25/48)\n",
      "2023-02-01 07:13:50,076 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:51,055 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 152.0 (TID 315) (10.200.136.126, executor 2, partition 30, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:51,056 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 152.0 (TID 312) in 2012 ms on 10.200.136.126 (executor 2) (26/48)\n",
      "2023-02-01 07:13:51,113 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:51,543 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 152.0 (TID 316) (10.200.136.127, executor 3, partition 28, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:51,544 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 152.0 (TID 313) in 1691 ms on 10.200.136.127 (executor 3) (27/48)\n",
      "2023-02-01 07:13:51,619 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 64 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:52,169 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 152.0 (TID 317) (10.200.140.94, executor 1, partition 29, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:52,170 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 152.0 (TID 314) in 2181 ms on 10.200.140.94 (executor 1) (28/48)\n",
      "2023-02-01 07:13:53,737 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 152.0 (TID 318) (10.200.136.126, executor 2, partition 31, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:53,738 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 152.0 (TID 315) in 2683 ms on 10.200.136.126 (executor 2) (29/48)\n",
      "2023-02-01 07:13:54,079 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 152.0 (TID 319) (10.200.140.94, executor 1, partition 32, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:54,080 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 152.0 (TID 317) in 1911 ms on 10.200.140.94 (executor 1) (30/48)\n",
      "2023-02-01 07:13:54,175 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 65 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:55,017 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 152.0 (TID 320) (10.200.136.127, executor 3, partition 33, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:55,018 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 152.0 (TID 316) in 3475 ms on 10.200.136.127 (executor 3) (31/48)\n",
      "2023-02-01 07:13:55,116 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to 10.200.136.127:35826\n",
      "2023-02-01 07:13:55,299 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 152.0 (TID 321) (10.200.136.126, executor 2, partition 34, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:55,300 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 152.0 (TID 318) in 1563 ms on 10.200.136.126 (executor 2) (32/48)\n",
      "2023-02-01 07:13:55,396 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 66 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:56,838 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 152.0 (TID 322) (10.200.140.94, executor 1, partition 36, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:56,838 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 152.0 (TID 319) in 2759 ms on 10.200.140.94 (executor 1) (33/48)\n",
      "2023-02-01 07:13:56,976 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 67 to 10.200.140.94:57642\n",
      "2023-02-01 07:13:57,785 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 152.0 (TID 323) (10.200.136.127, executor 3, partition 35, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:57,786 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 152.0 (TID 320) in 2769 ms on 10.200.136.127 (executor 3) (34/48)\n",
      "2023-02-01 07:13:58,276 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 152.0 (TID 324) (10.200.136.126, executor 2, partition 39, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:58,277 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 152.0 (TID 321) in 2978 ms on 10.200.136.126 (executor 2) (35/48)\n",
      "2023-02-01 07:13:58,401 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 68 to 10.200.136.126:48696\n",
      "2023-02-01 07:13:58,822 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 152.0 (TID 325) (10.200.140.94, executor 1, partition 37, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:58,823 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 152.0 (TID 322) in 1985 ms on 10.200.140.94 (executor 1) (36/48)\n",
      "2023-02-01 07:13:59,986 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 152.0 (TID 326) (10.200.140.94, executor 1, partition 38, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:13:59,987 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 152.0 (TID 325) in 1164 ms on 10.200.140.94 (executor 1) (37/48)\n",
      "2023-02-01 07:14:00,119 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 152.0 (TID 327) (10.200.136.126, executor 2, partition 40, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:00,120 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 152.0 (TID 324) in 1844 ms on 10.200.136.126 (executor 2) (38/48)\n",
      "2023-02-01 07:14:00,600 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 152.0 (TID 328) (10.200.136.127, executor 3, partition 42, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:00,600 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 152.0 (TID 323) in 2815 ms on 10.200.136.127 (executor 3) (39/48)\n",
      "2023-02-01 07:14:00,687 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:01,696 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 152.0 (TID 329) (10.200.136.126, executor 2, partition 41, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:01,696 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 152.0 (TID 327) in 1577 ms on 10.200.136.126 (executor 2) (40/48)\n",
      "2023-02-01 07:14:01,912 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 152.0 (TID 330) (10.200.136.127, executor 3, partition 43, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:01,912 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 152.0 (TID 328) in 1312 ms on 10.200.136.127 (executor 3) (41/48)\n",
      "2023-02-01 07:14:02,184 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 152.0 (TID 331) (10.200.140.94, executor 1, partition 44, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:02,185 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 152.0 (TID 326) in 2199 ms on 10.200.140.94 (executor 1) (42/48)\n",
      "2023-02-01 07:14:02,286 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 69 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:03,066 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 152.0 (TID 332) (10.200.136.126, executor 2, partition 45, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:03,067 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 152.0 (TID 329) in 1372 ms on 10.200.136.126 (executor 2) (43/48)\n",
      "2023-02-01 07:14:03,197 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to 10.200.136.126:48696\n",
      "2023-02-01 07:14:03,685 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 152.0 (TID 333) (10.200.136.127, executor 3, partition 46, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:03,685 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 152.0 (TID 330) in 1774 ms on 10.200.136.127 (executor 3) (44/48)\n",
      "2023-02-01 07:14:03,795 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 70 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:04,147 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 152.0 (TID 331) in 1963 ms on 10.200.140.94 (executor 1) (45/48)\n",
      "2023-02-01 07:14:04,488 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 152.0 (TID 334) (10.200.136.126, executor 2, partition 47, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:04,489 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 152.0 (TID 332) in 1423 ms on 10.200.136.126 (executor 2) (46/48)\n",
      "2023-02-01 07:14:05,724 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 152.0 (TID 333) in 2040 ms on 10.200.136.127 (executor 3) (47/48)\n",
      "2023-02-01 07:14:06,856 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 152.0 (TID 334) in 2368 ms on 10.200.136.126 (executor 2) (48/48)\n",
      "2023-02-01 07:14:06,857 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:06,857 INFO scheduler.DAGScheduler: ResultStage 152 (save at NativeMethodAccessorImpl.java:0) finished in 59.271 s\n",
      "2023-02-01 07:14:06,858 INFO scheduler.DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:14:06,858 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished\n",
      "2023-02-01 07:14:06,858 INFO scheduler.DAGScheduler: Job 77 finished: save at NativeMethodAccessorImpl.java:0, took 59.292532 s\n",
      "2023-02-01 07:14:06,860 INFO datasources.FileFormatWriter: Start to commit write Job 35300243-51c5-4cb2-9f94-bdfe5d78ef20.\n",
      "2023-02-01 07:14:08,850 INFO storage.BlockManagerInfo: Removed broadcast_150_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 496.4 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:08,851 INFO storage.BlockManagerInfo: Removed broadcast_150_piece0 on 10.200.136.127:43545 in memory (size: 496.4 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:08,851 INFO storage.BlockManagerInfo: Removed broadcast_150_piece0 on 10.200.136.126:44915 in memory (size: 496.4 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:08,851 INFO storage.BlockManagerInfo: Removed broadcast_150_piece0 on 10.200.140.94:41276 in memory (size: 496.4 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:09,405 INFO storage.BlockManagerInfo: Removed broadcast_106_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:09,405 INFO storage.BlockManagerInfo: Removed broadcast_106_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:09,405 INFO storage.BlockManagerInfo: Removed broadcast_106_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:21,589 INFO datasources.FileFormatWriter: Write Job 35300243-51c5-4cb2-9f94-bdfe5d78ef20 committed. Elapsed time: 14727 ms.\n",
      "2023-02-01 07:14:21,593 INFO datasources.FileFormatWriter: Finished processing stats for write job 35300243-51c5-4cb2-9f94-bdfe5d78ef20.\n",
      "> 2023-02-01 07:14:21,605 [info] writing to target nosql, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/transactions/nosql/sets/transactions/1675235564750_621/', 'format': 'io.iguaz.v3io.spark.sql.kv', 'key': 'source'}\n",
      "2023-02-01 07:14:23,101 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,101 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-02-01 07:14:23,101 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,103 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,103 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2709 as timestamp))\n",
      "2023-02-01 07:14:23,103 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,105 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,105 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2797 as timestamp))\n",
      "2023-02-01 07:14:23,105 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,107 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,107 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2887 as timestamp))\n",
      "2023-02-01 07:14:23,107 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,109 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,109 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2979 as timestamp))\n",
      "2023-02-01 07:14:23,109 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,111 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,111 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3073 as timestamp))\n",
      "2023-02-01 07:14:23,111 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,113 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,113 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3169 as timestamp))\n",
      "2023-02-01 07:14:23,113 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,115 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,115 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3267 as timestamp))\n",
      "2023-02-01 07:14:23,115 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,117 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,117 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3367 as timestamp))\n",
      "2023-02-01 07:14:23,117 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,119 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,119 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3469 as timestamp))\n",
      "2023-02-01 07:14:23,119 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,121 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,121 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3573 as timestamp))\n",
      "2023-02-01 07:14:23,121 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,123 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,123 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3679 as timestamp))\n",
      "2023-02-01 07:14:23,123 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,125 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,125 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3787 as timestamp))\n",
      "2023-02-01 07:14:23,125 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,127 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,127 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3897 as timestamp))\n",
      "2023-02-01 07:14:23,127 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,129 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,129 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4009 as timestamp))\n",
      "2023-02-01 07:14:23,129 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,131 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 07:14:23,131 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4123 as timestamp))\n",
      "2023-02-01 07:14:23,131 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 07:14:23,567 INFO memory.MemoryStore: Block broadcast_151 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,583 INFO memory.MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,583 INFO storage.BlockManagerInfo: Added broadcast_151_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,584 INFO spark.SparkContext: Created broadcast 151 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,584 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,600 INFO scheduler.DAGScheduler: Registering RDD 758 (foreachPartition at KVRelation.scala:125) as input to shuffle 71\n",
      "2023-02-01 07:14:23,601 INFO scheduler.DAGScheduler: Got map stage job 78 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,601 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 153 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,601 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,601 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,601 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 153 (MapPartitionsRDD[758] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,612 INFO memory.MemoryStore: Block broadcast_152 stored as values in memory (estimated size 104.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,613 INFO memory.MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,614 INFO storage.BlockManagerInfo: Added broadcast_152_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,614 INFO spark.SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,614 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 153 (MapPartitionsRDD[758] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,614 INFO scheduler.TaskSchedulerImpl: Adding task set 153.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,615 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 153.0 (TID 335) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:23,615 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 153.0 (TID 336) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:23,633 INFO memory.MemoryStore: Block broadcast_153 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,637 INFO storage.BlockManagerInfo: Added broadcast_152_piece0 in memory on 10.200.136.126:44915 (size: 36.2 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:23,644 INFO storage.BlockManagerInfo: Added broadcast_152_piece0 in memory on 10.200.136.127:43545 (size: 36.2 KiB, free: 1006.0 MiB)\n",
      "2023-02-01 07:14:23,648 INFO memory.MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,648 INFO storage.BlockManagerInfo: Added broadcast_153_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,648 INFO spark.SparkContext: Created broadcast 153 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,649 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Registering RDD 763 (foreachPartition at KVRelation.scala:125) as input to shuffle 72\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Got map stage job 79 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 154 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,663 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 154 (MapPartitionsRDD[763] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,668 INFO memory.MemoryStore: Block broadcast_154 stored as values in memory (estimated size 106.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,669 INFO memory.MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,670 INFO storage.BlockManagerInfo: Added broadcast_154_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,670 INFO spark.SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,673 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 154 (MapPartitionsRDD[763] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,673 INFO scheduler.TaskSchedulerImpl: Adding task set 154.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,674 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 154.0 (TID 337) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:23,681 INFO storage.BlockManagerInfo: Added broadcast_154_piece0 in memory on 10.200.140.94:41276 (size: 36.7 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:23,691 INFO memory.MemoryStore: Block broadcast_155 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,692 INFO storage.BlockManagerInfo: Added broadcast_151_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:23,706 INFO memory.MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,706 INFO storage.BlockManagerInfo: Added broadcast_155_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,707 INFO spark.SparkContext: Created broadcast 155 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,708 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,716 INFO scheduler.DAGScheduler: Registering RDD 768 (foreachPartition at KVRelation.scala:125) as input to shuffle 73\n",
      "2023-02-01 07:14:23,716 INFO scheduler.DAGScheduler: Got map stage job 80 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,716 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 155 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,716 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,716 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,717 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 155 (MapPartitionsRDD[768] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,721 INFO memory.MemoryStore: Block broadcast_156 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,723 INFO memory.MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,723 INFO storage.BlockManagerInfo: Added broadcast_156_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,723 INFO spark.SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,723 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 155 (MapPartitionsRDD[768] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,724 INFO scheduler.TaskSchedulerImpl: Adding task set 155.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,742 INFO memory.MemoryStore: Block broadcast_157 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,757 INFO memory.MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,757 INFO storage.BlockManagerInfo: Added broadcast_157_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,758 INFO spark.SparkContext: Created broadcast 157 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,759 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,767 INFO scheduler.DAGScheduler: Registering RDD 773 (foreachPartition at KVRelation.scala:125) as input to shuffle 74\n",
      "2023-02-01 07:14:23,768 INFO scheduler.DAGScheduler: Got map stage job 81 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,768 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 156 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,768 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,768 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,768 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 156 (MapPartitionsRDD[773] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,772 INFO memory.MemoryStore: Block broadcast_158 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,774 INFO memory.MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,774 INFO storage.BlockManagerInfo: Added broadcast_158_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,775 INFO spark.SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,776 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 156 (MapPartitionsRDD[773] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,776 INFO scheduler.TaskSchedulerImpl: Adding task set 156.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,780 INFO storage.BlockManagerInfo: Added broadcast_153_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:23,790 INFO storage.BlockManagerInfo: Added broadcast_151_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:23,793 INFO memory.MemoryStore: Block broadcast_159 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,808 INFO memory.MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,809 INFO storage.BlockManagerInfo: Added broadcast_159_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,809 INFO spark.SparkContext: Created broadcast 159 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,810 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,838 INFO scheduler.DAGScheduler: Registering RDD 778 (foreachPartition at KVRelation.scala:125) as input to shuffle 75\n",
      "2023-02-01 07:14:23,839 INFO scheduler.DAGScheduler: Got map stage job 82 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,839 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 157 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,839 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,839 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,839 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 157 (MapPartitionsRDD[778] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,844 INFO memory.MemoryStore: Block broadcast_160 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,845 INFO memory.MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,846 INFO storage.BlockManagerInfo: Added broadcast_160_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,846 INFO spark.SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,846 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 157 (MapPartitionsRDD[778] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,846 INFO scheduler.TaskSchedulerImpl: Adding task set 157.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,863 INFO memory.MemoryStore: Block broadcast_161 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,878 INFO memory.MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,878 INFO storage.BlockManagerInfo: Added broadcast_161_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,879 INFO spark.SparkContext: Created broadcast 161 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,879 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Registering RDD 783 (foreachPartition at KVRelation.scala:125) as input to shuffle 76\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Got map stage job 83 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 158 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,886 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 158 (MapPartitionsRDD[783] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,892 INFO memory.MemoryStore: Block broadcast_162 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,894 INFO memory.MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,894 INFO storage.BlockManagerInfo: Added broadcast_162_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,894 INFO spark.SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,894 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 158 (MapPartitionsRDD[783] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,894 INFO scheduler.TaskSchedulerImpl: Adding task set 158.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,911 INFO memory.MemoryStore: Block broadcast_163 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,926 INFO memory.MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,927 INFO storage.BlockManagerInfo: Added broadcast_163_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,927 INFO spark.SparkContext: Created broadcast 163 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,928 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,934 INFO scheduler.DAGScheduler: Registering RDD 788 (foreachPartition at KVRelation.scala:125) as input to shuffle 77\n",
      "2023-02-01 07:14:23,934 INFO scheduler.DAGScheduler: Got map stage job 84 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,934 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 159 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,934 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,934 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,935 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 159 (MapPartitionsRDD[788] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,939 INFO memory.MemoryStore: Block broadcast_164 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,941 INFO memory.MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,941 INFO storage.BlockManagerInfo: Added broadcast_164_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,941 INFO spark.SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,941 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 159 (MapPartitionsRDD[788] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,941 INFO scheduler.TaskSchedulerImpl: Adding task set 159.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:23,959 INFO memory.MemoryStore: Block broadcast_165 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,974 INFO memory.MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,974 INFO storage.BlockManagerInfo: Added broadcast_165_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,975 INFO spark.SparkContext: Created broadcast 165 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:23,976 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:23,983 INFO scheduler.DAGScheduler: Registering RDD 793 (foreachPartition at KVRelation.scala:125) as input to shuffle 78\n",
      "2023-02-01 07:14:23,984 INFO scheduler.DAGScheduler: Got map stage job 85 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:23,984 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 160 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:23,984 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:23,984 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:23,984 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 160 (MapPartitionsRDD[793] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:23,986 INFO memory.MemoryStore: Block broadcast_166 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,988 INFO memory.MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:23,988 INFO storage.BlockManagerInfo: Added broadcast_166_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:23,988 INFO spark.SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:23,989 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 160 (MapPartitionsRDD[793] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:23,989 INFO scheduler.TaskSchedulerImpl: Adding task set 160.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,006 INFO memory.MemoryStore: Block broadcast_167 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,022 INFO memory.MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,022 INFO storage.BlockManagerInfo: Added broadcast_167_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,023 INFO spark.SparkContext: Created broadcast 167 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,023 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Registering RDD 798 (foreachPartition at KVRelation.scala:125) as input to shuffle 79\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Got map stage job 86 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 161 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,030 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 161 (MapPartitionsRDD[798] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,033 INFO memory.MemoryStore: Block broadcast_168 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,034 INFO memory.MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,035 INFO storage.BlockManagerInfo: Added broadcast_168_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,035 INFO spark.SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,035 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 161 (MapPartitionsRDD[798] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,035 INFO scheduler.TaskSchedulerImpl: Adding task set 161.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,053 INFO memory.MemoryStore: Block broadcast_169 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,068 INFO memory.MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,068 INFO storage.BlockManagerInfo: Added broadcast_169_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,069 INFO spark.SparkContext: Created broadcast 169 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,070 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,082 INFO scheduler.DAGScheduler: Registering RDD 803 (foreachPartition at KVRelation.scala:125) as input to shuffle 80\n",
      "2023-02-01 07:14:24,083 INFO scheduler.DAGScheduler: Got map stage job 87 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,083 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 162 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,083 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,083 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,083 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 162 (MapPartitionsRDD[803] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,086 INFO memory.MemoryStore: Block broadcast_170 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,087 INFO memory.MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,087 INFO storage.BlockManagerInfo: Added broadcast_170_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,088 INFO spark.SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,088 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 162 (MapPartitionsRDD[803] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,088 INFO scheduler.TaskSchedulerImpl: Adding task set 162.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,134 INFO memory.MemoryStore: Block broadcast_171 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,151 INFO memory.MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,152 INFO storage.BlockManagerInfo: Added broadcast_171_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,152 INFO spark.SparkContext: Created broadcast 171 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,153 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Registering RDD 808 (foreachPartition at KVRelation.scala:125) as input to shuffle 81\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Got map stage job 88 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 163 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,159 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 163 (MapPartitionsRDD[808] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,162 INFO memory.MemoryStore: Block broadcast_172 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,163 INFO memory.MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,163 INFO storage.BlockManagerInfo: Added broadcast_172_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,164 INFO spark.SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,164 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 163 (MapPartitionsRDD[808] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,164 INFO scheduler.TaskSchedulerImpl: Adding task set 163.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,182 INFO memory.MemoryStore: Block broadcast_173 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,195 INFO memory.MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,195 INFO storage.BlockManagerInfo: Added broadcast_173_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,196 INFO spark.SparkContext: Created broadcast 173 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,196 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Registering RDD 813 (foreachPartition at KVRelation.scala:125) as input to shuffle 82\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Got map stage job 89 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 164 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,202 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 164 (MapPartitionsRDD[813] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,205 INFO memory.MemoryStore: Block broadcast_174 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,206 INFO memory.MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,206 INFO storage.BlockManagerInfo: Added broadcast_174_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,206 INFO spark.SparkContext: Created broadcast 174 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,207 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 164 (MapPartitionsRDD[813] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,207 INFO scheduler.TaskSchedulerImpl: Adding task set 164.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,224 INFO memory.MemoryStore: Block broadcast_175 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,239 INFO memory.MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,239 INFO storage.BlockManagerInfo: Added broadcast_175_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,240 INFO spark.SparkContext: Created broadcast 175 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,240 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Registering RDD 818 (foreachPartition at KVRelation.scala:125) as input to shuffle 83\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Got map stage job 90 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 165 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,246 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 165 (MapPartitionsRDD[818] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,249 INFO memory.MemoryStore: Block broadcast_176 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,250 INFO memory.MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,251 INFO storage.BlockManagerInfo: Added broadcast_176_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,251 INFO spark.SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,251 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 165 (MapPartitionsRDD[818] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,251 INFO scheduler.TaskSchedulerImpl: Adding task set 165.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,269 INFO memory.MemoryStore: Block broadcast_177 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,283 INFO memory.MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,284 INFO storage.BlockManagerInfo: Added broadcast_177_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,284 INFO spark.SparkContext: Created broadcast 177 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,285 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Registering RDD 823 (foreachPartition at KVRelation.scala:125) as input to shuffle 84\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Got map stage job 91 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 166 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,291 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 166 (MapPartitionsRDD[823] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,293 INFO memory.MemoryStore: Block broadcast_178 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,295 INFO memory.MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,295 INFO storage.BlockManagerInfo: Added broadcast_178_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,295 INFO spark.SparkContext: Created broadcast 178 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,296 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 166 (MapPartitionsRDD[823] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,296 INFO scheduler.TaskSchedulerImpl: Adding task set 166.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,314 INFO memory.MemoryStore: Block broadcast_179 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,329 INFO memory.MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,329 INFO storage.BlockManagerInfo: Added broadcast_179_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,329 INFO spark.SparkContext: Created broadcast 179 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,330 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Registering RDD 828 (foreachPartition at KVRelation.scala:125) as input to shuffle 85\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Got map stage job 92 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 167 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,336 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 167 (MapPartitionsRDD[828] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,338 INFO memory.MemoryStore: Block broadcast_180 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,340 INFO memory.MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,340 INFO storage.BlockManagerInfo: Added broadcast_180_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,340 INFO spark.SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,341 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 167 (MapPartitionsRDD[828] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,341 INFO scheduler.TaskSchedulerImpl: Adding task set 167.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:24,358 INFO memory.MemoryStore: Block broadcast_181 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,373 INFO memory.MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,373 INFO storage.BlockManagerInfo: Added broadcast_181_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,374 INFO spark.SparkContext: Created broadcast 181 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:24,374 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 07:14:24,379 INFO scheduler.DAGScheduler: Registering RDD 833 (foreachPartition at KVRelation.scala:125) as input to shuffle 86\n",
      "2023-02-01 07:14:24,380 INFO scheduler.DAGScheduler: Got map stage job 93 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 07:14:24,380 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 168 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:24,380 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 07:14:24,380 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:24,380 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 168 (MapPartitionsRDD[833] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:24,382 INFO memory.MemoryStore: Block broadcast_182 stored as values in memory (estimated size 103.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,384 INFO memory.MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:24,384 INFO storage.BlockManagerInfo: Added broadcast_182_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:24,384 INFO spark.SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:24,384 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 168 (MapPartitionsRDD[833] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 07:14:24,384 INFO scheduler.TaskSchedulerImpl: Adding task set 168.0 with 2 tasks resource profile 0\n",
      "2023-02-01 07:14:25,992 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 154.0 (TID 338) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:25,992 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 153.0 (TID 335) in 2377 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:14:25,998 INFO storage.BlockManagerInfo: Added broadcast_154_piece0 in memory on 10.200.136.126:44915 (size: 36.7 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:26,088 INFO storage.BlockManagerInfo: Added broadcast_153_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:26,180 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 155.0 (TID 339) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:26,181 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 154.0 (TID 337) in 2508 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:26,186 INFO storage.BlockManagerInfo: Added broadcast_156_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:26,274 INFO storage.BlockManagerInfo: Added broadcast_155_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:26,486 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 155.0 (TID 340) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:26,486 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 153.0 (TID 336) in 2871 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:26,487 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:26,487 INFO scheduler.DAGScheduler: ShuffleMapStage 153 (foreachPartition at KVRelation.scala:125) finished in 2.886 s\n",
      "2023-02-01 07:14:26,487 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:26,487 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 154, ShuffleMapStage 155, ShuffleMapStage 162, ShuffleMapStage 156, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 157, ShuffleMapStage 166, ShuffleMapStage 158, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:26,487 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:26,487 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:26,499 INFO storage.BlockManagerInfo: Added broadcast_156_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:26,588 INFO storage.BlockManagerInfo: Added broadcast_155_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.9 MiB)\n",
      "2023-02-01 07:14:27,371 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 156.0 (TID 341) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:27,371 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 155.0 (TID 339) in 1191 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:27,377 INFO storage.BlockManagerInfo: Added broadcast_158_piece0 in memory on 10.200.140.94:41276 (size: 35.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:27,386 INFO storage.BlockManagerInfo: Added broadcast_157_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:27,391 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 156.0 (TID 342) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:27,391 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 155.0 (TID 340) in 905 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:27,391 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:27,392 INFO scheduler.DAGScheduler: ShuffleMapStage 155 (foreachPartition at KVRelation.scala:125) finished in 3.675 s\n",
      "2023-02-01 07:14:27,392 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:27,392 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 154, ShuffleMapStage 162, ShuffleMapStage 156, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 157, ShuffleMapStage 166, ShuffleMapStage 158, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:27,392 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:27,392 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:27,397 INFO storage.BlockManagerInfo: Added broadcast_158_piece0 in memory on 10.200.136.127:43545 (size: 35.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:27,488 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 157.0 (TID 343) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:27,488 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 154.0 (TID 338) in 1496 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:14:27,488 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:27,488 INFO scheduler.DAGScheduler: ShuffleMapStage 154 (foreachPartition at KVRelation.scala:125) finished in 3.824 s\n",
      "2023-02-01 07:14:27,489 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:27,489 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 156, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 157, ShuffleMapStage 166, ShuffleMapStage 158, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:27,489 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:27,489 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:27,495 INFO storage.BlockManagerInfo: Added broadcast_157_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:27,504 INFO storage.BlockManagerInfo: Added broadcast_160_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:27,600 INFO storage.BlockManagerInfo: Added broadcast_159_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.8 MiB)\n",
      "2023-02-01 07:14:28,906 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 157.0 (TID 344) (10.200.136.127, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:28,907 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 156.0 (TID 342) in 1516 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:14:28,988 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 158.0 (TID 345) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:28,988 INFO storage.BlockManagerInfo: Added broadcast_160_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:28,989 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 156.0 (TID 341) in 1618 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:14:28,989 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:28,989 INFO scheduler.DAGScheduler: ShuffleMapStage 156 (foreachPartition at KVRelation.scala:125) finished in 5.221 s\n",
      "2023-02-01 07:14:28,989 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:28,989 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 157, ShuffleMapStage 166, ShuffleMapStage 158, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:28,989 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:28,989 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:28,999 INFO storage.BlockManagerInfo: Added broadcast_159_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:29,074 INFO storage.BlockManagerInfo: Added broadcast_162_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:29,086 INFO storage.BlockManagerInfo: Added broadcast_161_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:29,700 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 158.0 (TID 346) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:29,700 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 157.0 (TID 343) in 2213 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:14:29,769 INFO storage.BlockManagerInfo: Added broadcast_162_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:29,795 INFO storage.BlockManagerInfo: Added broadcast_161_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.7 MiB)\n",
      "2023-02-01 07:14:30,596 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 159.0 (TID 347) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:30,596 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 157.0 (TID 344) in 1690 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:30,596 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:30,597 INFO scheduler.DAGScheduler: ShuffleMapStage 157 (foreachPartition at KVRelation.scala:125) finished in 6.758 s\n",
      "2023-02-01 07:14:30,597 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:30,597 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 158, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:30,597 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:30,597 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:30,605 INFO storage.BlockManagerInfo: Added broadcast_164_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:30,676 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 159.0 (TID 348) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:30,676 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 158.0 (TID 345) in 1688 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:30,682 INFO storage.BlockManagerInfo: Added broadcast_164_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:30,692 INFO storage.BlockManagerInfo: Added broadcast_163_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:30,767 INFO storage.BlockManagerInfo: Added broadcast_163_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:31,305 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 160.0 (TID 349) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:31,305 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 158.0 (TID 346) in 1605 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:14:31,306 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:31,306 INFO scheduler.DAGScheduler: ShuffleMapStage 158 (foreachPartition at KVRelation.scala:125) finished in 7.420 s\n",
      "2023-02-01 07:14:31,306 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:31,306 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 159, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:31,306 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:31,306 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:31,390 INFO storage.BlockManagerInfo: Added broadcast_166_piece0 in memory on 10.200.136.126:44915 (size: 35.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:31,401 INFO storage.BlockManagerInfo: Added broadcast_165_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:32,089 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 160.0 (TID 350) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:32,089 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 159.0 (TID 348) in 1413 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:32,170 INFO storage.BlockManagerInfo: Added broadcast_166_piece0 in memory on 10.200.140.94:41276 (size: 35.8 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:32,180 INFO storage.BlockManagerInfo: Added broadcast_165_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:32,508 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 161.0 (TID 351) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:32,508 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 159.0 (TID 347) in 1912 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:32,508 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:32,508 INFO scheduler.DAGScheduler: ShuffleMapStage 159 (foreachPartition at KVRelation.scala:125) finished in 8.573 s\n",
      "2023-02-01 07:14:32,508 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:32,509 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 160, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:32,509 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:32,509 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:32,517 INFO storage.BlockManagerInfo: Added broadcast_168_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.6 MiB)\n",
      "2023-02-01 07:14:32,686 INFO storage.BlockManagerInfo: Added broadcast_167_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:32,787 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 161.0 (TID 352) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:32,788 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 160.0 (TID 349) in 1483 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:14:32,794 INFO storage.BlockManagerInfo: Added broadcast_168_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:32,885 INFO storage.BlockManagerInfo: Added broadcast_167_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:33,371 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 162.0 (TID 353) (10.200.140.94, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:33,372 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 160.0 (TID 350) in 1283 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:14:33,372 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:33,372 INFO scheduler.DAGScheduler: ShuffleMapStage 160 (foreachPartition at KVRelation.scala:125) finished in 9.388 s\n",
      "2023-02-01 07:14:33,372 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:33,372 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 161, ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:33,372 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:33,372 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:33,377 INFO storage.BlockManagerInfo: Added broadcast_170_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:33,387 INFO storage.BlockManagerInfo: Added broadcast_169_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:33,621 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 162.0 (TID 354) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:33,621 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 161.0 (TID 352) in 834 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:14:33,689 INFO storage.BlockManagerInfo: Added broadcast_170_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:33,700 INFO storage.BlockManagerInfo: Added broadcast_169_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:34,108 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 163.0 (TID 355) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:34,109 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 161.0 (TID 351) in 1601 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:34,109 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:34,109 INFO scheduler.DAGScheduler: ShuffleMapStage 161 (foreachPartition at KVRelation.scala:125) finished in 10.078 s\n",
      "2023-02-01 07:14:34,109 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:34,109 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 162, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:34,109 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:34,109 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:34,115 INFO storage.BlockManagerInfo: Added broadcast_172_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:34,288 INFO storage.BlockManagerInfo: Added broadcast_171_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:34,772 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 163.0 (TID 356) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:34,772 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 162.0 (TID 353) in 1401 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:34,777 INFO storage.BlockManagerInfo: Added broadcast_172_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:34,787 INFO storage.BlockManagerInfo: Added broadcast_171_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:35,208 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 164.0 (TID 357) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:35,209 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 162.0 (TID 354) in 1587 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:14:35,209 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:35,209 INFO scheduler.DAGScheduler: ShuffleMapStage 162 (foreachPartition at KVRelation.scala:125) finished in 11.126 s\n",
      "2023-02-01 07:14:35,209 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:35,209 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 163, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:35,209 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:35,209 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:35,288 INFO storage.BlockManagerInfo: Added broadcast_174_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:35,301 INFO storage.BlockManagerInfo: Added broadcast_173_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:36,181 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 164.0 (TID 358) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:36,182 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 163.0 (TID 356) in 1410 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:36,187 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 165.0 (TID 359) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:36,187 INFO storage.BlockManagerInfo: Added broadcast_174_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:36,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 163.0 (TID 355) in 2079 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:36,188 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:36,188 INFO scheduler.DAGScheduler: ShuffleMapStage 163 (foreachPartition at KVRelation.scala:125) finished in 12.029 s\n",
      "2023-02-01 07:14:36,188 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:36,188 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 164, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:36,188 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:36,188 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:36,193 INFO storage.BlockManagerInfo: Added broadcast_176_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:36,203 INFO storage.BlockManagerInfo: Added broadcast_175_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:36,268 INFO storage.BlockManagerInfo: Added broadcast_173_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:36,998 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 165.0 (TID 360) (10.200.136.126, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:36,998 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 164.0 (TID 357) in 1790 ms on 10.200.136.126 (executor 2) (1/2)\n",
      "2023-02-01 07:14:37,005 INFO storage.BlockManagerInfo: Added broadcast_176_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:37,092 INFO storage.BlockManagerInfo: Added broadcast_175_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:37,212 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 166.0 (TID 361) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:37,212 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 165.0 (TID 359) in 1025 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:14:37,219 INFO storage.BlockManagerInfo: Added broadcast_178_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:37,229 INFO storage.BlockManagerInfo: Added broadcast_177_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:37,573 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 166.0 (TID 362) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:37,574 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 164.0 (TID 358) in 1393 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:14:37,574 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:37,574 INFO scheduler.DAGScheduler: ShuffleMapStage 164 (foreachPartition at KVRelation.scala:125) finished in 13.371 s\n",
      "2023-02-01 07:14:37,574 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:37,574 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 165, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:37,574 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:37,574 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:37,579 INFO storage.BlockManagerInfo: Added broadcast_178_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:37,591 INFO storage.BlockManagerInfo: Added broadcast_177_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:38,510 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 167.0 (TID 363) (10.200.136.126, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:38,510 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 165.0 (TID 360) in 1512 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:14:38,510 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:38,511 INFO scheduler.DAGScheduler: ShuffleMapStage 165 (foreachPartition at KVRelation.scala:125) finished in 14.263 s\n",
      "2023-02-01 07:14:38,511 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:38,511 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 166, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:38,511 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:38,511 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:38,571 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 167.0 (TID 364) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:38,571 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 166.0 (TID 362) in 998 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:38,576 INFO storage.BlockManagerInfo: Added broadcast_180_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:38,585 INFO storage.BlockManagerInfo: Added broadcast_179_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1005.0 MiB)\n",
      "2023-02-01 07:14:38,589 INFO storage.BlockManagerInfo: Added broadcast_180_piece0 in memory on 10.200.136.126:44915 (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:38,673 INFO storage.BlockManagerInfo: Added broadcast_179_piece0 in memory on 10.200.136.126:44915 (size: 54.8 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:38,807 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 168.0 (TID 365) (10.200.136.127, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:38,807 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 166.0 (TID 361) in 1595 ms on 10.200.136.127 (executor 3) (2/2)\n",
      "2023-02-01 07:14:38,808 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:38,808 INFO scheduler.DAGScheduler: ShuffleMapStage 166 (foreachPartition at KVRelation.scala:125) finished in 14.517 s\n",
      "2023-02-01 07:14:38,808 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:38,808 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168, ShuffleMapStage 167)\n",
      "2023-02-01 07:14:38,808 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:38,808 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:38,890 INFO storage.BlockManagerInfo: Added broadcast_182_piece0 in memory on 10.200.136.127:43545 (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:38,901 INFO storage.BlockManagerInfo: Added broadcast_181_piece0 in memory on 10.200.136.127:43545 (size: 54.8 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:39,674 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 168.0 (TID 366) (10.200.140.94, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:39,674 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 167.0 (TID 364) in 1104 ms on 10.200.140.94 (executor 1) (1/2)\n",
      "2023-02-01 07:14:39,680 INFO storage.BlockManagerInfo: Added broadcast_182_piece0 in memory on 10.200.140.94:41276 (size: 35.9 KiB, free: 1005.0 MiB)\n",
      "2023-02-01 07:14:39,867 INFO storage.BlockManagerInfo: Added broadcast_181_piece0 in memory on 10.200.140.94:41276 (size: 54.8 KiB, free: 1004.9 MiB)\n",
      "2023-02-01 07:14:40,288 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 167.0 (TID 363) in 1778 ms on 10.200.136.126 (executor 2) (2/2)\n",
      "2023-02-01 07:14:40,288 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:40,289 INFO scheduler.DAGScheduler: ShuffleMapStage 167 (foreachPartition at KVRelation.scala:125) finished in 15.953 s\n",
      "2023-02-01 07:14:40,289 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:40,289 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 168)\n",
      "2023-02-01 07:14:40,289 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:40,289 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:40,690 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 168.0 (TID 365) in 1883 ms on 10.200.136.127 (executor 3) (1/2)\n",
      "2023-02-01 07:14:41,188 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 168.0 (TID 366) in 1514 ms on 10.200.140.94 (executor 1) (2/2)\n",
      "2023-02-01 07:14:41,188 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:14:41,189 INFO scheduler.DAGScheduler: ShuffleMapStage 168 (foreachPartition at KVRelation.scala:125) finished in 16.809 s\n",
      "2023-02-01 07:14:41,189 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 07:14:41,189 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 07:14:41,189 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 07:14:41,189 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 07:14:41,205 INFO adaptive.ShufflePartitionsUtil: For shuffle(71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86), advisory target size: 67108864, actual target size 3159149, minimum partition size: 1048576\n",
      "2023-02-01 07:14:41,274 INFO storage.BlockManagerInfo: Removed broadcast_166_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,275 INFO storage.BlockManagerInfo: Removed broadcast_166_piece0 on 10.200.140.94:41276 in memory (size: 35.8 KiB, free: 1004.9 MiB)\n",
      "2023-02-01 07:14:41,275 INFO storage.BlockManagerInfo: Removed broadcast_166_piece0 on 10.200.136.126:44915 in memory (size: 35.8 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,281 INFO storage.BlockManagerInfo: Removed broadcast_174_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,281 INFO storage.BlockManagerInfo: Removed broadcast_174_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,281 INFO storage.BlockManagerInfo: Removed broadcast_174_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.0 MiB)\n",
      "2023-02-01 07:14:41,292 INFO storage.BlockManagerInfo: Removed broadcast_158_piece0 on 10.200.140.94:41276 in memory (size: 35.8 KiB, free: 1005.0 MiB)\n",
      "2023-02-01 07:14:41,292 INFO storage.BlockManagerInfo: Removed broadcast_158_piece0 on 10.200.136.127:43545 in memory (size: 35.8 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,298 INFO storage.BlockManagerInfo: Removed broadcast_158_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,301 INFO storage.BlockManagerInfo: Removed broadcast_164_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,302 INFO storage.BlockManagerInfo: Removed broadcast_164_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:41,302 INFO storage.BlockManagerInfo: Removed broadcast_164_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,369 INFO storage.BlockManagerInfo: Removed broadcast_168_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,371 INFO storage.BlockManagerInfo: Removed broadcast_168_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,371 INFO storage.BlockManagerInfo: Removed broadcast_168_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,374 INFO storage.BlockManagerInfo: Removed broadcast_160_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,375 INFO storage.BlockManagerInfo: Removed broadcast_160_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,375 INFO storage.BlockManagerInfo: Removed broadcast_160_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,377 INFO storage.BlockManagerInfo: Removed broadcast_178_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,378 INFO storage.BlockManagerInfo: Removed broadcast_178_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,378 INFO storage.BlockManagerInfo: Removed broadcast_178_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:41,381 INFO storage.BlockManagerInfo: Removed broadcast_172_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,382 INFO storage.BlockManagerInfo: Removed broadcast_172_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:41,382 INFO storage.BlockManagerInfo: Removed broadcast_172_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,384 INFO storage.BlockManagerInfo: Removed broadcast_162_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,385 INFO storage.BlockManagerInfo: Removed broadcast_162_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,385 INFO storage.BlockManagerInfo: Removed broadcast_162_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,392 INFO storage.BlockManagerInfo: Removed broadcast_180_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,393 INFO storage.BlockManagerInfo: Removed broadcast_180_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,393 INFO storage.BlockManagerInfo: Removed broadcast_180_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,396 INFO storage.BlockManagerInfo: Removed broadcast_170_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,397 INFO storage.BlockManagerInfo: Removed broadcast_170_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,428 INFO codegen.CodeGenerator: Code generated in 10.084223 ms\n",
      "2023-02-01 07:14:41,449 INFO codegen.CodeGenerator: Code generated in 8.484167 ms\n",
      "2023-02-01 07:14:41,467 INFO storage.BlockManagerInfo: Removed broadcast_170_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.2 MiB)\n",
      "2023-02-01 07:14:41,470 INFO storage.BlockManagerInfo: Removed broadcast_182_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,470 INFO storage.BlockManagerInfo: Removed broadcast_182_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,470 INFO codegen.CodeGenerator: Code generated in 8.681587 ms\n",
      "2023-02-01 07:14:41,471 INFO storage.BlockManagerInfo: Removed broadcast_182_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,473 INFO storage.BlockManagerInfo: Removed broadcast_152_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,474 INFO storage.BlockManagerInfo: Removed broadcast_152_piece0 on 10.200.136.126:44915 in memory (size: 36.2 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,474 INFO storage.BlockManagerInfo: Removed broadcast_152_piece0 on 10.200.136.127:43545 in memory (size: 36.2 KiB, free: 1005.4 MiB)\n",
      "2023-02-01 07:14:41,476 INFO storage.BlockManagerInfo: Removed broadcast_154_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 36.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,477 INFO storage.BlockManagerInfo: Removed broadcast_154_piece0 on 10.200.140.94:41276 in memory (size: 36.7 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,477 INFO storage.BlockManagerInfo: Removed broadcast_154_piece0 on 10.200.136.126:44915 in memory (size: 36.7 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:41,479 INFO storage.BlockManagerInfo: Removed broadcast_156_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,480 INFO storage.BlockManagerInfo: Removed broadcast_156_piece0 on 10.200.140.94:41276 in memory (size: 35.9 KiB, free: 1005.3 MiB)\n",
      "2023-02-01 07:14:41,481 INFO storage.BlockManagerInfo: Removed broadcast_156_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:41,483 INFO storage.BlockManagerInfo: Removed broadcast_176_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 35.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,484 INFO storage.BlockManagerInfo: Removed broadcast_176_piece0 on 10.200.136.126:44915 in memory (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:41,484 INFO storage.BlockManagerInfo: Removed broadcast_176_piece0 on 10.200.136.127:43545 in memory (size: 35.9 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:14:41,491 INFO codegen.CodeGenerator: Code generated in 8.447793 ms\n",
      "2023-02-01 07:14:41,513 INFO codegen.CodeGenerator: Code generated in 9.126249 ms\n",
      "2023-02-01 07:14:41,534 INFO codegen.CodeGenerator: Code generated in 8.53546 ms\n",
      "2023-02-01 07:14:41,555 INFO codegen.CodeGenerator: Code generated in 8.313705 ms\n",
      "2023-02-01 07:14:41,574 INFO codegen.CodeGenerator: Code generated in 7.215616 ms\n",
      "2023-02-01 07:14:41,594 INFO codegen.CodeGenerator: Code generated in 8.099559 ms\n",
      "2023-02-01 07:14:41,614 INFO codegen.CodeGenerator: Code generated in 7.700668 ms\n",
      "2023-02-01 07:14:41,633 INFO codegen.CodeGenerator: Code generated in 7.866952 ms\n",
      "2023-02-01 07:14:41,653 INFO codegen.CodeGenerator: Code generated in 8.056237 ms\n",
      "2023-02-01 07:14:41,672 INFO codegen.CodeGenerator: Code generated in 7.373276 ms\n",
      "2023-02-01 07:14:41,693 INFO codegen.CodeGenerator: Code generated in 8.800562 ms\n",
      "2023-02-01 07:14:41,714 INFO codegen.CodeGenerator: Code generated in 8.558152 ms\n",
      "2023-02-01 07:14:41,736 INFO codegen.CodeGenerator: Code generated in 8.672105 ms\n",
      "2023-02-01 07:14:41,789 INFO spark.SparkContext: Starting job: foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 07:14:41,791 INFO scheduler.DAGScheduler: Got job 94 (foreachPartition at KVRelation.scala:125) with 48 output partitions\n",
      "2023-02-01 07:14:41,791 INFO scheduler.DAGScheduler: Final stage: ResultStage 185 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 07:14:41,791 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 183, ShuffleMapStage 169, ShuffleMapStage 184, ShuffleMapStage 176, ShuffleMapStage 170, ShuffleMapStage 177, ShuffleMapStage 178, ShuffleMapStage 179, ShuffleMapStage 171, ShuffleMapStage 180, ShuffleMapStage 172, ShuffleMapStage 173, ShuffleMapStage 174, ShuffleMapStage 181, ShuffleMapStage 175, ShuffleMapStage 182)\n",
      "2023-02-01 07:14:41,792 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 07:14:41,792 INFO scheduler.DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[901] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 07:14:41,868 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1519.3 KiB\n",
      "2023-02-01 07:14:41,868 INFO memory.MemoryStore: Block broadcast_183 stored as values in memory (estimated size 1519.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:41,874 INFO memory.MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 429.5 KiB, free 2.2 GiB)\n",
      "2023-02-01 07:14:41,874 INFO storage.BlockManagerInfo: Added broadcast_183_piece0 in memory on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 (size: 429.5 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:14:41,874 INFO spark.SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 07:14:41,875 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ResultStage 185 (MapPartitionsRDD[901] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-02-01 07:14:41,875 INFO scheduler.TaskSchedulerImpl: Adding task set 185.0 with 48 tasks resource profile 0\n",
      "2023-02-01 07:14:41,876 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 185.0 (TID 367) (10.200.140.94, executor 1, partition 3, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:41,877 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 185.0 (TID 368) (10.200.136.126, executor 2, partition 0, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:41,877 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 185.0 (TID 369) (10.200.136.127, executor 3, partition 1, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:41,889 INFO storage.BlockManagerInfo: Added broadcast_183_piece0 in memory on 10.200.136.127:43545 (size: 429.5 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:41,889 INFO storage.BlockManagerInfo: Added broadcast_183_piece0 in memory on 10.200.140.94:41276 (size: 429.5 KiB, free: 1004.9 MiB)\n",
      "2023-02-01 07:14:41,889 INFO storage.BlockManagerInfo: Added broadcast_183_piece0 in memory on 10.200.136.126:44915 (size: 429.5 KiB, free: 1005.1 MiB)\n",
      "2023-02-01 07:14:42,091 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:42,094 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 71 to 10.200.136.126:48696\n",
      "2023-02-01 07:14:42,367 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 72 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:49,090 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 185.0 (TID 370) (10.200.136.126, executor 2, partition 2, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:49,091 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 185.0 (TID 368) in 7214 ms on 10.200.136.126 (executor 2) (1/48)\n",
      "2023-02-01 07:14:49,270 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 185.0 (TID 371) (10.200.140.94, executor 1, partition 4, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:49,270 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 185.0 (TID 367) in 7394 ms on 10.200.140.94 (executor 1) (2/48)\n",
      "2023-02-01 07:14:49,487 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 185.0 (TID 372) (10.200.136.127, executor 3, partition 6, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:49,488 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 185.0 (TID 369) in 7611 ms on 10.200.136.127 (executor 3) (3/48)\n",
      "2023-02-01 07:14:49,603 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:50,588 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 185.0 (TID 373) (10.200.140.94, executor 1, partition 5, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:50,588 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 185.0 (TID 371) in 1318 ms on 10.200.140.94 (executor 1) (4/48)\n",
      "2023-02-01 07:14:50,910 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 185.0 (TID 374) (10.200.136.127, executor 3, partition 7, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:50,910 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 185.0 (TID 372) in 1423 ms on 10.200.136.127 (executor 3) (5/48)\n",
      "2023-02-01 07:14:51,589 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 185.0 (TID 375) (10.200.140.94, executor 1, partition 8, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:51,589 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 185.0 (TID 373) in 1002 ms on 10.200.140.94 (executor 1) (6/48)\n",
      "2023-02-01 07:14:51,624 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 73 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:51,903 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 185.0 (TID 376) (10.200.136.127, executor 3, partition 9, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:51,903 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 185.0 (TID 374) in 993 ms on 10.200.136.127 (executor 3) (7/48)\n",
      "2023-02-01 07:14:51,939 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:52,668 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 185.0 (TID 377) (10.200.140.94, executor 1, partition 10, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:52,668 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 185.0 (TID 375) in 1080 ms on 10.200.140.94 (executor 1) (8/48)\n",
      "2023-02-01 07:14:52,778 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 74 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:53,000 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 185.0 (TID 378) (10.200.136.127, executor 3, partition 11, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:53,000 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 185.0 (TID 376) in 1098 ms on 10.200.136.127 (executor 3) (9/48)\n",
      "2023-02-01 07:14:53,992 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 185.0 (TID 379) (10.200.140.94, executor 1, partition 15, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:53,992 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 185.0 (TID 377) in 1324 ms on 10.200.140.94 (executor 1) (10/48)\n",
      "2023-02-01 07:14:54,098 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 76 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:54,271 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 185.0 (TID 380) (10.200.136.127, executor 3, partition 12, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:54,271 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 185.0 (TID 378) in 1271 ms on 10.200.136.127 (executor 3) (11/48)\n",
      "2023-02-01 07:14:54,309 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:55,487 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 185.0 (TID 381) (10.200.136.127, executor 3, partition 13, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:55,487 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 185.0 (TID 380) in 1216 ms on 10.200.136.127 (executor 3) (12/48)\n",
      "2023-02-01 07:14:55,587 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 185.0 (TID 382) (10.200.140.94, executor 1, partition 16, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:55,587 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 185.0 (TID 379) in 1596 ms on 10.200.140.94 (executor 1) (13/48)\n",
      "2023-02-01 07:14:55,649 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 185.0 (TID 383) (10.200.136.126, executor 2, partition 14, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:55,649 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 185.0 (TID 370) in 6559 ms on 10.200.136.126 (executor 2) (14/48)\n",
      "2023-02-01 07:14:55,687 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 75 to 10.200.136.126:48696\n",
      "2023-02-01 07:14:56,484 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 185.0 (TID 384) (10.200.136.127, executor 3, partition 18, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:56,484 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 185.0 (TID 381) in 998 ms on 10.200.136.127 (executor 3) (15/48)\n",
      "2023-02-01 07:14:56,588 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:56,668 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 185.0 (TID 385) (10.200.140.94, executor 1, partition 17, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:56,669 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 185.0 (TID 382) in 1083 ms on 10.200.140.94 (executor 1) (16/48)\n",
      "2023-02-01 07:14:57,086 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 185.0 (TID 386) (10.200.136.126, executor 2, partition 21, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:57,087 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 185.0 (TID 383) in 1438 ms on 10.200.136.126 (executor 2) (17/48)\n",
      "2023-02-01 07:14:57,122 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to 10.200.136.126:48696\n",
      "2023-02-01 07:14:57,323 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 185.0 (TID 387) (10.200.136.127, executor 3, partition 19, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:57,323 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 185.0 (TID 384) in 839 ms on 10.200.136.127 (executor 3) (18/48)\n",
      "2023-02-01 07:14:57,595 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 185.0 (TID 388) (10.200.140.94, executor 1, partition 20, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:57,596 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 185.0 (TID 385) in 928 ms on 10.200.140.94 (executor 1) (19/48)\n",
      "2023-02-01 07:14:57,768 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 77 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:58,084 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 185.0 (TID 389) (10.200.136.127, executor 3, partition 24, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:58,085 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 185.0 (TID 387) in 762 ms on 10.200.136.127 (executor 3) (20/48)\n",
      "2023-02-01 07:14:58,118 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:58,387 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 185.0 (TID 390) (10.200.136.126, executor 2, partition 22, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:58,388 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 185.0 (TID 386) in 1302 ms on 10.200.136.126 (executor 2) (21/48)\n",
      "2023-02-01 07:14:58,785 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 185.0 (TID 391) (10.200.140.94, executor 1, partition 23, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:58,786 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 185.0 (TID 388) in 1191 ms on 10.200.140.94 (executor 1) (22/48)\n",
      "2023-02-01 07:14:58,878 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 78 to 10.200.140.94:57642\n",
      "2023-02-01 07:14:58,912 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 185.0 (TID 392) (10.200.136.127, executor 3, partition 25, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:58,912 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 185.0 (TID 389) in 828 ms on 10.200.136.127 (executor 3) (23/48)\n",
      "2023-02-01 07:14:59,302 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 185.0 (TID 393) (10.200.136.126, executor 2, partition 26, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:59,303 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 185.0 (TID 390) in 916 ms on 10.200.136.126 (executor 2) (24/48)\n",
      "2023-02-01 07:14:59,386 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 79 to 10.200.136.126:48696\n",
      "2023-02-01 07:14:59,791 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 185.0 (TID 394) (10.200.140.94, executor 1, partition 27, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:59,792 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 185.0 (TID 391) in 1007 ms on 10.200.140.94 (executor 1) (25/48)\n",
      "2023-02-01 07:14:59,818 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 185.0 (TID 395) (10.200.136.127, executor 3, partition 30, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:14:59,819 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 185.0 (TID 392) in 907 ms on 10.200.136.127 (executor 3) (26/48)\n",
      "2023-02-01 07:14:59,856 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 81 to 10.200.136.127:35826\n",
      "2023-02-01 07:14:59,974 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to 10.200.140.94:57642\n",
      "2023-02-01 07:15:00,305 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 185.0 (TID 396) (10.200.136.126, executor 2, partition 28, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:00,306 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 185.0 (TID 393) in 1004 ms on 10.200.136.126 (executor 2) (27/48)\n",
      "2023-02-01 07:15:00,488 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 80 to 10.200.136.126:48696\n",
      "2023-02-01 07:15:00,659 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 185.0 (TID 397) (10.200.136.127, executor 3, partition 31, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:00,660 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 185.0 (TID 395) in 842 ms on 10.200.136.127 (executor 3) (28/48)\n",
      "2023-02-01 07:15:00,803 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 185.0 (TID 398) (10.200.140.94, executor 1, partition 29, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:00,803 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 185.0 (TID 394) in 1012 ms on 10.200.140.94 (executor 1) (29/48)\n",
      "2023-02-01 07:15:01,481 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 185.0 (TID 399) (10.200.136.127, executor 3, partition 32, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:01,481 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 185.0 (TID 397) in 822 ms on 10.200.136.127 (executor 3) (30/48)\n",
      "2023-02-01 07:15:01,483 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 185.0 (TID 400) (10.200.136.126, executor 2, partition 33, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:01,483 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 185.0 (TID 396) in 1178 ms on 10.200.136.126 (executor 2) (31/48)\n",
      "2023-02-01 07:15:01,584 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to 10.200.136.126:48696\n",
      "2023-02-01 07:15:01,788 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 185.0 (TID 401) (10.200.140.94, executor 1, partition 34, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:01,789 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 185.0 (TID 398) in 986 ms on 10.200.140.94 (executor 1) (32/48)\n",
      "2023-02-01 07:15:01,967 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 82 to 10.200.140.94:57642\n",
      "2023-02-01 07:15:02,099 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 185.0 (TID 402) (10.200.136.127, executor 3, partition 36, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:02,099 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 185.0 (TID 399) in 618 ms on 10.200.136.127 (executor 3) (33/48)\n",
      "2023-02-01 07:15:02,134 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to 10.200.136.127:35826\n",
      "2023-02-01 07:15:02,475 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 185.0 (TID 403) (10.200.136.126, executor 2, partition 35, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:02,475 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 185.0 (TID 400) in 992 ms on 10.200.136.126 (executor 2) (34/48)\n",
      "2023-02-01 07:15:03,182 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 185.0 (TID 404) (10.200.140.94, executor 1, partition 39, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:03,182 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 185.0 (TID 401) in 1394 ms on 10.200.140.94 (executor 1) (35/48)\n",
      "2023-02-01 07:15:03,270 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to 10.200.140.94:57642\n",
      "2023-02-01 07:15:03,321 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 185.0 (TID 405) (10.200.136.127, executor 3, partition 37, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:03,321 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 185.0 (TID 402) in 1222 ms on 10.200.136.127 (executor 3) (36/48)\n",
      "2023-02-01 07:15:03,326 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 185.0 (TID 406) (10.200.136.126, executor 2, partition 38, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:03,327 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 185.0 (TID 403) in 852 ms on 10.200.136.126 (executor 2) (37/48)\n",
      "2023-02-01 07:15:03,400 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 83 to 10.200.136.126:48696\n",
      "2023-02-01 07:15:04,085 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 185.0 (TID 407) (10.200.136.127, executor 3, partition 40, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:04,086 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 185.0 (TID 405) in 765 ms on 10.200.136.127 (executor 3) (38/48)\n",
      "2023-02-01 07:15:04,121 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 84 to 10.200.136.127:35826\n",
      "2023-02-01 07:15:04,182 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 185.0 (TID 408) (10.200.140.94, executor 1, partition 41, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:04,182 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 185.0 (TID 404) in 1000 ms on 10.200.140.94 (executor 1) (39/48)\n",
      "2023-02-01 07:15:04,250 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 185.0 (TID 409) (10.200.136.126, executor 2, partition 42, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:04,250 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 185.0 (TID 406) in 924 ms on 10.200.136.126 (executor 2) (40/48)\n",
      "2023-02-01 07:15:04,290 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to 10.200.136.126:48696\n",
      "2023-02-01 07:15:04,874 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 185.0 (TID 410) (10.200.136.127, executor 3, partition 45, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:04,874 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 185.0 (TID 407) in 789 ms on 10.200.136.127 (executor 3) (41/48)\n",
      "2023-02-01 07:15:04,914 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to 10.200.136.127:35826\n",
      "2023-02-01 07:15:04,990 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 185.0 (TID 411) (10.200.140.94, executor 1, partition 43, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:04,990 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 185.0 (TID 408) in 809 ms on 10.200.140.94 (executor 1) (42/48)\n",
      "2023-02-01 07:15:05,083 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 85 to 10.200.140.94:57642\n",
      "2023-02-01 07:15:05,500 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 185.0 (TID 412) (10.200.136.126, executor 2, partition 44, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:05,500 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 185.0 (TID 409) in 1250 ms on 10.200.136.126 (executor 2) (43/48)\n",
      "2023-02-01 07:15:05,814 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 185.0 (TID 413) (10.200.136.127, executor 3, partition 46, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:05,814 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 185.0 (TID 410) in 941 ms on 10.200.136.127 (executor 3) (44/48)\n",
      "2023-02-01 07:15:06,068 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 185.0 (TID 414) (10.200.140.94, executor 1, partition 47, NODE_LOCAL, 4581 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 07:15:06,069 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 185.0 (TID 411) in 1079 ms on 10.200.140.94 (executor 1) (45/48)\n",
      "2023-02-01 07:15:06,176 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 86 to 10.200.140.94:57642\n",
      "2023-02-01 07:15:06,324 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 185.0 (TID 412) in 825 ms on 10.200.136.126 (executor 2) (46/48)\n",
      "2023-02-01 07:15:06,696 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 185.0 (TID 413) in 882 ms on 10.200.136.127 (executor 3) (47/48)\n",
      "2023-02-01 07:15:07,393 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 185.0 (TID 414) in 1325 ms on 10.200.140.94 (executor 1) (48/48)\n",
      "2023-02-01 07:15:07,393 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool \n",
      "2023-02-01 07:15:07,393 INFO scheduler.DAGScheduler: ResultStage 185 (foreachPartition at KVRelation.scala:125) finished in 25.586 s\n",
      "2023-02-01 07:15:07,394 INFO scheduler.DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 07:15:07,394 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished\n",
      "2023-02-01 07:15:07,394 INFO scheduler.DAGScheduler: Job 94 finished: foreachPartition at KVRelation.scala:125, took 25.604328 s\n",
      "2023-02-01 07:15:07,417 INFO storage.BlockManagerInfo: Removed broadcast_183_piece0 on transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:7079 in memory (size: 429.5 KiB, free: 2.2 GiB)\n",
      "2023-02-01 07:15:07,418 INFO storage.BlockManagerInfo: Removed broadcast_183_piece0 on 10.200.136.126:44915 in memory (size: 429.5 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:15:07,419 INFO storage.BlockManagerInfo: Removed broadcast_183_piece0 on 10.200.136.127:43545 in memory (size: 429.5 KiB, free: 1005.5 MiB)\n",
      "2023-02-01 07:15:07,468 INFO storage.BlockManagerInfo: Removed broadcast_183_piece0 on 10.200.140.94:41276 in memory (size: 429.5 KiB, free: 1005.3 MiB)\n",
      "> 2023-02-01 07:15:07,707 [info] ingestion task completed, targets:\n",
      "> 2023-02-01 07:15:07,708 [info] [{'name': 'parquet', 'kind': 'parquet', 'path': 'v3io:///projects/fraud-demo-dani/FeatureStore/transactions/parquet/sets/transactions/{run_id}/', 'status': 'ready', 'updated': '2023-02-01T07:14:21.605577+00:00', 'run_id': '1675235564750_621', 'partitioned': True}, {'name': 'nosql', 'kind': 'nosql', 'path': 'v3io:///projects/fraud-demo-dani/FeatureStore/transactions/nosql/sets/transactions/{run_id}/', 'status': 'ready', 'updated': '2023-02-01T07:15:07.592084+00:00', 'run_id': '1675235564750_621', 'partitioned': False}]\n",
      "2023-02-01 07:15:07,719 INFO server.AbstractConnector: Stopped Spark@c49104b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-02-01 07:15:07,720 INFO ui.SparkUI: Stopped Spark web UI at http://transactions-ingest-f44332e0-495b00860bca00f6-driver-svc.default-tenant.svc:4040\n",
      "2023-02-01 07:15:07,724 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "2023-02-01 07:15:07,725 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "2023-02-01 07:15:07,731 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n",
      "2023-02-01 07:15:07,929 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-02-01 07:15:07,947 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-02-01 07:15:07,947 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-02-01 07:15:07,951 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-02-01 07:15:07,958 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-02-01 07:15:07,972 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "> 2023-02-01 07:15:08,139 [info] To track results use the CLI: {'info_cmd': 'mlrun get run 71e69146bc1c4656a929e0d50dbe9701 -p fraud-demo-dani', 'logs_cmd': 'mlrun logs 71e69146bc1c4656a929e0d50dbe9701 -p fraud-demo-dani'}\n",
      "> 2023-02-01 07:15:08,139 [info] Or click for UI: {'ui_url': 'https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/71e69146bc1c4656a929e0d50dbe9701/overview'}\n",
      "> 2023-02-01 07:15:08,139 [info] run executed, status=completed\n",
      "2023-02-01 07:15:08,688 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2023-02-01 07:15:08,688 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-277ac685-80b2-4958-b39d-06eacf45d28d/spark-ab481ff1-b206-4c24-bdf4-4260dc0806eb/pyspark-819231b8-ffff-4eca-bf1d-4933864cc255\n",
      "2023-02-01 07:15:08,693 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-277ac685-80b2-4958-b39d-06eacf45d28d/spark-ab481ff1-b206-4c24-bdf4-4260dc0806eb\n",
      "2023-02-01 07:15:08,696 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9e429367-de46-43f2-8997-097b6a6b2d6d\n",
      "final state: completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>fraud-demo-dani</td>\n",
       "      <td><div title=\"71e69146bc1c4656a929e0d50dbe9701\"><a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/71e69146bc1c4656a929e0d50dbe9701/overview\" target=\"_blank\" >...0dbe9701</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 01 07:04:04</td>\n",
       "      <td>completed</td>\n",
       "      <td>transactions-ingest</td>\n",
       "      <td><div class=\"dictlist\">job-type=feature-ingest</div><div class=\"dictlist\">feature-set=store://feature-sets/fraud-demo-dani/transactions</div><div class=\"dictlist\">v3io_user=dani</div><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=dani</div><div class=\"dictlist\">mlrun/client_version=1.3.0-rc16</div><div class=\"dictlist\">mlrun/job=transactions-ingest-f44332e0</div><div class=\"dictlist\">host=transactions-ingest-f44332e0-driver</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">infer_options=63</div><div class=\"dictlist\">overwrite=True</div><div class=\"dictlist\">featureset=store://feature-sets/fraud-demo-dani/transactions</div><div class=\"dictlist\">source={'kind': 'csv', 'name': 'flow1_source', 'path': 'v3io:///projects/fraud-demo-dani/artifacts/transactions_data.csv'}</div><div class=\"dictlist\">targets=None</div></td>\n",
       "      <td><div class=\"dictlist\">featureset=store://feature-sets/fraud-demo-dani/transactions:latest</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result4e654555-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result4e654555-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result4e654555\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result4e654555-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/71e69146bc1c4656a929e0d50dbe9701/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:15:09,889 [info] run executed, status=completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7f8bfa89feb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore.sources import CSVSource\n",
    "\n",
    "# Creating our partitioned csv source\n",
    "transaction_source = CSVSource(\"flow1_source\", path=project.get_artifact('transactions_data').target_path)\n",
    "\n",
    "# Ingest our transactions dataset through our defined pipeline\n",
    "fstore.ingest(transaction_set, transaction_source, run_config=run_config, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ingestion process, you will be able to see all the different features that were created with the help of the UI, as you can see in the image below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Features Catalog - fraud prevention](./images/features-catalog-transaction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>_c0</th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>es_barsandrestaurants_count_1d</th>\n",
       "      <th>es_tech_count_1d</th>\n",
       "      <th>es_sportsandtoys_count_1d</th>\n",
       "      <th>es_wellnessandbeauty_count_1d</th>\n",
       "      <th>es_hyper_count_1d</th>\n",
       "      <th>es_fashion_count_1d</th>\n",
       "      <th>es_home_count_1d</th>\n",
       "      <th>es_contents_count_1d</th>\n",
       "      <th>es_travel_count_1d</th>\n",
       "      <th>es_leisure_count_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57659</th>\n",
       "      <td>C118437987</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>222339</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57660</th>\n",
       "      <td>C1187730955</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>472810</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>35.99</td>\n",
       "      <td>0</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57661</th>\n",
       "      <td>C1187799344</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>172138</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57662</th>\n",
       "      <td>C1187979419</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>124869</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0</td>\n",
       "      <td>M209847108</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57663</th>\n",
       "      <td>C1189224644</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>343184</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>30.32</td>\n",
       "      <td>0</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            source  timestamp     _c0  step age  zipcodeOri  zipMerchant  \\\n",
       "57659   C118437987 2023-02-02  222339    75   2       28007        28007   \n",
       "57660  C1187730955 2023-02-02  472810   147   3       28007        28007   \n",
       "57661  C1187799344 2023-02-02  172138    60   3       28007        28007   \n",
       "57662  C1187979419 2023-02-02  124869    44   2       28007        28007   \n",
       "57663  C1189224644 2023-02-02  343184   111   2       28007        28007   \n",
       "\n",
       "       amount  fraud       target  ... es_barsandrestaurants_count_1d  \\\n",
       "57659   10.83      0  M1823072687  ...                           35.0   \n",
       "57660   35.99      0   M348934600  ...                           17.0   \n",
       "57661   10.75      0  M1823072687  ...                           12.0   \n",
       "57662    3.40      0   M209847108  ...                           17.0   \n",
       "57663   30.32      0   M348934600  ...                           21.0   \n",
       "\n",
       "      es_tech_count_1d es_sportsandtoys_count_1d  \\\n",
       "57659             35.0                      35.0   \n",
       "57660             17.0                      17.0   \n",
       "57661             12.0                      12.0   \n",
       "57662             17.0                      17.0   \n",
       "57663             21.0                      21.0   \n",
       "\n",
       "      es_wellnessandbeauty_count_1d  es_hyper_count_1d  es_fashion_count_1d  \\\n",
       "57659                          35.0               35.0                 35.0   \n",
       "57660                          17.0               17.0                 17.0   \n",
       "57661                          12.0               12.0                 12.0   \n",
       "57662                          17.0               17.0                 17.0   \n",
       "57663                          21.0               21.0                 21.0   \n",
       "\n",
       "       es_home_count_1d  es_contents_count_1d  es_travel_count_1d  \\\n",
       "57659              35.0                  35.0                35.0   \n",
       "57660              17.0                  17.0                17.0   \n",
       "57661              12.0                  12.0                12.0   \n",
       "57662              17.0                  17.0                17.0   \n",
       "57663              21.0                  21.0                21.0   \n",
       "\n",
       "       es_leisure_count_1d  \n",
       "57659                 35.0  \n",
       "57660                 17.0  \n",
       "57661                 12.0  \n",
       "57662                 17.0  \n",
       "57663                 21.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_set.to_dataframe().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>event</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1974668487</td>\n",
       "      <td>details_change</td>\n",
       "      <td>2023-01-31 16:22:19.133221086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1973547259</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-31 19:23:53.390361508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C515668508</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-31 15:56:59.671538302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source           event                     timestamp\n",
       "0  C1974668487  details_change 2023-01-31 16:22:19.133221086\n",
       "1  C1973547259           login 2023-01-31 19:23:53.390361508\n",
       "2   C515668508           login 2023-01-31 15:56:59.671538302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch our user_events dataset from the server\n",
    "user_events_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/events.csv', \n",
    "                               index_col=0, quotechar=\"\\'\", parse_dates=['timestamp'])\n",
    "\n",
    "# Adjust to the last 2 days to see the latest aggregations in our online feature vectors\n",
    "user_events_data = adjust_data_timespan(user_events_data, new_period='2d')\n",
    "\n",
    "# Preview\n",
    "user_events_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Create a FeatureSet and Preprocessing Pipeline\n",
    "\n",
    "Now we will define the events feature set.\n",
    "This is a pretty straight forward pipeline in which we only one hot encode the event categories and save the data to the default targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set = fstore.FeatureSet(\"events\",\n",
    "                                    entities=[fstore.Entity(\"source\")],\n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"user events feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"394pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 393.88 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 389.88,-94 389.88,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-27.05 40.7,-27.15 42.83,-27.3 44.92,-27.49 46.98,-27.74 48.99,-28.03 50.95,-28.36 52.84,-28.75 54.66,-29.18 56.4,-29.65 58.06,-30.16 59.63,-30.71 61.11,-31.31 62.49,-31.94 63.76,-32.61 64.93,-33.31 65.99,-34.04 66.93,-34.8 67.77,-35.59 68.48,-36.41 69.09,-37.25 69.58,-38.11 69.95,-38.99 70.21,-39.89 70.36,-40.8 70.4,-41.72 70.33,-42.65 70.16,-43.59 69.89,-44.53 69.53,-45.47 69.07,-46.41 68.52,-47.35 67.89,-48.28 67.18,-49.2 66.4,-50.11 65.55,-51.01 64.63,-51.89 63.65,-52.75 62.62,-53.59 61.53,-54.41 60.4,-55.2 59.23,-55.96 58.02,-56.69 56.78,-57.39 55.5,-58.06 54.2,-58.69 52.88,-59.29 51.53,-59.84 50.17,-60.35 48.79,-60.82 47.4,-61.25 46,-61.64 44.59,-61.97 43.17,-62.26 41.75,-62.51 40.32,-62.7 38.89,-62.85 37.45,-62.95 36.02,-63 34.58,-63 33.15,-62.95 31.71,-62.85 30.28,-62.7 28.85,-62.51 27.43,-62.26 26.01,-61.97 24.6,-61.64 23.2,-61.25 21.81,-60.82 20.43,-60.35 19.07,-59.84 17.72,-59.29 16.4,-58.69 15.1,-58.06 13.82,-57.39 12.58,-56.69 11.37,-55.96 10.2,-55.2 9.07,-54.41 7.98,-53.59 6.95,-52.75 5.97,-51.89 5.05,-51.01 4.2,-50.11 3.42,-49.2 2.71,-48.28 2.08,-47.35 1.53,-46.41 1.07,-45.47 0.71,-44.53 0.44,-43.59 0.27,-42.65 0.2,-41.72 0.24,-40.8 0.39,-39.89 0.65,-38.99 1.02,-38.11 1.51,-37.25 2.11,-36.41 2.83,-35.59 3.66,-34.8 4.61,-34.04 5.67,-33.31 6.84,-32.61 8.11,-31.94 9.49,-31.31 10.97,-30.71 12.54,-30.16 14.2,-29.65 15.94,-29.18 17.76,-28.75 19.65,-28.36 21.61,-28.03 23.62,-27.74 25.68,-27.49 27.77,-27.3 29.9,-27.15 32.05,-27.05 34.22,-27 36.38,-27 38.55,-27.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>OneHotEncoder</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"191.74\" cy=\"-45\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.74\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;OneHotEncoder -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;OneHotEncoder</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.81,-45C77.82,-45 86.77,-45 96.08,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.29,-48.5 106.29,-45 96.29,-41.5 96.29,-48.5\"/>\n",
       "</g>\n",
       "<!-- parquet/parquet -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.88,-86.73C385.88,-88.53 369.52,-90 349.38,-90 329.24,-90 312.88,-88.53 312.88,-86.73 312.88,-86.73 312.88,-57.27 312.88,-57.27 312.88,-55.47 329.24,-54 349.38,-54 369.52,-54 385.88,-55.47 385.88,-57.27 385.88,-57.27 385.88,-86.73 385.88,-86.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.88,-86.73C385.88,-84.92 369.52,-83.45 349.38,-83.45 329.24,-83.45 312.88,-84.92 312.88,-86.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"349.38\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">parquet</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;parquet/parquet -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.3,-56.37C273.13,-58.94 288.57,-61.62 302.39,-64.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.15,-67.53 312.6,-65.79 303.35,-60.64 302.15,-67.53\"/>\n",
       "</g>\n",
       "<!-- nosql/nosql -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.88,-32.73C376.88,-34.53 364.56,-36 349.38,-36 334.21,-36 321.88,-34.53 321.88,-32.73 321.88,-32.73 321.88,-3.27 321.88,-3.27 321.88,-1.47 334.21,0 349.38,0 364.56,0 376.88,-1.47 376.88,-3.27 376.88,-3.27 376.88,-32.73 376.88,-32.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.88,-32.73C376.88,-30.92 364.56,-29.45 349.38,-29.45 334.21,-29.45 321.88,-30.92 321.88,-32.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"349.38\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">nosql</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;nosql/nosql -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.3,-33.63C276.56,-30.46 295.75,-27.13 311.7,-24.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.42,-27.79 321.68,-22.63 311.22,-20.89 312.42,-27.79\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8bfa58c370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and add value mapping\n",
    "events_mapping = {'event': list(user_events_data.event.unique())}\n",
    "\n",
    "# One Hot Encode\n",
    "user_events_set.graph.to(OneHotEncoder(mapping=events_mapping))\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "user_events_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "user_events_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:15:47,282 [info] writing to target parquet, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/events/parquet/sets/events/1675235747282_902/', 'format': 'parquet', 'partitionBy': ['year', 'month', 'day', 'hour']}\n",
      "> 2023-02-01 07:15:56,617 [info] writing to target nosql, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/events/nosql/sets/events/1675235747282_902/', 'format': 'io.iguaz.v3io.spark.sql.kv', 'key': 'source'}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"transaction_spark_function\").getOrCreate()\n",
    "\n",
    "# Ingestion of our newly created events feature set locally\n",
    "events_df = fstore.ingest(user_events_set, user_events_data, spark_context=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a labels dataset for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Create a FeatureSet\n",
    "This feature set contains the label for the fraud demo, it will be ingested directly to the default targets without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    labels = df[['fraud','timestamp']].copy()\n",
    "    labels = labels.rename(columns={\"fraud\": \"label\"})\n",
    "    labels['timestamp'] = labels['timestamp'].astype(\"datetime64[ms]\")\n",
    "    labels['label'] = labels['label'].astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"151pt\" height=\"193pt\"\n",
       " viewBox=\"0.00 0.00 150.99 193.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 189.25)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-189.25 146.99,-189.25 146.99,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"74.74,-149.3 76.89,-149.4 79.02,-149.55 81.12,-149.74 83.18,-149.99 85.19,-150.28 87.14,-150.61 89.03,-151 90.85,-151.43 92.6,-151.9 94.26,-152.41 95.83,-152.96 97.3,-153.56 98.68,-154.19 99.96,-154.86 101.12,-155.56 102.18,-156.29 103.13,-157.05 103.96,-157.84 104.68,-158.66 105.28,-159.5 105.77,-160.36 106.14,-161.24 106.4,-162.14 106.55,-163.05 106.59,-163.97 106.53,-164.9 106.36,-165.84 106.09,-166.78 105.72,-167.72 105.26,-168.66 104.72,-169.6 104.09,-170.53 103.38,-171.45 102.59,-172.36 101.74,-173.26 100.82,-174.14 99.84,-175 98.81,-175.84 97.73,-176.66 96.6,-177.45 95.42,-178.21 94.21,-178.94 92.97,-179.64 91.7,-180.31 90.39,-180.94 89.07,-181.54 87.73,-182.09 86.36,-182.6 84.98,-183.07 83.59,-183.5 82.19,-183.89 80.78,-184.22 79.36,-184.51 77.94,-184.76 76.51,-184.95 75.08,-185.1 73.65,-185.2 72.21,-185.25 70.78,-185.25 69.34,-185.2 67.91,-185.1 66.47,-184.95 65.05,-184.76 63.62,-184.51 62.2,-184.22 60.79,-183.89 59.39,-183.5 58,-183.07 56.62,-182.6 55.26,-182.09 53.92,-181.54 52.59,-180.94 51.29,-180.31 50.02,-179.64 48.77,-178.94 47.56,-178.21 46.39,-177.45 45.26,-176.66 44.18,-175.84 43.14,-175 42.16,-174.14 41.25,-173.26 40.39,-172.36 39.61,-171.45 38.9,-170.53 38.27,-169.6 37.72,-168.66 37.27,-167.72 36.9,-166.78 36.63,-165.84 36.46,-164.9 36.39,-163.97 36.43,-163.05 36.58,-162.14 36.84,-161.24 37.22,-160.36 37.71,-159.5 38.31,-158.66 39.03,-157.84 39.86,-157.05 40.81,-156.29 41.86,-155.56 43.03,-154.86 44.3,-154.19 45.68,-153.56 47.16,-152.96 48.73,-152.41 50.39,-151.9 52.13,-151.43 53.96,-151 55.85,-150.61 57.8,-150.28 59.81,-149.99 61.87,-149.74 63.97,-149.55 66.1,-149.4 68.25,-149.3 70.41,-149.25 72.58,-149.25 74.74,-149.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.49\" y=\"-163.55\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- create_labels -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>create_labels</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"71.49\" cy=\"-95.25\" rx=\"71.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.49\" y=\"-91.55\" font-family=\"Times,serif\" font-size=\"14.00\">create_labels</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;create_labels -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;create_labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.49,-148.95C71.49,-141.23 71.49,-131.96 71.49,-123.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.99,-123.35 71.49,-113.35 67.99,-123.35 74.99,-123.35\"/>\n",
       "</g>\n",
       "<!-- parquet/labels -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet/labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.99,-37.6C100.99,-39.68 87.77,-41.38 71.49,-41.38 55.22,-41.38 41.99,-39.68 41.99,-37.6 41.99,-37.6 41.99,-3.65 41.99,-3.65 41.99,-1.57 55.22,0.13 71.49,0.13 87.77,0.13 100.99,-1.57 100.99,-3.65 100.99,-3.65 100.99,-37.6 100.99,-37.6\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.99,-37.6C100.99,-35.52 87.77,-33.83 71.49,-33.83 55.22,-33.83 41.99,-35.52 41.99,-37.6\"/>\n",
       "<text text-anchor=\"start\" x=\"49.99\" y=\"-21.43\" font-family=\"Times,serif\" font-size=\"14.00\">labels</text>\n",
       "<text text-anchor=\"start\" x=\"52.49\" y=\"-12.22\" font-family=\"Times,serif\" font-size=\"8.00\">(parquet)</text>\n",
       "</g>\n",
       "<!-- create_labels&#45;&gt;parquet/labels -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>create_labels&#45;&gt;parquet/labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.49,-77.04C71.49,-69.4 71.49,-60.2 71.49,-51.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.99,-51.28 71.49,-41.28 67.99,-51.28 74.99,-51.28\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8b90093e80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore import ParquetTarget\n",
    "import os\n",
    "\n",
    "# Define the \"labels\" feature set\n",
    "labels_set = fstore.FeatureSet(\"labels\", \n",
    "                           entities=[fstore.Entity(\"source\")], \n",
    "                           timestamp_key='timestamp',\n",
    "                           description=\"training labels\",\n",
    "                           engine=\"pandas\")\n",
    "\n",
    "labels_set.graph.to(name=\"create_labels\", handler=create_labels)\n",
    "\n",
    "\n",
    "# specify only Parquet (offline) target since its not used for real-time\n",
    "target = ParquetTarget(name='labels',path=f'v3io:///projects/{project.name}/target.parquet')\n",
    "labels_set.set_targets([target], with_defaults=False)\n",
    "labels_set.plot(with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-31 21:15:54.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 20:02:48.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 03:48:04.432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label               timestamp\n",
       "source                                    \n",
       "C1000148617      0 2023-01-31 21:15:54.668\n",
       "C1000148617      0 2023-01-30 20:02:48.943\n",
       "C1000148617      0 2023-02-01 03:48:04.432"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest the labels feature set\n",
    "labels_df = fstore.ingest(labels_set, transactions_data)\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Deploy a real-time pipeline\n",
    "\n",
    "When dealing with real-time aggregation, it's important to be able to update these aggregations in real-time.\n",
    "For this purpose, we will create live serving functions that will update the online feature store of the `transactions` FeatureSet and `Events` FeatureSet.\n",
    "\n",
    "Using MLRun's `serving` runtime, craetes a nuclio function loaded with our feature set's computational graph definition\n",
    "and an `HttpSource` to define the HTTP trigger.\n",
    "\n",
    "Notice that the implementation below does not require any rewrite of the pipeline logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Deploy our FeatureSet live endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Storey engine to deploy the live endpoint.<br>\n",
    "Storey is a streaming library for real time event processing and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_set.spec.engine = 'storey'\n",
    "transaction_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "transaction_stream = f'v3io:///projects/{project.name}/streams/transaction'\n",
    "transaction_pusher = mlrun.datastore.get_stream_pusher(transaction_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:16:04,424 [info] Starting remote function deploy\n",
      "2023-02-01 07:16:04  (info) Deploying function\n",
      "2023-02-01 07:16:04  (info) Building\n",
      "2023-02-01 07:16:04  (info) Staging files and preparing base images\n",
      "2023-02-01 07:16:05  (info) Building processor image\n",
      "2023-02-01 07:17:10  (info) Build complete\n",
      "2023-02-01 07:17:18  (info) Function deploy complete\n",
      "> 2023-02-01 07:17:18,795 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-dani-transactions-ingest.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['fraud-demo-dani-transactions-ingest-fraud-demo-dani.default-tenant.app.vmdev94.lab.iguazeng.com/']}\n"
     ]
    }
   ],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=transaction_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "transaction_set_endpoint = fstore.deploy_ingestion_service(featureset=transaction_set, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining our `transactions` feature set we can now use MLRun and Storey to deploy it as a live endpoint, ready to ingest new data!\n",
    "\n",
    "Using MLRun's `serving` runtime, we will create a nuclio function loaded with our feature set's computational graph definition and an `HttpSource` to define the HTTP trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 123,\n",
       " 'age': '4',\n",
       " 'gender': 'M',\n",
       " 'zipcodeOri': 28007,\n",
       " 'zipMerchant': 28007,\n",
       " 'category': 'es_transportation',\n",
       " 'amount': 48.69,\n",
       " 'fraud': 0,\n",
       " 'timestamp': '2023-02-01 07:17:18.846234',\n",
       " 'source': 'C1165343027',\n",
       " 'target': 'M348934600',\n",
       " 'device': 'abc6880ac18649d2868082b2e4b1bf99'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Select a sample from the dataset and serialize it to JSON\n",
    "transaction_sample = json.loads(transactions_data.sample(1).to_json(orient='records'))[0]\n",
    "transaction_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "transaction_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"a59b075d-4ee3-40d6-b90b-decca77a034f\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(transaction_set_endpoint, json=transaction_sample).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_pusher.push(transaction_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Deploy our FeatureSet live endpoint\n",
    "Deploy the events feature set's ingestion service using the feature set and all the previously defined resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set.spec.engine = 'storey'\n",
    "user_events_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "events_stream = f'v3io:///projects/{project.name}/streams/events'\n",
    "events_pusher = mlrun.datastore.get_stream_pusher(events_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 07:17:19,077 [info] Starting remote function deploy\n",
      "2023-02-01 07:17:19  (info) Deploying function\n",
      "2023-02-01 07:17:19  (info) Building\n",
      "2023-02-01 07:17:19  (info) Staging files and preparing base images\n",
      "2023-02-01 07:17:19  (info) Building processor image\n",
      "2023-02-01 07:18:24  (info) Build complete\n",
      "2023-02-01 07:18:32  (info) Function deploy complete\n",
      "> 2023-02-01 07:18:33,152 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-dani-events-ingest.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['fraud-demo-dani-events-ingest-fraud-demo-dani.default-tenant.app.vmdev94.lab.iguazeng.com/']}\n"
     ]
    }
   ],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=events_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "events_set_endpoint = fstore.deploy_ingestion_service(featureset=user_events_set, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C1644291222',\n",
       " 'event': 'login',\n",
       " 'timestamp': '2023-02-01 07:18:33.199387'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a sample from the events dataset and serialize it to JSON\n",
    "user_events_sample = json.loads(user_events_data.sample(1).to_json(orient='records'))[0]\n",
    "user_events_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "user_events_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"8a60fc97-c41c-48d6-9ba4-e0e30c6f4b36\"}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(events_set_endpoint, json=user_events_sample).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_pusher.push(user_events_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "You've completed Part 1 of the data-ingestion with the feature store.\n",
    "Proceed to [Part 2](02-create-training-model.ipynb) to learn how to train an ML model using the feature store data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-extended",
   "language": "python",
   "name": "conda-env-mlrun-extended-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
