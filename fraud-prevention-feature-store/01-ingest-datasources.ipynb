{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - This demo works with the online feature store, which is currently not part of the Open Source default deployment.\n",
    "\n",
    "This demo showcases financial fraud prevention and using the MLRun feature store to define complex features that help identify fraud. Fraud prevention specifically is a challenge as it requires processing raw transaction and events in real-time and being able to quickly respond and block transactions before they occur.\n",
    "\n",
    "To address this, we create a development pipeline and a production pipeline. Both pipelines share the same feature engineering and model code, but serve data very differently. Furthermore, we automate the data and model monitoring process, identify drift and trigger retraining in a CI/CD pipeline. This process is described in the diagram below:\n",
    "\n",
    "![Feature store demo diagram - fraud prevention](./images/feature_store_demo_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is described as follows:\n",
    "\n",
    "| TRANSACTIONS                                                                    || &#x2551; |USER EVENTS                                                                           || \n",
    "|-----------------|----------------------------------------------------------------|----------|-----------------|----------------------------------------------------------------|\n",
    "| **age**         | age group value 0-6. Some values are marked as U for unknown   | &#x2551; | **source**      | The party/entity related to the event                          |\n",
    "| **gender**      | A character to define the gender                               | &#x2551; | **event**       | event, such as login or password change                        |\n",
    "| **zipcodeOri**  | ZIP code of the person originating the transaction             | &#x2551; | **timestamp**   | The date and time of the event                                 |\n",
    "| **zipMerchant** | ZIP code of the merchant receiving the transaction             | &#x2551; |                 |                                                                |\n",
    "| **category**    | category of the transaction (e.g., transportation, food, etc.) | &#x2551; |                 |                                                                |\n",
    "| **amount**      | the total amount of the transaction                            | &#x2551; |                 |                                                                |\n",
    "| **fraud**       | whether the transaction is fraudulent                          | &#x2551; |                 |                                                                |\n",
    "| **timestamp**   | the date and time in which the transaction took place          | &#x2551; |                 |                                                                |\n",
    "| **source**      | the ID of the party/entity performing the transaction          | &#x2551; |                 |                                                                |\n",
    "| **target**      | the ID of the party/entity receiving the transaction           | &#x2551; |                 |                                                                |\n",
    "| **device**      | the device ID used to perform the transaction                  | &#x2551; |                 |                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to **Ingest** different data sources to the **Feature Store**.\n",
    "\n",
    "The following FeatureSets will be created:\n",
    "- **Transactions**: Monetary transactions between a source and a target.\n",
    "- **Events**: Account events such as account login or a password change.\n",
    "- **Label**: Fraud label for the data.\n",
    "\n",
    "By the end of this tutorial you’ll learn how to:\n",
    "\n",
    "- Create an ingestion pipeline for each data source.\n",
    "- Define preprocessing, aggregation and validation of the pipeline.\n",
    "- Run the pipeline locally within the notebook.\n",
    "- Launch a real-time function to ingest live data.\n",
    "- Schedule a cron to run the task when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'fraud-demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-25 09:12:48,515 [info] Created and saved project fraud-demo-dani: {'from_template': None, 'overwrite': False, 'context': './', 'save': True}\n",
      "> 2023-01-25 09:12:48,517 [info] created project fraud-demo and saved in MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Fetch, Process and Ingest our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions to adjust the timestamps of our data\n",
    "# while keeping the order of the selected events and\n",
    "# the relative distance from one event to the other\n",
    "\n",
    "def date_adjustment(sample, data_max, new_max, old_data_period, new_data_period):\n",
    "    '''\n",
    "        Adjust a specific sample's date according to the original and new time periods\n",
    "    '''\n",
    "    sample_dates_scale = ((data_max - sample) / old_data_period)\n",
    "    sample_delta = new_data_period * sample_dates_scale\n",
    "    new_sample_ts = new_max - sample_delta\n",
    "    return new_sample_ts\n",
    "\n",
    "def adjust_data_timespan(dataframe, timestamp_col='timestamp', new_period='2d', new_max_date_str='now'):\n",
    "    '''\n",
    "        Adjust the dataframe timestamps to the new time period\n",
    "    '''\n",
    "    # Calculate old time period\n",
    "    data_min = dataframe.timestamp.min()\n",
    "    data_max = dataframe.timestamp.max()\n",
    "    old_data_period = data_max-data_min\n",
    "    \n",
    "    # Set new time period\n",
    "    new_time_period = pd.Timedelta(new_period)\n",
    "    new_max = pd.Timestamp(new_max_date_str)\n",
    "    new_min = new_max-new_time_period\n",
    "    new_data_period = new_max-new_min\n",
    "    \n",
    "    # Apply the timestamp change\n",
    "    df = dataframe.copy()\n",
    "    df[timestamp_col] = df[timestamp_col].apply(lambda x: date_adjustment(x, data_max, new_max, old_data_period, new_data_period))\n",
    "    df[timestamp_col].astype(\"datetime64[s]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184435</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>72.84</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-24 23:25:15.001587510</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333175</th>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>23.58</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-23 22:12:09.276967113</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451032</th>\n",
       "      <td>141</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-25 05:57:24.766019439</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>741ccdad2743422c98939329976a9c06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        step age gender  zipcodeOri  zipMerchant           category  amount  \\\n",
       "184435    63   5      M       28007        28007  es_transportation   72.84   \n",
       "333175   108   5      M       28007        28007  es_transportation   23.58   \n",
       "451032   141   5      M       28007        28007  es_transportation    4.31   \n",
       "\n",
       "        fraud                     timestamp       source       target  \\\n",
       "184435      0 2023-01-24 23:25:15.001587510  C1000148617  M1823072687   \n",
       "333175      0 2023-01-23 22:12:09.276967113  C1000148617  M1823072687   \n",
       "451032      0 2023-01-25 05:57:24.766019439  C1000148617   M348934600   \n",
       "\n",
       "                                  device  \n",
       "184435  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "333175  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "451032  741ccdad2743422c98939329976a9c06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch the transactions dataset from the server\n",
    "transactions_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# use only first 50k\n",
    "transactions_data = transactions_data.sort_values(by='source', axis=0)[:50000]\n",
    "\n",
    "# Adjust the samples timestamp for the past 2 days\n",
    "transactions_data = adjust_data_timespan(transactions_data, new_period='2d')\n",
    "\n",
    "# logging the data\n",
    "project.log_dataset('transactions_data', transactions_data, format='csv')\n",
    "\n",
    "# Preview\n",
    "transactions_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Create a FeatureSet and Preprocessing Pipeline\n",
    "Create the FeatureSet (data pipeline) definition for the **credit transaction processing** which describes the offline/online data transformations and aggregations.<br>\n",
    "The feature store will automatically add an offline `parquet` target and an online `NoSQL` target by using `set_targets()`.\n",
    "\n",
    "The data pipeline consists of:\n",
    "\n",
    "* **Extracting** the data components (hour, day of week)\n",
    "* **Mapping** the age values\n",
    "* **One hot encoding** for the transaction category and the gender\n",
    "* **Aggregating** the amount (avg, sum, count, max over 2/12/24 hour time windows)\n",
    "* **Aggregating** the transactions per category (over 14 days time windows)\n",
    "* **Writing** the results to **offline** (Parquet) and **online** (NoSQL) targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLRun's Feature Store\n",
    "import mlrun.feature_store as fstore\n",
    "from mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transactions FeatureSet\n",
    "transaction_set = fstore.FeatureSet(\"transactions\", \n",
    "                                    entities=[fstore.Entity(\"source\")], \n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"transactions feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"907pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 907.45 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 903.45,-94 903.45,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-27.05 40.7,-27.15 42.83,-27.3 44.92,-27.49 46.98,-27.74 48.99,-28.03 50.95,-28.36 52.84,-28.75 54.66,-29.18 56.4,-29.65 58.06,-30.16 59.63,-30.71 61.11,-31.31 62.49,-31.94 63.76,-32.61 64.93,-33.31 65.99,-34.04 66.93,-34.8 67.77,-35.59 68.48,-36.41 69.09,-37.25 69.58,-38.11 69.95,-38.99 70.21,-39.89 70.36,-40.8 70.4,-41.72 70.33,-42.65 70.16,-43.59 69.89,-44.53 69.53,-45.47 69.07,-46.41 68.52,-47.35 67.89,-48.28 67.18,-49.2 66.4,-50.11 65.55,-51.01 64.63,-51.89 63.65,-52.75 62.62,-53.59 61.53,-54.41 60.4,-55.2 59.23,-55.96 58.02,-56.69 56.78,-57.39 55.5,-58.06 54.2,-58.69 52.88,-59.29 51.53,-59.84 50.17,-60.35 48.79,-60.82 47.4,-61.25 46,-61.64 44.59,-61.97 43.17,-62.26 41.75,-62.51 40.32,-62.7 38.89,-62.85 37.45,-62.95 36.02,-63 34.58,-63 33.15,-62.95 31.71,-62.85 30.28,-62.7 28.85,-62.51 27.43,-62.26 26.01,-61.97 24.6,-61.64 23.2,-61.25 21.81,-60.82 20.43,-60.35 19.07,-59.84 17.72,-59.29 16.4,-58.69 15.1,-58.06 13.82,-57.39 12.58,-56.69 11.37,-55.96 10.2,-55.2 9.07,-54.41 7.98,-53.59 6.95,-52.75 5.97,-51.89 5.05,-51.01 4.2,-50.11 3.42,-49.2 2.71,-48.28 2.08,-47.35 1.53,-46.41 1.07,-45.47 0.71,-44.53 0.44,-43.59 0.27,-42.65 0.2,-41.72 0.24,-40.8 0.39,-39.89 0.65,-38.99 1.02,-38.11 1.51,-37.25 2.11,-36.41 2.83,-35.59 3.66,-34.8 4.61,-34.04 5.67,-33.31 6.84,-32.61 8.11,-31.94 9.49,-31.31 10.97,-30.71 12.54,-30.16 14.2,-29.65 15.94,-29.18 17.76,-28.75 19.65,-28.36 21.61,-28.03 23.62,-27.74 25.68,-27.49 27.77,-27.3 29.9,-27.15 32.05,-27.05 34.22,-27 36.38,-27 38.55,-27.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- DateExtractor -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>DateExtractor</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"183.94\" cy=\"-45\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"183.94\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">DateExtractor</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;DateExtractor -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;DateExtractor</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.94,-45C78.02,-45 87.01,-45 96.29,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.46,-48.5 106.45,-45 96.45,-41.5 96.46,-48.5\"/>\n",
       "</g>\n",
       "<!-- MapValues -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>MapValues</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"359.03\" cy=\"-45\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"359.03\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">MapValues</text>\n",
       "</g>\n",
       "<!-- DateExtractor&#45;&gt;MapValues -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>DateExtractor&#45;&gt;MapValues</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.36,-45C269.86,-45 278.49,-45 286.92,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.08,-48.5 297.08,-45 287.08,-41.5 287.08,-48.5\"/>\n",
       "</g>\n",
       "<!-- OneHotEncoder -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>OneHotEncoder</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"541.92\" cy=\"-45\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"541.92\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- MapValues&#45;&gt;OneHotEncoder -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>MapValues&#45;&gt;OneHotEncoder</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.02,-45C429.27,-45 437.9,-45 446.57,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"446.75,-48.5 456.75,-45 446.75,-41.5 446.75,-48.5\"/>\n",
       "</g>\n",
       "<!-- Aggregates -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Aggregates</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"726.75\" cy=\"-45\" rx=\"63.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"726.75\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">Aggregates</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;Aggregates -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;Aggregates</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M627.11,-45C635.71,-45 644.39,-45 652.85,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"653.04,-48.5 663.04,-45 653.04,-41.5 653.04,-48.5\"/>\n",
       "</g>\n",
       "<!-- parquet/parquet -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M899.45,-86.73C899.45,-88.53 883.09,-90 862.95,-90 842.81,-90 826.45,-88.53 826.45,-86.73 826.45,-86.73 826.45,-57.27 826.45,-57.27 826.45,-55.47 842.81,-54 862.95,-54 883.09,-54 899.45,-55.47 899.45,-57.27 899.45,-57.27 899.45,-86.73 899.45,-86.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M899.45,-86.73C899.45,-84.92 883.09,-83.45 862.95,-83.45 842.81,-83.45 826.45,-84.92 826.45,-86.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.95\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">parquet</text>\n",
       "</g>\n",
       "<!-- Aggregates&#45;&gt;parquet/parquet -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Aggregates&#45;&gt;parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.25,-55.36C791.37,-57.8 804.21,-60.38 816.06,-62.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"815.69,-66.26 826.18,-64.8 817.07,-59.4 815.69,-66.26\"/>\n",
       "</g>\n",
       "<!-- nosql/nosql -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.45,-32.73C890.45,-34.53 878.12,-36 862.95,-36 847.77,-36 835.45,-34.53 835.45,-32.73 835.45,-32.73 835.45,-3.27 835.45,-3.27 835.45,-1.47 847.77,0 862.95,0 878.12,0 890.45,-1.47 890.45,-3.27 890.45,-3.27 890.45,-32.73 890.45,-32.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.45,-32.73C890.45,-30.92 878.12,-29.45 862.95,-29.45 847.77,-29.45 835.45,-30.92 835.45,-32.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"862.95\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">nosql</text>\n",
       "</g>\n",
       "<!-- Aggregates&#45;&gt;nosql/nosql -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Aggregates&#45;&gt;nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M779.25,-34.64C794.6,-31.55 811.1,-28.23 825.29,-25.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"826.05,-28.79 835.16,-23.39 824.67,-21.93 826.05,-28.79\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f4d69a77d60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and add value mapping\n",
    "main_categories = [\"es_transportation\", \"es_health\", \"es_otherservices\",\n",
    "       \"es_food\", \"es_hotelservices\", \"es_barsandrestaurants\",\n",
    "       \"es_tech\", \"es_sportsandtoys\", \"es_wellnessandbeauty\",\n",
    "       \"es_hyper\", \"es_fashion\", \"es_home\", \"es_contents\",\n",
    "       \"es_travel\", \"es_leisure\"]\n",
    "\n",
    "# One Hot Encode the newly defined mappings\n",
    "one_hot_encoder_mapping = {'category': main_categories,\n",
    "                           'gender': list(transactions_data.gender.unique())}\n",
    "\n",
    "# Define the graph steps\n",
    "transaction_set.graph\\\n",
    "    .to(DateExtractor(parts = ['hour','month'], timestamp_col = 'timestamp'))\\\n",
    "    .to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\\\n",
    "    .to(OneHotEncoder(mapping=one_hot_encoder_mapping))\n",
    "\n",
    "\n",
    "# Add aggregations for 2, 12, and 24 hour time windows\n",
    "transaction_set.add_aggregation(name='amount',\n",
    "                                column='amount',\n",
    "                                operations=['avg','sum', 'count','max'],\n",
    "                                windows=['12h'],\n",
    "                                period='1h')\n",
    "\n",
    "\n",
    "# Add the category aggregations over a 14 day window\n",
    "for category in main_categories:\n",
    "    transaction_set.add_aggregation(name=category,column=f'category_{category}',\n",
    "                                    operations=['count'], windows=['14d'], period='1d')\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "transaction_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "transaction_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running with a Spark operator, the MLRun execution details are returned, allowing tracking of the job’s status and results.\n",
    "Spark operator ingestion is always executed remotely.\n",
    "\n",
    "The cell below should be executed only once to build the spark job image before running the first ingest. It may take a few minutes to prepare the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-25 09:13:04,744 [info] running build to add mlrun package, set with_mlrun=False to skip if its already in the image\n",
      "> 2023-01-25 09:13:05,083 [info] Started building image: .remote-spark-default-image\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 from registry datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80 \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Building stage 'datanode-registry.iguazio-platform.app.vmdev94.lab.iguazeng.com:80/iguazio/shell:3.5.3-b318.20230119151021' [idx: '0', base-idx: '-1'] \n",
      "\u001b[36mINFO\u001b[0m[0000] Unpacking rootfs as cmd RUN python -m pip install \"mlrun[complete]==1.3.0-rc8\" requires it. \n",
      "\u001b[36mINFO\u001b[0m[0067] RUN python -m pip install \"mlrun[complete]==1.3.0-rc8\" \n",
      "\u001b[36mINFO\u001b[0m[0067] Initializing snapshotter ...                 \n",
      "\u001b[36mINFO\u001b[0m[0067] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0120] Cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0120] Args: [-c python -m pip install \"mlrun[complete]==1.3.0-rc8\"] \n",
      "\u001b[36mINFO\u001b[0m[0120] Util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0120] Performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0120] Running: [/bin/sh -c python -m pip install \"mlrun[complete]==1.3.0-rc8\"] \n",
      "Collecting mlrun[complete]==1.3.0-rc8\n",
      "  Downloading mlrun-1.3.0rc8-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting pyarrow<7,>=3\n",
      "  Downloading pyarrow-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 16.5 MB/s eta 0:00:00\n",
      "Collecting inflection~=0.5.0\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting fastapi~=0.88.0\n",
      "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 22.3 MB/s eta 0:00:00\n",
      "Collecting kubernetes~=12.0\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 30.2 MB/s eta 0:00:00\n",
      "Collecting nuclio-jupyter~=0.9.6\n",
      "  Downloading nuclio_jupyter-0.9.6-py3-none-any.whl (51 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.1/51.1 kB 18.7 MB/s eta 0:00:00\n",
      "Collecting numpy<1.23.0,>=1.16.5\n",
      "  Downloading numpy-1.22.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 17.1 MB/s eta 0:00:00\n",
      "Collecting python-dotenv~=0.17.0\n",
      "  Downloading python_dotenv-0.17.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting humanfriendly~=8.2\n",
      "  Downloading humanfriendly-8.2-py2.py3-none-any.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.3/86.3 kB 31.1 MB/s eta 0:00:00\n",
      "Collecting orjson~=3.3\n",
      "  Downloading orjson-3.8.5-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 kB 21.7 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.13\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 23.1 MB/s eta 0:00:00\n",
      "Collecting kfp<1.8.14,~=1.8.0\n",
      "  Downloading kfp-1.8.13.tar.gz (300 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.1/300.1 kB 24.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting aiohttp-retry~=2.8\n",
      "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.10.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 630.1/630.1 kB 24.2 MB/s eta 0:00:00\n",
      "Collecting semver~=2.13\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting ipython~=7.0\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 kB 24.1 MB/s eta 0:00:00\n",
      "Collecting storey~=1.3.6\n",
      "  Downloading storey-1.3.7-py3-none-any.whl (156 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.1/156.1 kB 25.8 MB/s eta 0:00:00\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: cryptography<3.4,~=3.0 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (3.3.2)\n",
      "Collecting v3iofs~=0.1.15\n",
      "  Downloading v3iofs-0.1.15-py3-none-any.whl (13 kB)\n",
      "Collecting click~=8.0.0\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.5/97.5 kB 31.2 MB/s eta 0:00:00\n",
      "Collecting v3io~=0.5.20\n",
      "  Downloading v3io-0.5.20-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.2/64.2 kB 20.1 MB/s eta 0:00:00\n",
      "Collecting aiohttp~=3.8\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 23.6 MB/s eta 0:00:00\n",
      "Collecting chardet<4.0,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 kB 29.7 MB/s eta 0:00:00\n",
      "Collecting GitPython~=3.0\n",
      "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 kB 25.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests~=2.22 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (2.28.1)\n",
      "Collecting fsspec~=2021.8.1\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.3/119.3 kB 25.0 MB/s eta 0:00:00\n",
      "Collecting pymysql~=1.0\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 93.3 kB/s eta 0:00:00\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.4.46-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 20.2 MB/s eta 0:00:00\n",
      "Collecting pandas<1.5.0,~=1.2\n",
      "  Downloading pandas-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 19.7 MB/s eta 0:00:00\n",
      "Collecting alembic~=1.9\n",
      "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.6/210.6 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting v3io-frames~=0.10.2\n",
      "  Downloading v3io_frames-0.10.2-py3-none-any.whl (35 kB)\n",
      "Collecting deepdiff~=5.0\n",
      "  Downloading deepdiff-5.8.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.5/69.5 kB 24.5 MB/s eta 0:00:00\n",
      "Collecting distributed~=2021.11.2\n",
      "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 802.2/802.2 kB 24.5 MB/s eta 0:00:00\n",
      "Collecting dask~=2021.11.2\n",
      "  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 23.9 MB/s eta 0:00:00\n",
      "Collecting tabulate~=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /conda/lib/python3.9/site-packages (from mlrun[complete]==1.3.0-rc8) (1.26.13)\n",
      "Collecting redis~=4.3\n",
      "  Downloading redis-4.4.2-py3-none-any.whl (237 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 237.8/237.8 kB 27.8 MB/s eta 0:00:00\n",
      "Collecting azure-storage-blob~=12.13\n",
      "  Downloading azure_storage_blob-12.14.1-py3-none-any.whl (383 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 383.2/383.2 kB 24.8 MB/s eta 0:00:00\n",
      "Collecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 19.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting s3fs~=2021.8.1\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "Collecting boto3<1.17.107,~=1.9\n",
      "  Downloading boto3-1.17.106-py2.py3-none-any.whl (131 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.6/131.6 kB 39.8 MB/s eta 0:00:00\n",
      "Collecting msrest~=0.6.21\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.2/85.2 kB 31.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.2-py2.py3-none-any.whl (215 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.1/215.1 kB 25.7 MB/s eta 0:00:00\n",
      "Collecting azure-identity~=1.5\n",
      "  Downloading azure_identity-1.12.0-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 kB 28.1 MB/s eta 0:00:00\n",
      "Collecting gcsfs~=2021.8.1\n",
      "  Downloading gcsfs-2021.8.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting azure-keyvault-secrets~=4.2\n",
      "  Downloading azure_keyvault_secrets-4.6.0-py3-none-any.whl (291 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 291.6/291.6 kB 25.9 MB/s eta 0:00:00\n",
      "Collecting adlfs~=2021.8.1\n",
      "  Downloading adlfs-2021.8.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting botocore<1.20.107,>=1.20.106\n",
      "  Downloading botocore-1.20.106-py2.py3-none-any.whl (7.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 20.8 MB/s eta 0:00:00\n",
      "Collecting plotly~=5.4\n",
      "  Downloading plotly-5.13.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 19.4 MB/s eta 0:00:00\n",
      "Collecting azure-core~=1.24\n",
      "  Downloading azure_core-1.26.2-py3-none-any.whl (173 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.8/173.8 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting kafka-python~=2.0\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 246.5/246.5 kB 27.0 MB/s eta 0:00:00\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46\n",
      "  Downloading azure_datalake_store-0.0.52-py2.py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.7/61.7 kB 24.0 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.8/77.8 kB 28.3 MB/s eta 0:00:00\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 26.7 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 kB 38.4 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 24.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 35.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /conda/lib/python3.9/site-packages (from aiohttp~=3.8->mlrun[complete]==1.3.0-rc8) (2.0.4)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 28.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.11.0 in /conda/lib/python3.9/site-packages (from azure-core~=1.24->mlrun[complete]==1.3.0-rc8) (1.16.0)\n",
      "Collecting typing-extensions>=4.0.1\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting msal<2.0.0,>=1.12.0\n",
      "  Downloading msal-1.20.0-py2.py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.0/90.0 kB 34.1 MB/s eta 0:00:00\n",
      "Collecting azure-common~=1.1\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting azure-storage-blob~=12.13\n",
      "  Downloading azure_storage_blob-12.14.0-py3-none-any.whl (383 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 383.2/383.2 kB 26.7 MB/s eta 0:00:00\n",
      "  Downloading azure_storage_blob-12.13.1-py3-none-any.whl (377 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 377.4/377.4 kB 27.5 MB/s eta 0:00:00\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.2/79.2 kB 27.5 MB/s eta 0:00:00\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 29.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.12 in /conda/lib/python3.9/site-packages (from cryptography<3.4,~=3.0->mlrun[complete]==1.3.0-rc8) (1.14.6)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting cloudpickle>=1.1.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /conda/lib/python3.9/site-packages (from dask~=2021.11.2->mlrun[complete]==1.3.0-rc8) (0.12.0)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 15.7 MB/s eta 0:00:00\n",
      "Collecting ordered-set<4.2.0,>=4.1.0\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting tornado>=6.0.3\n",
      "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 424.0/424.0 kB 25.7 MB/s eta 0:00:00\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 kB 29.6 MB/s eta 0:00:00\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 40.3 MB/s eta 0:00:00\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 322.4/322.4 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.2.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools in /conda/lib/python3.9/site-packages (from distributed~=2021.11.2->mlrun[complete]==1.3.0-rc8) (65.6.3)\n",
      "Collecting starlette==0.22.0\n",
      "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 24.0 MB/s eta 0:00:00\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 28.6 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 kB 30.9 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 25.3 MB/s eta 0:00:00\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.3/120.3 kB 33.4 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 19.6 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 28.3 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 21.8 MB/s eta 0:00:00\n",
      "Collecting db-dtypes<2.0.0dev,>=0.3.0\n",
      "  Downloading db_dtypes-1.0.5-py2.py3-none-any.whl (14 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.8/116.8 kB 29.1 MB/s eta 0:00:00\n",
      "Collecting matplotlib-inline\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting jedi>=0.16\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 21.3 MB/s eta 0:00:00\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.4/386.4 kB 27.7 MB/s eta 0:00:00\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting pexpect>4.3\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.0/59.0 kB 23.3 MB/s eta 0:00:00\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 25.2 MB/s eta 0:00:00\n",
      "Collecting absl-py<2,>=0.9\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 30.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 33.9 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting google-auth>=1.2\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 26.2 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 22.4 MB/s eta 0:00:00\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 24.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 21.7 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 32.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 21.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=14.05.14 in /conda/lib/python3.9/site-packages (from kubernetes~=12.0->mlrun[complete]==1.3.0-rc8) (2022.12.7)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 14.6 MB/s eta 0:00:00\n",
      "Collecting notebook>=5.2.0\n",
      "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 439.1/439.1 kB 26.4 MB/s eta 0:00:00\n",
      "Collecting nbconvert>=6.4.5\n",
      "  Downloading nbconvert-7.2.9-py3-none-any.whl (274 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.9/274.9 kB 27.5 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 27.2 MB/s eta 0:00:00\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /conda/lib/python3.9/site-packages (from requests~=2.22->mlrun[complete]==1.3.0-rc8) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 535.9/535.9 kB 364.1 kB/s eta 0:00:00\n",
      "Collecting storey~=1.3.6\n",
      "  Downloading storey-1.3.6-py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.9/155.9 kB 462.6 kB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of sqlalchemy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.4.45-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 3.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of semver to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests~=2.22\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 138.2 kB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of redis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting redis~=4.3\n",
      "  Downloading redis-4.4.1-py3-none-any.whl (237 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 237.6/237.6 kB 425.1 kB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.4-cp39-cp39-manylinux1_x86_64.whl (629 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 630.0/630.0 kB 1.5 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of python-dotenv to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dotenv~=0.17.0\n",
      "  Downloading python_dotenv-0.17.0-py2.py3-none-any.whl (18 kB)\n",
      "INFO: pip is looking at multiple versions of pymysql to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymysql~=1.0\n",
      "  Downloading PyMySQL-1.0.1-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 6.5 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 19.5 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<7,>=3\n",
      "  Downloading pyarrow-6.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 13.4 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<3.20,>=3.13\n",
      "  Downloading protobuf-3.19.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 20.6 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of plotly to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting plotly~=5.4\n",
      "  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.2/15.2 MB 15.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas<1.5.0,~=1.2\n",
      "  Downloading pandas-1.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 17.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson~=3.3\n",
      "  Downloading orjson-3.8.4-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.7/140.7 kB 16.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy<1.23.0,>=1.16.5\n",
      "  Downloading numpy-1.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 17.2 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of nuclio-jupyter to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of nest-asyncio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of msrest to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of mergedeep to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.3-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of kubernetes to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kubernetes~=12.0\n",
      "  Downloading kubernetes-12.0.0-py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 18.9 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of kfp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kfp<1.8.14,~=1.8.0\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 22.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: pip is looking at multiple versions of kafka-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kafka-python~=2.0\n",
      "  Downloading kafka_python-2.0.1-py2.py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.2/232.2 kB 21.0 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of ipython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ipython~=7.0\n",
      "  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 kB 21.3 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of inflection to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting inflection~=0.5.0\n",
      "  Downloading inflection-0.5.0-py2.py3-none-any.whl (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of humanfriendly to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery[pandas] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.1/215.1 kB 22.4 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 15.8 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.4.0-py2.py3-none-any.whl (212 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 24.5 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.18.1-py2.py3-none-any.whl (189 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.8/189.8 kB 27.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery[pandas]~=3.2\n",
      "  Downloading google_cloud_bigquery-3.3.6-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 25.2 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.5-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 25.1 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.3-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 25.0 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.2-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 17.6 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.3.1-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 23.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery[pandas] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_bigquery-3.3.0-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 24.1 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery-3.2.0-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.8/211.8 kB 22.6 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.41.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 18.3 MB/s eta 0:00:00\n",
      "Collecting xxhash>=1\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting grpcio-tools<1.42,>1.34.0\n",
      "  Downloading grpcio_tools-1.41.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 19.9 MB/s eta 0:00:00\n",
      "Collecting future>=0.18.2\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 22.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ujson>=3\n",
      "  Downloading ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 19.9 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos>=1.5.3\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 223.0/223.0 kB 22.8 MB/s eta 0:00:00\n",
      "Collecting adal>=0.4.2\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 12.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pycparser in /conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography<3.4,~=3.0->mlrun[complete]==1.3.0-rc8) (2.21)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.6/115.6 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.8/96.8 kB 29.1 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 18.4 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.18.0-py2.py3-none-any.whl (187 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.7/187.7 kB 16.8 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery_storage-2.17.0-py2.py3-none-any.whl (187 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187.7/187.7 kB 695.1 kB/s eta 0:00:00\n",
      "  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.4/185.4 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting parso<0.9.0,>=0.8.0\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.8/100.8 kB 11.7 MB/s eta 0:00:00\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting portalocker<3,>=1.0\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting importlib-metadata>=3.6\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting jupyter-core>=4.7\n",
      "  Downloading jupyter_core-5.1.5-py3-none-any.whl (93 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.5/93.5 kB 11.6 MB/s eta 0:00:00\n",
      "Collecting mistune<3,>=2.0.3\n",
      "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 11.7 MB/s eta 0:00:00\n",
      "Collecting markupsafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting nbformat>=5.1\n",
      "  Downloading nbformat-5.7.3-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.1/78.1 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting bleach\n",
      "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 11.3 MB/s eta 0:00:00\n",
      "Collecting nbclassic>=0.4.7\n",
      "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.8/9.8 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.5/122.5 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-25.0.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 2.3 MB/s eta 0:00:00\n",
      "Collecting ipykernel\n",
      "  Downloading ipykernel-6.20.2-py3-none-any.whl (149 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.2/149.2 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.5/133.5 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 751.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in /conda/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp<1.8.14,~=1.8.0->mlrun[complete]==1.3.0-rc8) (0.38.4)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.7.1-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.7.0-py2.py3-none-any.whl (19 kB)\n",
      "  Downloading google_auth_oauthlib-0.5.3-py2.py3-none-any.whl (19 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.47.2-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.47.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.5-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.3-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.1-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.46.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.43.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.42.0-py3-none-any.whl (10.0 kB)\n",
      "  Downloading grpcio_status-1.41.1-py3-none-any.whl (9.2 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting platformdirs>=2.5\n",
      "  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
      "Collecting jupyter-server>=1.8\n",
      "  Downloading jupyter_server-2.1.0-py3-none-any.whl (365 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 365.2/365.2 kB 618.4 kB/s eta 0:00:00\n",
      "Collecting notebook-shim>=0.1.0\n",
      "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 730.9 kB/s eta 0:00:00\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
      "Collecting types-cryptography>=3.3.21\n",
      "  Downloading types_cryptography-3.3.23.2-py3-none-any.whl (30 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 685.2 kB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting comm>=0.1.1\n",
      "  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
      "Collecting debugpy>=1.0\n",
      "  Downloading debugpy-1.6.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 974.1 kB/s eta 0:00:00\n",
      "Collecting jupyter-events>=0.4.0\n",
      "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter-server-terminals\n",
      "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting jsonschema[format-nongpl]>=3.2.0\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Downloading python_json_logger-2.0.4-py3-none-any.whl (7.8 kB)\n",
      "Collecting jsonschema[format-nongpl]>=3.2.0\n",
      "  Downloading jsonschema-4.17.1-py3-none-any.whl (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.2/90.2 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.17.0-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.8/83.8 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.16.0-py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.15.0-py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.7/82.7 kB 1.3 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.14.0-py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.4/82.4 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.13.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.6/81.6 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.12.1-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.12.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.3/81.3 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.11.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.3-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.2-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.1-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.10.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.9.1-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.9.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.8.0-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 1.7 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.2-py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.1-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.7.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 1.5 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.2-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.1-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.8/80.8 kB 703.3 kB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.6.0-py3-none-any.whl (80 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.4/80.4 kB 959.7 kB/s eta 0:00:00\n",
      "  Downloading jsonschema-4.5.1-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 1.0 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.5.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.5.0-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 996.7 kB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.5.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 1.0 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.4.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.3-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.9/71.9 kB 1.1 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.2-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.8/71.8 kB 1.1 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.2 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.1-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.7/71.7 kB 1.1 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.3.0-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 kB 1.1 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.3.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.4/69.4 kB 1.4 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.2.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.2.0-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.4/69.4 kB 1.4 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.2.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.2-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 1.4 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.2 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 1.5 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.1 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 1.4 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.1.0 does not provide the extra 'format-nongpl'\n",
      "  Downloading jsonschema-4.0.1-py3-none-any.whl (69 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.1/69.1 kB 1.3 MB/s eta 0:00:00\n",
      "WARNING: jsonschema 4.0.1 does not provide the extra 'format-nongpl'\n",
      "WARNING: jsonschema 3.2.0 does not provide the extra 'format-nongpl'\n",
      "Building wheels for collected packages: adlfs, aiobotocore, kfp, fire, future, kfp-server-api, strip-hints\n",
      "  Building wheel for adlfs (setup.py): started\n",
      "  Building wheel for adlfs (setup.py): finished with status 'done'\n",
      "  Created wheel for adlfs: filename=adlfs-2021.8.2-py3-none-any.whl size=21465 sha256=ce75d65e89af03635be95e2d33eb1d0d290d21e192587acfd3dbbf9eb414c4a8\n",
      "  Stored in directory: /.cache/pip/wheels/00/03/70/7dbccaa3fe1feac9884a8376949e729c73b2497eff79290674\n",
      "  Building wheel for aiobotocore (setup.py): started\n",
      "  Building wheel for aiobotocore (setup.py): finished with status 'done'\n",
      "  Created wheel for aiobotocore: filename=aiobotocore-1.4.2-py3-none-any.whl size=49909 sha256=163ea3aa6f5de2983c255819aeadf4b24817ffe43bb796500a6221acf2cc0feb\n",
      "  Stored in directory: /.cache/pip/wheels/74/dc/76/2e9011d2d0191acf6dcb82a60d8f32c97340bc40c14c383e4a\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.13-py3-none-any.whl size=422423 sha256=be0a98c4f1eaf923db1d7996183700453b11063076e015e12d5d1f59cf434df7\n",
      "  Stored in directory: /.cache/pip/wheels/ba/0e/01/06569da8d3c237c7759be9f4070cef228d38cfe69fe77036f1\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=346bde4a950917618372b5f87981086d069ae37fce063578ec2aafa9f4aed438\n",
      "  Stored in directory: /.cache/pip/wheels/34/a9/61/d515d3cd1e8a349fed305bc67a9c7d68fc38d51053b6decad6\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=90fed855e361afee1248b4a695be5f6f519327e7d748498875f4fa2dc5a2cff4\n",
      "  Stored in directory: /.cache/pip/wheels/0c/ff/54/efb16da5b1058114a457b3c7167904915d2e5764b637ae8d3d\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=d3d3021df6ea4c76677ff31540f70a9bf997718478cc70b57ba97f9089a1cd02\n",
      "  Stored in directory: /.cache/pip/wheels/1d/5e/cc/d6c7bfba9ec05cf878694715074232539159539737f01bc680\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22283 sha256=d8b5274e9cbeff5872377ff38665ec68bfcdd9520cd8579a5da0048ceb6f7887\n",
      "  Stored in directory: /.cache/pip/wheels/63/7d/5a/867fa96b7d29e9dc3f26ddffe45d960690603b434081da419d\n",
      "Successfully built adlfs aiobotocore kfp fire future kfp-server-api strip-hints\n",
      "Installing collected packages: webencodings, wcwidth, types-cryptography, sortedcontainers, Send2Trash, pytz, python-dotenv, pyasn1, ptyprocess, pickleshare, msgpack, mistune, kafka-python, ipython-genutils, heapdict, fastjsonschema, chardet, backcall, azure-common, zipp, zict, xxhash, wrapt, websocket-client, uritemplate, ujson, typing-extensions, traitlets, tornado, tinycss2, termcolor, tenacity, tblib, tabulate, strip-hints, soupsieve, sniffio, smmap, semver, rsa, rfc3986-validator, rfc3339-validator, pyzmq, pyyaml, python-json-logger, python-dateutil, pyrsistent, pyparsing, pymysql, PyJWT, pygments, pyasn1-modules, psutil, protobuf, prompt-toolkit, prometheus-client, portalocker, platformdirs, pexpect, parso, pandocfilters, orjson, ordered-set, oauthlib, numpy, nest-asyncio, multidict, mergedeep, markupsafe, locket, jupyterlab-pygments, jmespath, isodate, inflection, humanfriendly, grpcio, greenlet, google-crc32c, future, fsspec, frozenlist, entrypoints, docstring-parser, defusedxml, decorator, debugpy, cloudpickle, click, cachetools, bleach, attrs, async-timeout, absl-py, yarl, v3io, typer, terminado, sqlalchemy, requests-toolbelt, requests-oauthlib, redis, pydantic, pyarrow, proto-plus, plotly, partd, pandas, packaging, matplotlib-inline, Mako, kfp-server-api, kfp-pipeline-spec, jupyter-core, jsonschema, jinja2, jedi, importlib-metadata, httplib2, grpcio-tools, googleapis-common-protos, google-resumable-media, google-auth, gitdb, fire, Deprecated, deepdiff, comm, botocore, beautifulsoup4, azure-core, argon2-cffi-bindings, anyio, aiosignal, aioitertools, v3iofs, v3io-frames, starlette, s3transfer, nbformat, msrest, kubernetes, jupyter-server-terminals, jupyter-client, ipython, grpcio-status, google-auth-oauthlib, google-auth-httplib2, google-api-core, GitPython, db-dtypes, dask, argon2-cffi, alembic, aiohttp, adal, storey, nbclient, msal, jupyter-events, ipykernel, google-cloud-core, google-api-python-client, gcsfs, fastapi, distributed, boto3, azure-storage-blob, azure-keyvault-secrets, azure-datalake-store, aiohttp-retry, aiobotocore, s3fs, nbconvert, msal-extensions, google-cloud-storage, google-cloud-bigquery-storage, kfp, jupyter-server, google-cloud-bigquery, azure-identity, notebook-shim, adlfs, nbclassic, notebook, nuclio-jupyter, mlrun\n",
      "Successfully installed Deprecated-1.2.13 GitPython-3.1.30 Mako-1.2.4 PyJWT-2.5.0 Send2Trash-1.8.0 absl-py-1.4.0 adal-1.2.7 adlfs-2021.8.2 aiobotocore-1.4.2 aiohttp-3.8.3 aiohttp-retry-2.8.3 aioitertools-0.11.0 aiosignal-1.3.1 alembic-1.9.2 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.2 attrs-22.2.0 azure-common-1.1.28 azure-core-1.26.2 azure-datalake-store-0.0.52 azure-identity-1.12.0 azure-keyvault-secrets-4.6.0 azure-storage-blob-12.13.1 backcall-0.2.0 beautifulsoup4-4.11.1 bleach-6.0.0 boto3-1.17.106 botocore-1.20.106 cachetools-4.2.4 chardet-3.0.4 click-8.0.4 cloudpickle-2.2.1 comm-0.1.2 dask-2021.11.2 db-dtypes-1.0.5 debugpy-1.6.6 decorator-5.1.1 deepdiff-5.8.1 defusedxml-0.7.1 distributed-2021.11.2 docstring-parser-0.15 entrypoints-0.4 fastapi-0.88.0 fastjsonschema-2.16.2 fire-0.5.0 frozenlist-1.3.3 fsspec-2021.8.1 future-0.18.3 gcsfs-2021.8.1 gitdb-4.0.10 google-api-core-2.10.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.5.3 google-cloud-bigquery-3.2.0 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-cloud-storage-1.44.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 greenlet-2.0.1 grpcio-1.41.1 grpcio-status-1.41.1 grpcio-tools-1.41.1 heapdict-1.0.1 httplib2-0.21.0 humanfriendly-8.2 importlib-metadata-6.0.0 inflection-0.5.1 ipykernel-6.20.2 ipython-7.34.0 ipython-genutils-0.2.0 isodate-0.6.1 jedi-0.18.2 jinja2-3.1.2 jmespath-0.10.0 jsonschema-3.2.0 jupyter-client-7.4.9 jupyter-core-5.1.5 jupyter-events-0.6.3 jupyter-server-2.1.0 jupyter-server-terminals-0.4.4 jupyterlab-pygments-0.2.2 kafka-python-2.0.2 kfp-1.8.13 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-12.0.1 locket-1.0.0 markupsafe-2.1.2 matplotlib-inline-0.1.6 mergedeep-1.3.4 mistune-2.0.4 mlrun-1.3.0rc8 msal-1.20.0 msal-extensions-1.0.0 msgpack-1.0.4 msrest-0.6.21 multidict-6.0.4 nbclassic-0.4.8 nbclient-0.7.2 nbconvert-7.2.9 nbformat-5.7.3 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 nuclio-jupyter-0.9.6 numpy-1.22.4 oauthlib-3.2.2 ordered-set-4.1.0 orjson-3.8.5 packaging-21.3 pandas-1.4.4 pandocfilters-1.5.0 parso-0.8.3 partd-1.3.0 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-2.6.2 plotly-5.13.0 portalocker-2.7.0 prometheus-client-0.16.0 prompt-toolkit-3.0.36 proto-plus-1.22.2 protobuf-3.19.6 psutil-5.9.4 ptyprocess-0.7.0 pyarrow-6.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.10.4 pygments-2.14.0 pymysql-1.0.2 pyparsing-3.0.9 pyrsistent-0.19.3 python-dateutil-2.8.2 python-dotenv-0.17.1 python-json-logger-2.0.4 pytz-2022.7.1 pyyaml-5.4.1 pyzmq-25.0.0 redis-4.4.2 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rsa-4.9 s3fs-2021.8.1 s3transfer-0.4.2 semver-2.13.0 smmap-5.0.0 sniffio-1.3.0 sortedcontainers-2.4.0 soupsieve-2.3.2.post1 sqlalchemy-1.4.46 starlette-0.22.0 storey-1.3.7 strip-hints-0.1.10 tabulate-0.8.10 tblib-1.7.0 tenacity-8.1.0 termcolor-2.2.0 terminado-0.17.1 tinycss2-1.2.1 tornado-6.2 traitlets-5.8.1 typer-0.7.0 types-cryptography-3.3.23.2 typing-extensions-4.4.0 ujson-5.7.0 uritemplate-3.0.1 v3io-0.5.20 v3io-frames-0.10.2 v3iofs-0.1.15 wcwidth-0.2.6 webencodings-0.5.1 websocket-client-1.4.2 wrapt-1.14.1 xxhash-3.2.0 yarl-1.8.2 zict-2.2.0 zipp-3.11.0\n",
      "\u001b[36mINFO\u001b[0m[0253] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0292] Pushing image to docker-registry.default-tenant.app.vmdev94.lab.iguazeng.com:80/remote-spark-default-image \n",
      "\u001b[36mINFO\u001b[0m[0303] Pushed docker-registry.default-tenant.app.vmdev94.lab.iguazeng.com:80/remote-spark-default-image@sha256:ac921399385581ed5e392dd7daac5c8ffa9f7722a9301cb39e16cc64eac53ad5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlrun.runtimes import RemoteSparkRuntime\n",
    "RemoteSparkRuntime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlrun: start-code\n",
    "\n",
    "from mlrun.feature_store.api import ingest\n",
    "def ingest_handler(context):\n",
    "    ingest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n",
    "    \n",
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = mlrun.code_to_function(name='transactions_func', kind='spark', handler=\"ingest_handler\")\n",
    "\n",
    "function.with_driver_requests(cpu=\"1000m\", mem=\"4G\")\n",
    "function.with_driver_limits(cpu=\"1000\")\n",
    "function.with_executor_requests(cpu=\"500m\", mem=\"2G\")\n",
    "function.with_executor_limits(cpu=\"500m\")\n",
    "function.spec.replicas = 3\n",
    "function.with_igz_spark()\n",
    "function.spec.use_default_image = True\n",
    "\n",
    "run_config = fstore.RunConfig(function=function, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-01-25 09:18:34,065 [info] starting run transactions-ingest uid=777c75df7c6b4b7a9c6e4d95635de8ae DB=http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.200.136.74 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///etc/config/mlrun/spark-function-code.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2023-01-25 09:18:44,339 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2023-01-25 09:18:47,676 [info] starting ingestion task to store://feature-sets/fraud-demo-dani/transactions:latest.\n",
      "2023-01-25 09:18:48,321 INFO spark.SparkContext: Running Spark version 3.2.1\n",
      "2023-01-25 09:18:48,352 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-25 09:18:48,353 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-01-25 09:18:48,353 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-01-25 09:18:48,354 INFO spark.SparkContext: Submitted application: transactions-ingest-777c75df7c6b4b7a9c6e4d95635de8ae\n",
      "2023-01-25 09:18:48,378 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-01-25 09:18:48,398 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-01-25 09:18:48,400 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-01-25 09:18:48,471 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2023-01-25 09:18:48,471 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2023-01-25 09:18:48,472 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-01-25 09:18:48,472 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-01-25 09:18:48,472 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2023-01-25 09:18:48,745 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2023-01-25 09:18:48,774 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-01-25 09:18:48,820 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-01-25 09:18:48,846 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-01-25 09:18:48,846 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-01-25 09:18:48,850 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-01-25 09:18:48,877 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-2d7f9048-4112-4b81-a4b8-4303ad4eeaa0/blockmgr-fcc32394-d109-45ec-b7d7-193bc911a4f3\n",
      "2023-01-25 09:18:48,907 INFO memory.MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "2023-01-25 09:18:48,925 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-01-25 09:18:49,036 INFO util.log: Logging initialized @6798ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-01-25 09:18:49,121 INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.11+9-LTS\n",
      "2023-01-25 09:18:49,144 INFO server.Server: Started @6907ms\n",
      "2023-01-25 09:18:49,183 INFO server.AbstractConnector: Started ServerConnector@5d335cd8{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-01-25 09:18:49,183 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-01-25 09:18:49,210 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46b97b50{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,213 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ad9bf36{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,216 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7622b88a{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,217 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64dd47ee{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,218 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a41331d{/stages,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,219 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@217b1c78{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,220 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a5f0110{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,222 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32007cc2{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,222 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e6e012a{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,223 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44cf29e{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,224 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71bcff86{/storage,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,225 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c06b1bf{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,226 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3be43f29{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,227 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@629d042{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,228 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@240a344b{/environment,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,228 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b4fbf27{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,229 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a85cbaf{/executors,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,230 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@95a98a0{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,231 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1212f7b1{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,232 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@152796fa{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,242 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@222a9cb{/static,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,242 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41eac78{/,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,244 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423c0a8a{/api,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,245 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d2b7d3b{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,245 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@428bd41e{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:49,247 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:4040\n",
      "2023-01-25 09:18:49,263 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1674638328311\n",
      "2023-01-25 09:18:49,263 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1674638328311\n",
      "2023-01-25 09:18:49,263 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1674638328311\n",
      "2023-01-25 09:18:49,263 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1674638328311\n",
      "2023-01-25 09:18:49,264 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1674638328311\n",
      "2023-01-25 09:18:49,265 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2023-01-25 09:18:49,357 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2023-01-25 09:18:50,436 INFO k8s.ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 3, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
      "2023-01-25 09:18:50,525 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-01-25 09:18:50,554 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2023-01-25 09:18:50,554 INFO netty.NettyBlockTransferService: Server created on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079\n",
      "2023-01-25 09:18:50,556 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-01-25 09:18:50,565 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-01-25 09:18:50,569 INFO storage.BlockManagerMasterEndpoint: Registering block manager transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 with 2.2 GiB RAM, BlockManagerId(driver, transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-01-25 09:18:50,572 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-01-25 09:18:50,574 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-01-25 09:18:50,645 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-01-25 09:18:50,665 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a9d4466{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:18:50,685 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-01-25 09:19:08,191 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.140.66:42774) with ID 2,  ResourceProfileId 0\n",
      "2023-01-25 09:19:08,676 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.140.66:38378 with 1007.8 MiB RAM, BlockManagerId(2, 10.200.140.66, 38378, None)\n",
      "2023-01-25 09:19:09,165 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.201.8.107:45122) with ID 1,  ResourceProfileId 0\n",
      "2023-01-25 09:19:09,577 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.201.8.107:34250 with 1007.8 MiB RAM, BlockManagerId(1, 10.201.8.107, 34250, None)\n",
      "2023-01-25 09:19:10,270 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.201.8.108:33356) with ID 3,  ResourceProfileId 0\n",
      "2023-01-25 09:19:10,351 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-01-25 09:19:10,573 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.201.8.108:46028 with 1007.8 MiB RAM, BlockManagerId(3, 10.201.8.108, 46028, None)\n",
      "2023-01-25 09:19:10,981 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-01-25 09:19:11,010 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2023-01-25 09:19:11,027 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29ab0f52{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:19:11,028 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3deda3b5{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:19:11,029 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28370183{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:19:11,030 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b84a19c{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:19:11,056 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66ec24dc{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-01-25 09:19:12,793 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "2023-01-25 09:19:13,679 INFO datasources.InMemoryFileIndex: It took 215 ms to list leaf files for 1 paths.\n",
      "2023-01-25 09:19:14,055 INFO datasources.InMemoryFileIndex: It took 203 ms to list leaf files for 1 paths.\n",
      "2023-01-25 09:19:16,805 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:16,807 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-01-25 09:19:16,810 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-01-25 09:19:17,385 INFO codegen.CodeGenerator: Code generated in 233.842387 ms\n",
      "2023-01-25 09:19:17,458 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:17,546 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:17,550 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:17,556 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:17,582 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:17,730 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:17,750 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-01-25 09:19:17,751 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:17,751 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:17,752 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:17,758 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:17,832 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:17,855 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:17,856 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:17,856 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:17,878 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-01-25 09:19:17,879 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-01-25 09:19:17,946 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:18,669 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.200.140.66:38378 (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:22,479 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.140.66:38378 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:29,609 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11683 ms on 10.200.140.66 (executor 2) (1/1)\n",
      "2023-01-25 09:19:29,611 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:19:29,616 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 11.842 s\n",
      "2023-01-25 09:19:29,620 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-25 09:19:29,620 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-01-25 09:19:29,622 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 11.891828 s\n",
      "2023-01-25 09:19:29,648 INFO codegen.CodeGenerator: Code generated in 12.411079 ms\n",
      "2023-01-25 09:19:29,708 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:29,715 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:29,715 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-01-25 09:19:29,716 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-01-25 09:19:29,725 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:29,754 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:29,755 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:29,756 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:29,757 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:29,777 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.200.140.66:38378 in memory (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:29,834 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:29,836 INFO scheduler.DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:29,836 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:29,836 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:29,836 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:29,837 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:29,867 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:29,878 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:29,879 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 10.2 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:29,880 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:29,881 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:29,881 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:29,883 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:29,884 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:30,070 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.140.66:38378 (size: 10.2 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:30,688 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.201.8.107:34250 (size: 10.2 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:35,474 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.140.66:38378 (size: 54.9 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:19:38,192 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8309 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:19:40,260 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.201.8.107:34250 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:48,671 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 18788 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:19:48,672 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:19:48,673 INFO scheduler.DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 18.833 s\n",
      "2023-01-25 09:19:48,673 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-25 09:19:48,673 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-01-25 09:19:48,673 INFO scheduler.DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 18.838860 s\n",
      "2023-01-25 09:19:48,720 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 10.2 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:48,723 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.140.66:38378 in memory (size: 10.2 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:19:48,757 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.201.8.107:34250 in memory (size: 10.2 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:48,767 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:48,767 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-01-25 09:19:48,768 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:48,835 INFO codegen.CodeGenerator: Code generated in 35.002566 ms\n",
      "2023-01-25 09:19:48,840 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:48,865 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:48,867 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:48,867 INFO spark.SparkContext: Created broadcast 4 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:48,872 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:48,915 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:48,916 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.200.140.66:38378 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:48,971 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:48,971 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-01-25 09:19:48,971 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:49,026 INFO codegen.CodeGenerator: Code generated in 35.812069 ms\n",
      "2023-01-25 09:19:49,031 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:49,049 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.200.140.66:38378 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:49,049 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.201.8.107:34250 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:49,051 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:49,058 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:49,059 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:49,060 INFO spark.SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:49,061 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:49,220 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:49,221 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-01-25 09:19:49,221 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:49,269 INFO codegen.CodeGenerator: Code generated in 33.954787 ms\n",
      "2023-01-25 09:19:49,274 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:49,298 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:49,299 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:49,300 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:49,301 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:50,720 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:55,493 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:56,020 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,021 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-01-25 09:19:56,021 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,025 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,025 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2743 as timestamp))\n",
      "2023-01-25 09:19:56,026 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,030 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,030 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2833 as timestamp))\n",
      "2023-01-25 09:19:56,030 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,034 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,034 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2925 as timestamp))\n",
      "2023-01-25 09:19:56,034 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,038 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,038 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3019 as timestamp))\n",
      "2023-01-25 09:19:56,038 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,042 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,042 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3115 as timestamp))\n",
      "2023-01-25 09:19:56,042 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,046 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,046 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3213 as timestamp))\n",
      "2023-01-25 09:19:56,046 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,050 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,050 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3313 as timestamp))\n",
      "2023-01-25 09:19:56,050 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,053 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,053 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3415 as timestamp))\n",
      "2023-01-25 09:19:56,054 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,057 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,057 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3519 as timestamp))\n",
      "2023-01-25 09:19:56,057 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,060 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,060 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3625 as timestamp))\n",
      "2023-01-25 09:19:56,060 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,063 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,063 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3733 as timestamp))\n",
      "2023-01-25 09:19:56,064 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,066 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,067 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3843 as timestamp))\n",
      "2023-01-25 09:19:56,067 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,070 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,070 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3955 as timestamp))\n",
      "2023-01-25 09:19:56,070 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,073 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,073 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4069 as timestamp))\n",
      "2023-01-25 09:19:56,073 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,076 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:19:56,077 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4185 as timestamp))\n",
      "2023-01-25 09:19:56,077 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:19:56,988 INFO codegen.CodeGenerator: Code generated in 224.461199 ms\n",
      "2023-01-25 09:19:56,993 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,010 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,011 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,012 INFO spark.SparkContext: Created broadcast 7 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:57,013 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:57,109 INFO scheduler.DAGScheduler: Registering RDD 32 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-01-25 09:19:57,114 INFO scheduler.DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:57,115 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:57,115 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:57,116 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:57,118 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[32] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:57,167 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 223.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,170 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 64.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,171 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 64.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,172 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:57,174 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[32] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:57,174 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:57,176 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:57,176 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:57,272 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.140.66:38378 (size: 64.1 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:57,394 INFO codegen.CodeGenerator: Code generated in 138.572425 ms\n",
      "2023-01-25 09:19:57,398 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,421 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,425 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,427 INFO spark.SparkContext: Created broadcast 9 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:57,428 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:57,447 INFO scheduler.DAGScheduler: Registering RDD 37 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "2023-01-25 09:19:57,447 INFO scheduler.DAGScheduler: Got map stage job 3 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:57,447 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:57,447 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:57,447 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:57,455 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[37] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:57,503 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,506 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,508 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,508 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:57,509 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[37] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:57,509 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:57,511 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:19:57,763 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:57,767 INFO codegen.CodeGenerator: Code generated in 146.290618 ms\n",
      "2023-01-25 09:19:57,776 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,804 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,805 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,806 INFO spark.SparkContext: Created broadcast 11 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:57,807 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:57,822 INFO scheduler.DAGScheduler: Registering RDD 42 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "2023-01-25 09:19:57,822 INFO scheduler.DAGScheduler: Got map stage job 4 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:57,822 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:57,822 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:57,822 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:57,823 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[42] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:57,842 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,846 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:57,853 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:57,855 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:57,855 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[42] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:57,856 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:57,965 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.201.8.108:46028 (size: 64.1 KiB, free: 1007.8 MiB)\n",
      "2023-01-25 09:19:58,143 INFO codegen.CodeGenerator: Code generated in 118.262928 ms\n",
      "2023-01-25 09:19:58,148 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,164 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,165 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,166 INFO spark.SparkContext: Created broadcast 13 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:58,167 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:58,179 INFO scheduler.DAGScheduler: Registering RDD 47 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "2023-01-25 09:19:58,179 INFO scheduler.DAGScheduler: Got map stage job 5 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:58,179 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 5 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:58,179 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:58,179 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:58,180 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:58,193 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,196 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,196 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,197 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:58,198 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[47] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:58,198 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:58,396 INFO codegen.CodeGenerator: Code generated in 107.868326 ms\n",
      "2023-01-25 09:19:58,401 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,417 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,418 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,418 INFO spark.SparkContext: Created broadcast 15 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:58,419 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:58,431 INFO scheduler.DAGScheduler: Registering RDD 52 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "2023-01-25 09:19:58,431 INFO scheduler.DAGScheduler: Got map stage job 6 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:58,431 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:58,431 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:58,431 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:58,432 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[52] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:58,448 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,451 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,452 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,452 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:58,453 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[52] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:58,453 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:58,636 INFO codegen.CodeGenerator: Code generated in 102.585106 ms\n",
      "2023-01-25 09:19:58,641 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,656 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,657 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,658 INFO spark.SparkContext: Created broadcast 17 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:58,659 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:58,671 INFO scheduler.DAGScheduler: Registering RDD 57 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "2023-01-25 09:19:58,671 INFO scheduler.DAGScheduler: Got map stage job 7 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:58,671 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:58,671 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:58,672 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:58,672 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[57] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:58,681 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,683 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,684 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,684 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:58,684 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[57] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:58,684 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:58,860 INFO codegen.CodeGenerator: Code generated in 84.427895 ms\n",
      "2023-01-25 09:19:58,864 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,880 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,881 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,882 INFO spark.SparkContext: Created broadcast 19 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:58,883 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:58,893 INFO scheduler.DAGScheduler: Registering RDD 62 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "2023-01-25 09:19:58,894 INFO scheduler.DAGScheduler: Got map stage job 8 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:58,894 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 8 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:58,894 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:58,894 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:58,895 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[62] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:58,904 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,906 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:58,907 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:58,907 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:58,908 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[62] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:58,908 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:59,089 INFO codegen.CodeGenerator: Code generated in 92.376658 ms\n",
      "2023-01-25 09:19:59,093 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,109 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,110 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,111 INFO spark.SparkContext: Created broadcast 21 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:59,112 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:59,125 INFO scheduler.DAGScheduler: Registering RDD 67 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 7\n",
      "2023-01-25 09:19:59,125 INFO scheduler.DAGScheduler: Got map stage job 9 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:59,125 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:59,125 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:59,125 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:59,126 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[67] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:59,137 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,140 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,140 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,141 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:59,141 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[67] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:59,141 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:59,305 INFO codegen.CodeGenerator: Code generated in 78.817948 ms\n",
      "2023-01-25 09:19:59,309 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,325 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,326 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,327 INFO spark.SparkContext: Created broadcast 23 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:59,328 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:59,337 INFO scheduler.DAGScheduler: Registering RDD 72 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 8\n",
      "2023-01-25 09:19:59,338 INFO scheduler.DAGScheduler: Got map stage job 10 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:59,338 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 10 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:59,338 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:59,338 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:59,339 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[72] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:59,346 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,348 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,349 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,349 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:59,350 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[72] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:59,350 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:59,512 INFO codegen.CodeGenerator: Code generated in 77.863678 ms\n",
      "2023-01-25 09:19:59,522 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,538 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,539 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,539 INFO spark.SparkContext: Created broadcast 25 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:59,540 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Registering RDD 77 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 9\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Got map stage job 11 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:59,559 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[77] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:59,566 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,569 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,569 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,570 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:59,570 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[77] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:59,570 INFO scheduler.TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:59,728 INFO codegen.CodeGenerator: Code generated in 75.728252 ms\n",
      "2023-01-25 09:19:59,732 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,749 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,749 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,750 INFO spark.SparkContext: Created broadcast 27 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:59,751 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:59,759 INFO scheduler.DAGScheduler: Registering RDD 82 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 10\n",
      "2023-01-25 09:19:59,760 INFO scheduler.DAGScheduler: Got map stage job 12 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:59,760 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:59,760 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:59,760 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:59,760 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[82] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:59,770 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,772 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,774 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,774 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:59,775 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[82] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:59,775 INFO scheduler.TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:19:59,952 INFO codegen.CodeGenerator: Code generated in 96.776885 ms\n",
      "2023-01-25 09:19:59,956 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,972 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,973 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,974 INFO spark.SparkContext: Created broadcast 29 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:19:59,974 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:19:59,984 INFO scheduler.DAGScheduler: Registering RDD 87 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 11\n",
      "2023-01-25 09:19:59,984 INFO scheduler.DAGScheduler: Got map stage job 13 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:19:59,984 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:19:59,984 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:19:59,984 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:19:59,985 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[87] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:19:59,992 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,994 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:19:59,995 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:19:59,996 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:19:59,996 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[87] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:19:59,996 INFO scheduler.TaskSchedulerImpl: Adding task set 13.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:20:00,160 INFO codegen.CodeGenerator: Code generated in 83.785364 ms\n",
      "2023-01-25 09:20:00,165 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,181 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,182 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,182 INFO spark.SparkContext: Created broadcast 31 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:20:00,183 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:20:00,192 INFO scheduler.DAGScheduler: Registering RDD 92 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 12\n",
      "2023-01-25 09:20:00,192 INFO scheduler.DAGScheduler: Got map stage job 14 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:20:00,192 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 14 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:20:00,192 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:20:00,193 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:20:00,193 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[92] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:20:00,210 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,212 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,212 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,214 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:20:00,214 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[92] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:20:00,214 INFO scheduler.TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:20:00,280 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:00,364 INFO codegen.CodeGenerator: Code generated in 78.697639 ms\n",
      "2023-01-25 09:20:00,367 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,387 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,388 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,389 INFO spark.SparkContext: Created broadcast 33 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:20:00,389 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:20:00,398 INFO scheduler.DAGScheduler: Registering RDD 97 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 13\n",
      "2023-01-25 09:20:00,398 INFO scheduler.DAGScheduler: Got map stage job 15 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:20:00,398 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:20:00,399 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:20:00,399 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:20:00,399 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[97] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:20:00,405 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,407 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,408 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,408 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:20:00,409 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[97] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:20:00,409 INFO scheduler.TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:20:00,563 INFO codegen.CodeGenerator: Code generated in 74.727819 ms\n",
      "2023-01-25 09:20:00,567 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,582 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,583 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,584 INFO spark.SparkContext: Created broadcast 35 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:20:00,584 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:20:00,593 INFO scheduler.DAGScheduler: Registering RDD 102 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 14\n",
      "2023-01-25 09:20:00,593 INFO scheduler.DAGScheduler: Got map stage job 16 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:20:00,593 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:20:00,593 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:20:00,594 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:20:00,594 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[102] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:20:00,601 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,603 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,604 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,604 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:20:00,604 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[102] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:20:00,604 INFO scheduler.TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:20:00,754 INFO codegen.CodeGenerator: Code generated in 65.776589 ms\n",
      "2023-01-25 09:20:00,757 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,773 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,773 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,774 INFO spark.SparkContext: Created broadcast 37 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:20:00,775 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:20:00,783 INFO scheduler.DAGScheduler: Registering RDD 107 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "2023-01-25 09:20:00,783 INFO scheduler.DAGScheduler: Got map stage job 17 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:20:00,783 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 17 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:20:00,783 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:20:00,783 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:20:00,784 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[107] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:20:00,789 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,792 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:20:00,792 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:00,792 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:20:00,793 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[107] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:20:00,793 INFO scheduler.TaskSchedulerImpl: Adding task set 17.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:20:01,065 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:05,660 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:10,469 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 6) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:10,471 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 13295 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:20:10,483 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:11,277 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-01-25 09:20:14,160 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:14,161 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 16650 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:14,256 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:15,368 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-01-25 09:20:17,882 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:17,883 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 6) in 7415 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:20:17,883 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:17,884 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 20.428 s\n",
      "2023-01-25 09:20:17,885 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:17,885 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 2, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 4, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:17,886 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:17,886 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:18,260 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:18,869 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:21,471 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:21,471 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 7311 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:21,559 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:22,164 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:23,674 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 10) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:23,675 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 5793 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:20:23,675 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:23,676 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 25.852 s\n",
      "2023-01-25 09:20:23,676 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:23,676 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 2, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:23,676 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:23,676 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:23,768 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:24,270 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:24,875 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 11) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:24,875 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 27700 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:20:24,875 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:24,876 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 27.755 s\n",
      "2023-01-25 09:20:24,876 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:24,876 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:24,876 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:24,876 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:25,066 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1007.7 MiB)\n",
      "2023-01-25 09:20:25,876 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.6 MiB)\n",
      "2023-01-25 09:20:27,462 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 12) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:27,462 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 5992 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:27,566 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:28,074 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:28,791 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 13) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:28,792 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 10) in 5118 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:20:28,792 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:28,793 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 30.612 s\n",
      "2023-01-25 09:20:28,793 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:28,793 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:28,793 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:28,793 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:28,876 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:29,369 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:32,959 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 14) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:32,959 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 12) in 5497 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:32,971 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:33,472 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:34,361 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:34,361 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 11) in 9486 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:20:34,361 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:34,362 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 35.929 s\n",
      "2023-01-25 09:20:34,362 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:34,362 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 7, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:34,362 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:34,362 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:34,458 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:35,062 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-01-25 09:20:35,988 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:35,988 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 13) in 7197 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:20:36,076 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:36,568 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:20:39,377 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:39,377 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 14) in 6418 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:20:39,378 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:39,378 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 40.705 s\n",
      "2023-01-25 09:20:39,378 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:39,378 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 8)\n",
      "2023-01-25 09:20:39,378 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:39,378 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:39,465 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:39,868 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:20:40,383 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:40,384 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 6023 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:20:40,466 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:41,067 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:41,382 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 19) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:41,382 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 5395 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:20:41,382 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:41,383 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 42.488 s\n",
      "2023-01-25 09:20:41,383 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:41,383 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 9, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-01-25 09:20:41,383 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:41,383 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:41,471 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:20:41,875 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:45,076 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 20) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:45,077 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 5700 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:45,159 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:45,571 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:45,870 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 21) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:45,871 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 5488 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:20:45,871 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:45,872 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 46.745 s\n",
      "2023-01-25 09:20:45,872 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:45,872 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-01-25 09:20:45,872 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:45,872 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:45,958 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:46,168 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 22) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:46,168 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 19) in 4786 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:20:46,178 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:46,464 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:46,768 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:49,573 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 23) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:49,574 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 20) in 4498 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:20:49,574 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:49,574 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 50.235 s\n",
      "2023-01-25 09:20:49,575 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:49,575 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-01-25 09:20:49,575 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:49,575 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:49,657 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:50,072 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:51,578 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 24) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:51,578 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 22) in 5410 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:20:51,660 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:52,074 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:52,859 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 25) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:52,859 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 21) in 6989 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:20:52,859 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:52,860 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 53.300 s\n",
      "2023-01-25 09:20:52,860 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:52,860 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-01-25 09:20:52,860 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:52,860 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:52,871 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.201.8.108:46028 (size: 71.1 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:53,366 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:20:54,374 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 26) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:54,374 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 23) in 4801 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:20:54,569 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.201.8.107:34250 (size: 71.1 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:55,067 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:20:56,511 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 27) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:56,511 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 24) in 4933 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:20:56,511 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:20:56,512 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 56.751 s\n",
      "2023-01-25 09:20:56,512 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:20:56,512 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 13, ShuffleMapStage 14)\n",
      "2023-01-25 09:20:56,512 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:20:56,512 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:20:56,521 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,559 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:56,560 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:20:56,571 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,573 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:56,575 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:56,657 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:56,662 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,666 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:56,668 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:20:56,673 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,675 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:56,759 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:20:56,765 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,769 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:56,769 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:56,774 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,775 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:56,859 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:56,864 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:20:56,867 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:20:56,868 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:20:57,268 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:20:58,279 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 28) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:20:58,279 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 25) in 5420 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:20:58,371 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:20:58,770 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:21:00,760 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 29) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:00,760 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 26) in 6387 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:00,760 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:00,761 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 60.776 s\n",
      "2023-01-25 09:21:00,761 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:00,761 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 14)\n",
      "2023-01-25 09:21:00,761 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:00,761 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:00,770 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.201.8.107:34250 (size: 71.1 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:01,175 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:01,906 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 30) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:01,907 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 27) in 5396 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:01,975 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.200.140.66:38378 (size: 71.1 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:02,375 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:03,364 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 31) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:03,364 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 28) in 5086 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:03,364 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:03,365 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 63.172 s\n",
      "2023-01-25 09:21:03,365 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:03,365 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17)\n",
      "2023-01-25 09:21:03,365 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:03,365 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:03,374 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:03,958 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:05,477 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 32) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:05,478 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 29) in 4718 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:05,487 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:05,870 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:06,515 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 33) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:06,515 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 30) in 4609 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:21:06,515 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:06,516 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 66.116 s\n",
      "2023-01-25 09:21:06,516 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:06,516 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 16, ShuffleMapStage 17)\n",
      "2023-01-25 09:21:06,516 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:06,516 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:06,575 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:07,071 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:07,902 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 34) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:07,902 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 31) in 4538 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:21:07,965 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:08,458 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:10,562 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 32) in 5085 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:10,562 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:10,563 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 69.968 s\n",
      "2023-01-25 09:21:10,563 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:10,563 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 17)\n",
      "2023-01-25 09:21:10,563 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:10,563 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:11,390 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 33) in 4875 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:13,271 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 34) in 5369 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:13,271 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:13,272 INFO scheduler.DAGScheduler: ShuffleMapStage 17 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 72.488 s\n",
      "2023-01-25 09:21:13,272 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:13,272 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:21:13,272 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:13,272 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:13,301 INFO adaptive.ShufflePartitionsUtil: For shuffle(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), advisory target size: 67108864, actual target size 6208691, minimum partition size: 1048576\n",
      "2023-01-25 09:21:13,350 INFO codegen.CodeGenerator: Code generated in 11.036871 ms\n",
      "2023-01-25 09:21:13,363 INFO codegen.CodeGenerator: Code generated in 7.913191 ms\n",
      "2023-01-25 09:21:13,409 INFO codegen.CodeGenerator: Code generated in 11.668724 ms\n",
      "2023-01-25 09:21:13,420 INFO codegen.CodeGenerator: Code generated in 7.836674 ms\n",
      "2023-01-25 09:21:13,450 INFO codegen.CodeGenerator: Code generated in 11.267648 ms\n",
      "2023-01-25 09:21:13,461 INFO codegen.CodeGenerator: Code generated in 7.853743 ms\n",
      "2023-01-25 09:21:13,488 INFO codegen.CodeGenerator: Code generated in 11.13381 ms\n",
      "2023-01-25 09:21:13,500 INFO codegen.CodeGenerator: Code generated in 7.989577 ms\n",
      "2023-01-25 09:21:13,539 INFO codegen.CodeGenerator: Code generated in 12.489071 ms\n",
      "2023-01-25 09:21:13,550 INFO codegen.CodeGenerator: Code generated in 8.462108 ms\n",
      "2023-01-25 09:21:13,576 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:13,578 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.201.8.107:34250 in memory (size: 71.1 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:13,578 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.200.140.66:38378 in memory (size: 71.1 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:13,581 INFO codegen.CodeGenerator: Code generated in 16.816584 ms\n",
      "2023-01-25 09:21:13,583 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:13,584 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:13,587 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:13,593 INFO codegen.CodeGenerator: Code generated in 8.569028 ms\n",
      "2023-01-25 09:21:13,596 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:13,596 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:13,601 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:13,606 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:13,606 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:13,612 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:13,619 INFO codegen.CodeGenerator: Code generated in 10.926742 ms\n",
      "2023-01-25 09:21:13,630 INFO codegen.CodeGenerator: Code generated in 7.969844 ms\n",
      "2023-01-25 09:21:13,654 INFO codegen.CodeGenerator: Code generated in 10.420825 ms\n",
      "2023-01-25 09:21:13,665 INFO codegen.CodeGenerator: Code generated in 7.982402 ms\n",
      "2023-01-25 09:21:13,689 INFO codegen.CodeGenerator: Code generated in 10.234559 ms\n",
      "2023-01-25 09:21:13,701 INFO codegen.CodeGenerator: Code generated in 8.515829 ms\n",
      "2023-01-25 09:21:13,725 INFO codegen.CodeGenerator: Code generated in 10.218602 ms\n",
      "2023-01-25 09:21:13,736 INFO codegen.CodeGenerator: Code generated in 8.418681 ms\n",
      "2023-01-25 09:21:13,766 INFO codegen.CodeGenerator: Code generated in 10.602256 ms\n",
      "2023-01-25 09:21:13,777 INFO codegen.CodeGenerator: Code generated in 8.226271 ms\n",
      "2023-01-25 09:21:13,801 INFO codegen.CodeGenerator: Code generated in 10.16556 ms\n",
      "2023-01-25 09:21:13,811 INFO codegen.CodeGenerator: Code generated in 7.802673 ms\n",
      "2023-01-25 09:21:13,835 INFO codegen.CodeGenerator: Code generated in 10.084428 ms\n",
      "2023-01-25 09:21:13,846 INFO codegen.CodeGenerator: Code generated in 7.937836 ms\n",
      "2023-01-25 09:21:13,869 INFO codegen.CodeGenerator: Code generated in 10.047061 ms\n",
      "2023-01-25 09:21:13,881 INFO codegen.CodeGenerator: Code generated in 7.992087 ms\n",
      "2023-01-25 09:21:13,904 INFO codegen.CodeGenerator: Code generated in 9.675141 ms\n",
      "2023-01-25 09:21:13,915 INFO codegen.CodeGenerator: Code generated in 7.594661 ms\n",
      "2023-01-25 09:21:13,938 INFO codegen.CodeGenerator: Code generated in 9.657324 ms\n",
      "2023-01-25 09:21:13,948 INFO codegen.CodeGenerator: Code generated in 7.787848 ms\n",
      "2023-01-25 09:21:15,690 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,690 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-01-25 09:21:15,691 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,693 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,693 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2743 as timestamp))\n",
      "2023-01-25 09:21:15,693 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,696 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,696 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2833 as timestamp))\n",
      "2023-01-25 09:21:15,696 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,698 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,698 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2925 as timestamp))\n",
      "2023-01-25 09:21:15,698 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,700 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,701 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3019 as timestamp))\n",
      "2023-01-25 09:21:15,701 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,703 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,703 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3115 as timestamp))\n",
      "2023-01-25 09:21:15,703 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,705 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,705 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3213 as timestamp))\n",
      "2023-01-25 09:21:15,706 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,708 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,708 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3313 as timestamp))\n",
      "2023-01-25 09:21:15,708 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,710 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,710 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3415 as timestamp))\n",
      "2023-01-25 09:21:15,710 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,712 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,713 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3519 as timestamp))\n",
      "2023-01-25 09:21:15,713 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,715 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,715 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3625 as timestamp))\n",
      "2023-01-25 09:21:15,715 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,717 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,717 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3733 as timestamp))\n",
      "2023-01-25 09:21:15,718 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,720 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,720 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3843 as timestamp))\n",
      "2023-01-25 09:21:15,720 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,722 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,722 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3955 as timestamp))\n",
      "2023-01-25 09:21:15,722 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,724 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,725 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4069 as timestamp))\n",
      "2023-01-25 09:21:15,725 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:15,727 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:21:15,727 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4185 as timestamp))\n",
      "2023-01-25 09:21:15,727 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:21:16,070 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,085 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,086 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,086 INFO spark.SparkContext: Created broadcast 39 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,087 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,128 INFO scheduler.DAGScheduler: Registering RDD 180 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
      "2023-01-25 09:21:16,128 INFO scheduler.DAGScheduler: Got map stage job 18 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,128 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,128 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,128 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,129 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[180] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,142 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 223.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,144 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 64.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,144 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 64.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,145 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,145 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[180] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,145 INFO scheduler.TaskSchedulerImpl: Adding task set 18.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,146 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 35) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:16,146 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 36) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:16,157 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.201.8.107:34250 (size: 64.1 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:16,158 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.200.140.66:38378 (size: 64.1 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:16,175 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:16,239 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,262 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,262 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,263 INFO spark.SparkContext: Created broadcast 41 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,264 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,275 INFO scheduler.DAGScheduler: Registering RDD 185 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 17\n",
      "2023-01-25 09:21:16,275 INFO scheduler.DAGScheduler: Got map stage job 19 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,275 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 19 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,275 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,276 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,276 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[185] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,284 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,287 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,287 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,287 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,292 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[185] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,292 INFO scheduler.TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,293 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 37) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:16,305 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:16,374 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,391 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,392 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,392 INFO spark.SparkContext: Created broadcast 43 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,393 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,402 INFO scheduler.DAGScheduler: Registering RDD 190 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 18\n",
      "2023-01-25 09:21:16,402 INFO scheduler.DAGScheduler: Got map stage job 20 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,402 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,402 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,402 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,403 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[190] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,409 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,412 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,412 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,413 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,413 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[190] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,413 INFO scheduler.TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,450 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,458 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.201.8.108:46028 in memory (size: 71.1 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:16,458 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.201.8.107:34250 in memory (size: 71.1 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:16,472 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,473 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:16,512 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,527 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,528 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,528 INFO spark.SparkContext: Created broadcast 45 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,529 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,537 INFO scheduler.DAGScheduler: Registering RDD 195 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 19\n",
      "2023-01-25 09:21:16,537 INFO scheduler.DAGScheduler: Got map stage job 21 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,537 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,537 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,537 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,538 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[195] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,546 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,548 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,548 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,548 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,549 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[195] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,549 INFO scheduler.TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,557 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:16,564 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 64.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,564 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.201.8.108:46028 in memory (size: 64.1 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:21:16,568 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.140.66:38378 in memory (size: 64.1 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:16,571 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:16,572 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,573 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:16,574 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:16,578 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,580 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:16,580 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:16,629 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,651 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,651 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,652 INFO spark.SparkContext: Created broadcast 47 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,652 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,661 INFO scheduler.DAGScheduler: Registering RDD 200 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 20\n",
      "2023-01-25 09:21:16,661 INFO scheduler.DAGScheduler: Got map stage job 22 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,661 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 22 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,661 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,661 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,662 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[200] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,668 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,670 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,671 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,671 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,671 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[200] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,671 INFO scheduler.TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,753 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,759 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:16,769 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,769 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,770 INFO spark.SparkContext: Created broadcast 49 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,770 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Registering RDD 205 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Got map stage job 23 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 23 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,782 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[205] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,789 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,792 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,793 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,799 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,799 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[205] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,799 INFO scheduler.TaskSchedulerImpl: Adding task set 23.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:16,896 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,912 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,912 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,913 INFO spark.SparkContext: Created broadcast 51 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:16,913 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:16,921 INFO scheduler.DAGScheduler: Registering RDD 210 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
      "2023-01-25 09:21:16,921 INFO scheduler.DAGScheduler: Got map stage job 24 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:16,921 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:16,921 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:16,922 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:16,922 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[210] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:16,930 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,932 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:16,932 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:16,933 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:16,933 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[210] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:16,933 INFO scheduler.TaskSchedulerImpl: Adding task set 24.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,010 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,034 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,034 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,035 INFO spark.SparkContext: Created broadcast 53 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,036 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,050 INFO scheduler.DAGScheduler: Registering RDD 215 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 23\n",
      "2023-01-25 09:21:17,050 INFO scheduler.DAGScheduler: Got map stage job 25 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,050 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,050 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,050 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,051 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[215] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,057 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,059 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,059 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,060 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,060 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[215] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,060 INFO scheduler.TaskSchedulerImpl: Adding task set 25.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,139 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,155 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,155 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,156 INFO spark.SparkContext: Created broadcast 55 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,156 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Registering RDD 220 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 24\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Got map stage job 26 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 26 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,165 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[220] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,171 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,173 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,173 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,174 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,174 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[220] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,174 INFO scheduler.TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,267 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,282 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,283 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,283 INFO spark.SparkContext: Created broadcast 57 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,284 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,306 INFO scheduler.DAGScheduler: Registering RDD 225 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 25\n",
      "2023-01-25 09:21:17,306 INFO scheduler.DAGScheduler: Got map stage job 27 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,306 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 27 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,306 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,306 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,307 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[225] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,313 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,315 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,315 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,315 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,316 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[225] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,316 INFO scheduler.TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,395 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,417 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,418 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,419 INFO spark.SparkContext: Created broadcast 59 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,419 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,427 INFO scheduler.DAGScheduler: Registering RDD 230 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 26\n",
      "2023-01-25 09:21:17,427 INFO scheduler.DAGScheduler: Got map stage job 28 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,427 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 28 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,427 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,427 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,428 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[230] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,434 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,436 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 70.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,436 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,436 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,437 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[230] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,437 INFO scheduler.TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,517 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,534 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,535 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,535 INFO spark.SparkContext: Created broadcast 61 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,536 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,544 INFO scheduler.DAGScheduler: Registering RDD 235 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 27\n",
      "2023-01-25 09:21:17,545 INFO scheduler.DAGScheduler: Got map stage job 29 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,545 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,545 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,545 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,545 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[235] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,555 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,557 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,558 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,558 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,558 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[235] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,558 INFO scheduler.TaskSchedulerImpl: Adding task set 29.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,665 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,680 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,681 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,684 INFO spark.SparkContext: Created broadcast 63 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,685 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,692 INFO scheduler.DAGScheduler: Registering RDD 240 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 28\n",
      "2023-01-25 09:21:17,692 INFO scheduler.DAGScheduler: Got map stage job 30 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,692 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 30 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,693 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,693 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,693 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[240] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,701 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,703 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,704 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,704 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,704 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[240] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,704 INFO scheduler.TaskSchedulerImpl: Adding task set 30.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,781 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,803 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,803 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,804 INFO spark.SparkContext: Created broadcast 65 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,805 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Registering RDD 245 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 29\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Got map stage job 31 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 31 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,813 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[245] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,821 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,823 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,823 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,824 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,824 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[245] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,824 INFO scheduler.TaskSchedulerImpl: Adding task set 31.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:17,903 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,919 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,920 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,920 INFO spark.SparkContext: Created broadcast 67 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:17,921 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Registering RDD 250 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 30\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Got map stage job 32 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 32 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:17,930 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[250] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:17,938 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,940 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:17,941 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:17,941 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:17,942 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[250] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:17,942 INFO scheduler.TaskSchedulerImpl: Adding task set 32.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:18,039 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:18,055 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:18,056 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:18,056 INFO spark.SparkContext: Created broadcast 69 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:21:18,057 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Registering RDD 255 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 31\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Got map stage job 33 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:21:18,065 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[255] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:21:18,072 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 255.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:18,074 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:21:18,074 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:18,075 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:21:18,075 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[255] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:21:18,075 INFO scheduler.TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:21:20,474 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 38) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:20,475 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 36) in 4329 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:20,484 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:20,661 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:22,258 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 39) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:22,259 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 37) in 5966 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:21:22,268 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:21:22,665 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:23,085 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 40) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:23,085 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 35) in 6939 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:23,085 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:23,086 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (summary at NativeMethodAccessorImpl.java:0) finished in 6.957 s\n",
      "2023-01-25 09:21:23,086 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:23,086 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-01-25 09:21:23,086 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:23,086 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:23,162 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:23,260 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:23,885 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 41) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:23,886 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 38) in 3412 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:21:23,886 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:23,886 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (summary at NativeMethodAccessorImpl.java:0) finished in 7.610 s\n",
      "2023-01-25 09:21:23,886 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:23,886 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-01-25 09:21:23,886 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:23,886 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:23,894 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:23,983 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:26,762 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 42) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:26,763 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 40) in 3679 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:26,772 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:26,867 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:27,884 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 43) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:27,885 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 39) in 5627 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:27,885 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:27,885 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (summary at NativeMethodAccessorImpl.java:0) finished in 11.482 s\n",
      "2023-01-25 09:21:27,886 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:27,886 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 21, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-01-25 09:21:27,886 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:27,886 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:27,962 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:21:28,060 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:28,588 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 44) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:28,588 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 41) in 4703 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:28,674 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:29,067 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:30,762 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 45) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:30,763 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 42) in 4001 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:30,763 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:30,763 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (summary at NativeMethodAccessorImpl.java:0) finished in 14.225 s\n",
      "2023-01-25 09:21:30,764 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:30,764 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 22, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-01-25 09:21:30,764 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:30,764 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:30,772 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:30,867 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:31,593 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 46) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:31,594 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 43) in 3710 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:21:31,663 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:32,066 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:32,493 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 47) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:32,493 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 44) in 3905 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:21:32,493 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:32,494 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (summary at NativeMethodAccessorImpl.java:0) finished in 15.832 s\n",
      "2023-01-25 09:21:32,494 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:32,494 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29, ShuffleMapStage 23)\n",
      "2023-01-25 09:21:32,494 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:32,494 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:32,501 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:32,579 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:34,702 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 48) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:34,702 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 45) in 3940 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:34,767 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:35,165 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:35,705 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 49) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:35,705 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 47) in 3212 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:35,774 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:36,168 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:36,761 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 25.0 (TID 50) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:36,761 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 46) in 5168 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:36,762 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:36,762 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (summary at NativeMethodAccessorImpl.java:0) finished in 19.979 s\n",
      "2023-01-25 09:21:36,762 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:36,762 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:36,762 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:36,762 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:36,770 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.201.8.108:46028 (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:36,866 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:39,277 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 51) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:39,277 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 48) in 4575 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:39,277 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:39,278 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (summary at NativeMethodAccessorImpl.java:0) finished in 22.356 s\n",
      "2023-01-25 09:21:39,278 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:39,278 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 25, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:39,278 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:39,278 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:39,285 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:39,367 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:40,379 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 52) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:40,379 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 25.0 (TID 50) in 3618 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:21:40,389 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:40,573 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 53) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:40,574 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 49) in 4869 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:21:40,574 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:40,574 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (summary at NativeMethodAccessorImpl.java:0) finished in 23.522 s\n",
      "2023-01-25 09:21:40,574 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:40,574 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 26, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:40,574 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:40,574 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:40,589 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.200.140.66:38378 (size: 71.0 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:21:40,590 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,591 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:40,658 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:40,669 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,671 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:40,672 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:40,681 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:40,759 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,760 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:40,768 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:40,771 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,772 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:40,857 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:40,861 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,863 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:40,863 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:40,866 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:40,869 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,871 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:40,872 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:40,958 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:21:40,960 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:40,962 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.201.8.108:46028 in memory (size: 71.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:21:42,763 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 54) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:42,764 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 51) in 3488 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:42,772 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.201.8.107:34250 (size: 71.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:43,167 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:43,981 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 55) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:43,982 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 53) in 3409 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:43,989 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.200.140.66:38378 (size: 70.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:44,082 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:45,581 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 56) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:45,581 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 52) in 5202 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:45,581 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:45,582 INFO scheduler.DAGScheduler: ShuffleMapStage 26 (summary at NativeMethodAccessorImpl.java:0) finished in 28.416 s\n",
      "2023-01-25 09:21:45,582 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:45,582 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 27, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:45,582 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:45,582 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:45,591 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.201.8.108:46028 (size: 70.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:45,972 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:21:47,063 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 57) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:47,063 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 54) in 4300 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:47,063 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:47,064 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (summary at NativeMethodAccessorImpl.java:0) finished in 29.757 s\n",
      "2023-01-25 09:21:47,064 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:47,064 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 28, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:47,064 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:47,064 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:47,076 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.201.8.107:34250 (size: 71.1 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:47,172 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:47,792 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 58) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:47,793 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 55) in 3812 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:47,868 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.200.140.66:38378 (size: 71.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:48,188 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:50,567 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 59) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:50,568 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 56) in 4988 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:50,568 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:50,568 INFO scheduler.DAGScheduler: ShuffleMapStage 28 (summary at NativeMethodAccessorImpl.java:0) finished in 33.140 s\n",
      "2023-01-25 09:21:50,569 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:50,569 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 29)\n",
      "2023-01-25 09:21:50,569 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:50,569 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:50,577 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.201.8.108:46028 (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:21:50,667 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:51,566 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 60) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:51,566 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 57) in 4503 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:51,576 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:51,669 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 61) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:51,669 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 58) in 3877 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:21:51,669 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:51,670 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (summary at NativeMethodAccessorImpl.java:0) finished in 34.123 s\n",
      "2023-01-25 09:21:51,670 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:51,670 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 30, ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-01-25 09:21:51,670 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:51,670 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:51,677 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.200.140.66:38378 (size: 71.1 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:21:51,769 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:21:51,969 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:21:54,163 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 62) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:54,164 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 59) in 3597 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:21:54,172 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.201.8.108:46028 (size: 71.1 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:21:54,565 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:21:54,983 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 63) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:54,983 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 61) in 3314 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:21:55,071 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.200.140.66:38378 (size: 70.9 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:21:55,387 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:21:56,089 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 64) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:56,090 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 60) in 4525 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:21:56,090 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:56,090 INFO scheduler.DAGScheduler: ShuffleMapStage 30 (summary at NativeMethodAccessorImpl.java:0) finished in 38.397 s\n",
      "2023-01-25 09:21:56,090 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:56,090 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 31, ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-01-25 09:21:56,090 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:56,090 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:56,098 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.201.8.107:34250 (size: 70.9 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:21:56,168 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:21:58,692 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 65) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:58,692 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 64) in 2603 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:21:58,761 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.201.8.107:34250 (size: 71.1 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:21:58,883 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 66) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:21:58,883 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 62) in 4720 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:21:58,883 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:21:58,883 INFO scheduler.DAGScheduler: ShuffleMapStage 31 (summary at NativeMethodAccessorImpl.java:0) finished in 41.069 s\n",
      "2023-01-25 09:21:58,884 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:21:58,884 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 32, ShuffleMapStage 33)\n",
      "2023-01-25 09:21:58,884 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:21:58,884 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:21:58,960 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.201.8.108:46028 (size: 71.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:58,977 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:21:59,162 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:22:00,903 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 63) in 5920 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:22:00,903 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:22:00,903 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (summary at NativeMethodAccessorImpl.java:0) finished in 42.972 s\n",
      "2023-01-25 09:22:00,904 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:22:00,904 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 33)\n",
      "2023-01-25 09:22:00,904 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:22:00,904 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:22:01,592 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 66) in 2710 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:22:03,405 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 65) in 4713 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:22:03,405 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:22:03,405 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (summary at NativeMethodAccessorImpl.java:0) finished in 45.339 s\n",
      "2023-01-25 09:22:03,405 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:22:03,405 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:22:03,405 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:22:03,405 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:22:03,433 INFO adaptive.ShufflePartitionsUtil: For shuffle(16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31), advisory target size: 67108864, actual target size 6208691, minimum partition size: 1048576\n",
      "2023-01-25 09:22:03,481 INFO codegen.CodeGenerator: Code generated in 10.557034 ms\n",
      "2023-01-25 09:22:03,514 INFO codegen.CodeGenerator: Code generated in 10.482428 ms\n",
      "2023-01-25 09:22:03,542 INFO codegen.CodeGenerator: Code generated in 10.211081 ms\n",
      "2023-01-25 09:22:03,569 INFO codegen.CodeGenerator: Code generated in 9.746426 ms\n",
      "2023-01-25 09:22:03,602 INFO codegen.CodeGenerator: Code generated in 10.308707 ms\n",
      "2023-01-25 09:22:03,632 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,633 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.201.8.107:34250 in memory (size: 71.1 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:22:03,639 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.201.8.108:46028 in memory (size: 71.1 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:22:03,642 INFO codegen.CodeGenerator: Code generated in 11.346459 ms\n",
      "2023-01-25 09:22:03,647 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,647 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.200.140.66:38378 in memory (size: 71.0 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:22:03,647 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.201.8.107:34250 in memory (size: 71.0 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:22:03,657 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,659 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:22:03,659 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.200.140.66:38378 in memory (size: 70.9 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:22:03,666 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,667 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:22:03,668 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:22:03,674 INFO codegen.CodeGenerator: Code generated in 11.512425 ms\n",
      "2023-01-25 09:22:03,679 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.200.140.66:38378 in memory (size: 71.1 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:22:03,679 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.201.8.108:46028 in memory (size: 71.1 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:22:03,686 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,689 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,690 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.200.140.66:38378 in memory (size: 70.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:22:03,690 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.201.8.108:46028 in memory (size: 70.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:22:03,695 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 70.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:03,696 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.201.8.107:34250 in memory (size: 70.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:22:03,696 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.201.8.108:46028 in memory (size: 70.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:22:03,705 INFO codegen.CodeGenerator: Code generated in 9.788144 ms\n",
      "2023-01-25 09:22:03,732 INFO codegen.CodeGenerator: Code generated in 9.774422 ms\n",
      "2023-01-25 09:22:03,759 INFO codegen.CodeGenerator: Code generated in 9.914496 ms\n",
      "2023-01-25 09:22:03,799 INFO codegen.CodeGenerator: Code generated in 9.903514 ms\n",
      "2023-01-25 09:22:03,828 INFO codegen.CodeGenerator: Code generated in 12.595004 ms\n",
      "2023-01-25 09:22:03,853 INFO codegen.CodeGenerator: Code generated in 9.641623 ms\n",
      "2023-01-25 09:22:03,879 INFO codegen.CodeGenerator: Code generated in 9.075389 ms\n",
      "2023-01-25 09:22:03,903 INFO codegen.CodeGenerator: Code generated in 8.499705 ms\n",
      "2023-01-25 09:22:03,926 INFO codegen.CodeGenerator: Code generated in 8.654298 ms\n",
      "2023-01-25 09:22:04,001 INFO scheduler.DAGScheduler: Registering RDD 322 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 32\n",
      "2023-01-25 09:22:04,001 INFO scheduler.DAGScheduler: Got map stage job 34 (summary at NativeMethodAccessorImpl.java:0) with 48 output partitions\n",
      "2023-01-25 09:22:04,001 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 50 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:22:04,001 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 47, ShuffleMapStage 41, ShuffleMapStage 48, ShuffleMapStage 34, ShuffleMapStage 49, ShuffleMapStage 35, ShuffleMapStage 42, ShuffleMapStage 36, ShuffleMapStage 43, ShuffleMapStage 44)\n",
      "2023-01-25 09:22:04,002 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:22:04,002 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[322] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:22:04,203 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "2023-01-25 09:22:04,203 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 2.5 MiB, free 2.2 GiB)\n",
      "2023-01-25 09:22:04,210 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 497.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:22:04,211 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 497.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:04,211 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:22:04,213 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[322] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-01-25 09:22:04,213 INFO scheduler.TaskSchedulerImpl: Adding task set 50.0 with 48 tasks resource profile 0\n",
      "2023-01-25 09:22:04,220 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 67) (10.200.140.66, executor 2, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:04,221 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 50.0 (TID 68) (10.201.8.107, executor 1, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:04,221 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 50.0 (TID 69) (10.201.8.108, executor 3, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:04,239 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.200.140.66:38378 (size: 497.3 KiB, free: 1006.0 MiB)\n",
      "2023-01-25 09:22:04,242 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.201.8.107:34250 (size: 497.3 KiB, free: 1006.1 MiB)\n",
      "2023-01-25 09:22:04,243 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.201.8.108:46028 (size: 497.3 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:22:04,684 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:04,775 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:04,870 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:14,170 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 50.0 (TID 70) (10.201.8.108, executor 3, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:14,170 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 50.0 (TID 69) in 9949 ms on 10.201.8.108 (executor 3) (1/48)\n",
      "2023-01-25 09:22:18,163 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 50.0 (TID 71) (10.201.8.108, executor 3, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:18,163 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 50.0 (TID 70) in 3994 ms on 10.201.8.108 (executor 3) (2/48)\n",
      "2023-01-25 09:22:18,269 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 50.0 (TID 72) (10.200.140.66, executor 2, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:18,269 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 67) in 14052 ms on 10.200.140.66 (executor 2) (3/48)\n",
      "2023-01-25 09:22:18,958 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 50.0 (TID 73) (10.201.8.107, executor 1, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:18,958 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 50.0 (TID 68) in 14738 ms on 10.201.8.107 (executor 1) (4/48)\n",
      "2023-01-25 09:22:20,278 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:21,761 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 50.0 (TID 74) (10.201.8.108, executor 3, partition 7, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:21,762 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 50.0 (TID 71) in 3599 ms on 10.201.8.108 (executor 3) (5/48)\n",
      "2023-01-25 09:22:22,070 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:24,066 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 50.0 (TID 75) (10.201.8.107, executor 1, partition 8, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:24,066 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 50.0 (TID 73) in 5109 ms on 10.201.8.107 (executor 1) (6/48)\n",
      "2023-01-25 09:22:26,173 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 50.0 (TID 76) (10.201.8.108, executor 3, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:26,173 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 50.0 (TID 74) in 4412 ms on 10.201.8.108 (executor 3) (7/48)\n",
      "2023-01-25 09:22:26,472 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:27,771 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 50.0 (TID 77) (10.201.8.107, executor 1, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:27,772 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 50.0 (TID 75) in 3706 ms on 10.201.8.107 (executor 1) (8/48)\n",
      "2023-01-25 09:22:27,876 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 50.0 (TID 78) (10.200.140.66, executor 2, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:27,876 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 50.0 (TID 72) in 9608 ms on 10.200.140.66 (executor 2) (9/48)\n",
      "2023-01-25 09:22:28,081 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:28,264 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 64.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:22:28,267 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.201.8.107:34250 in memory (size: 64.1 KiB, free: 1006.1 MiB)\n",
      "2023-01-25 09:22:28,272 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.200.140.66:38378 in memory (size: 64.1 KiB, free: 1006.1 MiB)\n",
      "2023-01-25 09:22:28,277 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:29,963 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 50.0 (TID 79) (10.201.8.108, executor 3, partition 13, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:29,963 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 50.0 (TID 76) in 3791 ms on 10.201.8.108 (executor 3) (10/48)\n",
      "2023-01-25 09:22:31,476 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 50.0 (TID 80) (10.201.8.107, executor 1, partition 11, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:31,476 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 50.0 (TID 77) in 3705 ms on 10.201.8.107 (executor 1) (11/48)\n",
      "2023-01-25 09:22:32,176 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 50.0 (TID 81) (10.200.140.66, executor 2, partition 14, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:32,177 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 50.0 (TID 78) in 4302 ms on 10.200.140.66 (executor 2) (12/48)\n",
      "2023-01-25 09:22:32,482 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:33,367 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 50.0 (TID 82) (10.201.8.108, executor 3, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:33,367 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 50.0 (TID 79) in 3405 ms on 10.201.8.108 (executor 3) (13/48)\n",
      "2023-01-25 09:22:33,667 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:34,859 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 50.0 (TID 83) (10.201.8.107, executor 1, partition 16, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:34,860 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 50.0 (TID 80) in 3384 ms on 10.201.8.107 (executor 1) (14/48)\n",
      "2023-01-25 09:22:35,162 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:35,679 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 50.0 (TID 84) (10.200.140.66, executor 2, partition 18, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:35,679 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 50.0 (TID 81) in 3503 ms on 10.200.140.66 (executor 2) (15/48)\n",
      "2023-01-25 09:22:35,981 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:36,771 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 50.0 (TID 85) (10.201.8.108, executor 3, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:36,771 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 50.0 (TID 82) in 3405 ms on 10.201.8.108 (executor 3) (16/48)\n",
      "2023-01-25 09:22:38,675 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 50.0 (TID 86) (10.201.8.107, executor 1, partition 19, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:38,675 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 50.0 (TID 83) in 3816 ms on 10.201.8.107 (executor 1) (17/48)\n",
      "2023-01-25 09:22:38,978 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:39,370 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 50.0 (TID 87) (10.200.140.66, executor 2, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:39,370 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 50.0 (TID 84) in 3691 ms on 10.200.140.66 (executor 2) (18/48)\n",
      "2023-01-25 09:22:40,376 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 50.0 (TID 88) (10.201.8.108, executor 3, partition 21, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:40,376 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 50.0 (TID 85) in 3605 ms on 10.201.8.108 (executor 3) (19/48)\n",
      "2023-01-25 09:22:40,669 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:42,182 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 50.0 (TID 89) (10.201.8.107, executor 1, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:42,182 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 50.0 (TID 86) in 3507 ms on 10.201.8.107 (executor 1) (20/48)\n",
      "2023-01-25 09:22:42,287 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 50.0 (TID 90) (10.200.140.66, executor 2, partition 22, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:42,288 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 50.0 (TID 87) in 2919 ms on 10.200.140.66 (executor 2) (21/48)\n",
      "2023-01-25 09:22:42,556 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:42,573 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:43,583 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 50.0 (TID 91) (10.201.8.108, executor 3, partition 23, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:43,583 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 50.0 (TID 88) in 3207 ms on 10.201.8.108 (executor 3) (22/48)\n",
      "2023-01-25 09:22:45,568 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 50.0 (TID 92) (10.201.8.107, executor 1, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:45,568 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 50.0 (TID 89) in 3386 ms on 10.201.8.107 (executor 1) (23/48)\n",
      "2023-01-25 09:22:45,679 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 50.0 (TID 93) (10.200.140.66, executor 2, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:45,679 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 50.0 (TID 90) in 3392 ms on 10.200.140.66 (executor 2) (24/48)\n",
      "2023-01-25 09:22:45,970 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:46,863 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 50.0 (TID 94) (10.201.8.108, executor 3, partition 26, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:46,864 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 50.0 (TID 91) in 3282 ms on 10.201.8.108 (executor 3) (25/48)\n",
      "2023-01-25 09:22:47,163 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:48,983 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 50.0 (TID 95) (10.201.8.107, executor 1, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:48,983 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 50.0 (TID 92) in 3415 ms on 10.201.8.107 (executor 1) (26/48)\n",
      "2023-01-25 09:22:49,090 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 50.0 (TID 96) (10.200.140.66, executor 2, partition 29, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:49,091 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 50.0 (TID 93) in 3413 ms on 10.200.140.66 (executor 2) (27/48)\n",
      "2023-01-25 09:22:49,276 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:50,362 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 50.0 (TID 97) (10.201.8.108, executor 3, partition 30, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:50,363 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 50.0 (TID 94) in 3500 ms on 10.201.8.108 (executor 3) (28/48)\n",
      "2023-01-25 09:22:50,661 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:52,191 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 50.0 (TID 98) (10.200.140.66, executor 2, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:52,191 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 50.0 (TID 96) in 3101 ms on 10.200.140.66 (executor 2) (29/48)\n",
      "2023-01-25 09:22:52,279 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 50.0 (TID 99) (10.201.8.107, executor 1, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:52,279 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 50.0 (TID 95) in 3297 ms on 10.201.8.107 (executor 1) (30/48)\n",
      "2023-01-25 09:22:52,490 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:52,564 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:53,672 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 50.0 (TID 100) (10.201.8.108, executor 3, partition 32, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:53,672 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 50.0 (TID 97) in 3310 ms on 10.201.8.108 (executor 3) (31/48)\n",
      "2023-01-25 09:22:55,572 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 50.0 (TID 101) (10.200.140.66, executor 2, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:55,572 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 50.0 (TID 98) in 3382 ms on 10.200.140.66 (executor 2) (32/48)\n",
      "2023-01-25 09:22:55,574 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 50.0 (TID 102) (10.201.8.107, executor 1, partition 35, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:55,575 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 50.0 (TID 99) in 3296 ms on 10.201.8.107 (executor 1) (33/48)\n",
      "2023-01-25 09:22:55,983 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.200.140.66:42774\n",
      "2023-01-25 09:22:56,558 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 50.0 (TID 103) (10.201.8.108, executor 3, partition 36, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:56,558 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 50.0 (TID 100) in 2886 ms on 10.201.8.108 (executor 3) (34/48)\n",
      "2023-01-25 09:22:56,862 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.201.8.108:33356\n",
      "2023-01-25 09:22:58,864 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 50.0 (TID 104) (10.201.8.107, executor 1, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:58,864 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 50.0 (TID 102) in 3290 ms on 10.201.8.107 (executor 1) (35/48)\n",
      "2023-01-25 09:22:59,068 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.201.8.107:45122\n",
      "2023-01-25 09:22:59,280 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 50.0 (TID 105) (10.200.140.66, executor 2, partition 39, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:22:59,280 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 50.0 (TID 101) in 3708 ms on 10.200.140.66 (executor 2) (36/48)\n",
      "2023-01-25 09:22:59,580 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.200.140.66:42774\n",
      "2023-01-25 09:23:00,072 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 50.0 (TID 106) (10.201.8.108, executor 3, partition 38, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:00,072 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 50.0 (TID 103) in 3514 ms on 10.201.8.108 (executor 3) (37/48)\n",
      "2023-01-25 09:23:02,263 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 50.0 (TID 107) (10.201.8.107, executor 1, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:02,264 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 50.0 (TID 104) in 3401 ms on 10.201.8.107 (executor 1) (38/48)\n",
      "2023-01-25 09:23:02,478 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.201.8.107:45122\n",
      "2023-01-25 09:23:02,691 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 50.0 (TID 108) (10.200.140.66, executor 2, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:02,692 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 50.0 (TID 105) in 3413 ms on 10.200.140.66 (executor 2) (39/48)\n",
      "2023-01-25 09:23:02,883 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 50.0 (TID 109) (10.201.8.108, executor 3, partition 41, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:02,883 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 50.0 (TID 106) in 2811 ms on 10.201.8.108 (executor 3) (40/48)\n",
      "2023-01-25 09:23:03,167 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.201.8.108:33356\n",
      "2023-01-25 09:23:05,670 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 50.0 (TID 110) (10.201.8.107, executor 1, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:05,670 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 50.0 (TID 107) in 3407 ms on 10.201.8.107 (executor 1) (41/48)\n",
      "2023-01-25 09:23:05,883 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 50.0 (TID 111) (10.200.140.66, executor 2, partition 44, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:05,884 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 50.0 (TID 108) in 3193 ms on 10.200.140.66 (executor 2) (42/48)\n",
      "2023-01-25 09:23:06,174 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 50.0 (TID 112) (10.201.8.108, executor 3, partition 45, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:06,175 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 50.0 (TID 109) in 3292 ms on 10.201.8.108 (executor 3) (43/48)\n",
      "2023-01-25 09:23:06,184 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.200.140.66:42774\n",
      "2023-01-25 09:23:06,362 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.201.8.108:33356\n",
      "2023-01-25 09:23:09,062 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 50.0 (TID 113) (10.201.8.107, executor 1, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:09,063 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 50.0 (TID 110) in 3393 ms on 10.201.8.107 (executor 1) (44/48)\n",
      "2023-01-25 09:23:09,362 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.201.8.107:45122\n",
      "2023-01-25 09:23:09,469 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 50.0 (TID 114) (10.201.8.108, executor 3, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:09,469 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 50.0 (TID 112) in 3295 ms on 10.201.8.108 (executor 3) (45/48)\n",
      "2023-01-25 09:23:09,678 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 50.0 (TID 111) in 3794 ms on 10.200.140.66 (executor 2) (46/48)\n",
      "2023-01-25 09:23:12,471 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 50.0 (TID 113) in 3409 ms on 10.201.8.107 (executor 1) (47/48)\n",
      "2023-01-25 09:23:12,574 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 50.0 (TID 114) in 3105 ms on 10.201.8.108 (executor 3) (48/48)\n",
      "2023-01-25 09:23:12,574 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:23:12,575 INFO scheduler.DAGScheduler: ShuffleMapStage 50 (summary at NativeMethodAccessorImpl.java:0) finished in 68.523 s\n",
      "2023-01-25 09:23:12,575 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:23:12,575 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:23:12,575 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:23:12,575 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:23:12,669 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 497.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:12,669 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.201.8.107:34250 in memory (size: 497.3 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:23:12,669 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.200.140.66:38378 in memory (size: 497.3 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:23:12,669 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.201.8.108:46028 in memory (size: 497.3 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:23:12,673 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:12,674 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.200.140.66:38378 in memory (size: 71.1 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:23:12,674 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.201.8.107:34250 in memory (size: 71.1 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:23:12,873 INFO spark.SparkContext: Starting job: summary at NativeMethodAccessorImpl.java:0\n",
      "2023-01-25 09:23:12,876 INFO scheduler.DAGScheduler: Got job 35 (summary at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-01-25 09:23:12,876 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-01-25 09:23:12,876 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)\n",
      "2023-01-25 09:23:12,876 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:23:12,877 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (SQLExecutionRDD[325] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-01-25 09:23:12,998 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "2023-01-25 09:23:12,999 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 2.7 MiB, free 2.2 GiB)\n",
      "2023-01-25 09:23:13,006 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 511.2 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:23:13,006 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 511.2 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:13,006 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:23:13,007 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (SQLExecutionRDD[325] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-01-25 09:23:13,007 INFO scheduler.TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0\n",
      "2023-01-25 09:23:13,008 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 115) (10.201.8.107, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:23:13,067 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.201.8.107:34250 (size: 511.2 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:23:13,470 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.201.8.107:45122\n",
      "2023-01-25 09:23:25,063 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 115) in 12056 ms on 10.201.8.107 (executor 1) (1/1)\n",
      "2023-01-25 09:23:25,063 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:23:25,063 INFO scheduler.DAGScheduler: ResultStage 68 (summary at NativeMethodAccessorImpl.java:0) finished in 12.186 s\n",
      "2023-01-25 09:23:25,064 INFO scheduler.DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-25 09:23:25,064 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished\n",
      "2023-01-25 09:23:25,064 INFO scheduler.DAGScheduler: Job 35 finished: summary at NativeMethodAccessorImpl.java:0, took 12.190842 s\n",
      "2023-01-25 09:23:25,097 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2023-01-25 09:23:25,119 INFO codegen.CodeGenerator: Code generated in 11.637734 ms\n",
      "2023-01-25 09:23:25,974 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 511.2 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:25,976 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.201.8.107:34250 in memory (size: 511.2 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:23:58,410 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,410 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:23:58,410 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:23:58,421 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,421 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:23:58,427 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:23:58,439 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:23:58,443 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,443 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,456 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,456 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:23:58,457 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,463 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,467 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:23:58,469 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,473 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,474 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,474 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,483 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,483 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,486 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,561 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,561 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,561 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:23:58,565 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,565 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:23:58,566 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,571 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,578 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,580 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,580 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:23:58,660 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,661 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,661 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:23:58,663 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,664 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:23:58,665 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,696 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,696 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:23:58,697 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,712 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,712 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:23:58,712 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,761 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,761 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.200.140.66:38378 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,761 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:23:58,764 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:23:58,765 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.201.8.107:34250 in memory (size: 54.8 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:23:58,765 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.201.8.108:46028 in memory (size: 54.8 KiB, free: 1007.4 MiB)\n",
      "2023-01-25 09:25:55,416 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,416 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-01-25 09:25:55,417 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,427 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,427 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2743 as timestamp))\n",
      "2023-01-25 09:25:55,427 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,437 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,437 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2833 as timestamp))\n",
      "2023-01-25 09:25:55,437 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,447 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,447 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2925 as timestamp))\n",
      "2023-01-25 09:25:55,447 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,458 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,458 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3019 as timestamp))\n",
      "2023-01-25 09:25:55,458 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,468 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,468 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3115 as timestamp))\n",
      "2023-01-25 09:25:55,469 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,479 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,479 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3213 as timestamp))\n",
      "2023-01-25 09:25:55,479 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,489 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,489 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3313 as timestamp))\n",
      "2023-01-25 09:25:55,489 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,498 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,498 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3415 as timestamp))\n",
      "2023-01-25 09:25:55,498 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,513 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,513 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3519 as timestamp))\n",
      "2023-01-25 09:25:55,514 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,523 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,523 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3625 as timestamp))\n",
      "2023-01-25 09:25:55,523 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,533 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,533 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3733 as timestamp))\n",
      "2023-01-25 09:25:55,533 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,544 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,544 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3843 as timestamp))\n",
      "2023-01-25 09:25:55,544 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,554 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,554 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3955 as timestamp))\n",
      "2023-01-25 09:25:55,555 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,564 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,565 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4069 as timestamp))\n",
      "2023-01-25 09:25:55,565 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:55,575 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:25:55,575 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4185 as timestamp))\n",
      "2023-01-25 09:25:55,575 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 8 more fields>\n",
      "2023-01-25 09:25:57,100 INFO codegen.CodeGenerator: Code generated in 108.514297 ms\n",
      "2023-01-25 09:25:57,102 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,117 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,118 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:25:57,118 INFO spark.SparkContext: Created broadcast 73 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:25:57,119 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:25:57,160 INFO scheduler.DAGScheduler: Registering RDD 329 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 33\n",
      "2023-01-25 09:25:57,160 INFO scheduler.DAGScheduler: Got map stage job 36 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-01-25 09:25:57,160 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:25:57,160 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:25:57,160 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:25:57,161 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[329] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:25:57,165 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 247.4 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,167 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 71.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,167 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:25:57,168 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:25:57,168 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[329] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:25:57,168 INFO scheduler.TaskSchedulerImpl: Adding task set 69.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:25:57,169 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 116) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:25:57,169 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 69.0 (TID 117) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:25:57,179 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.200.140.66:38378 (size: 71.1 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:25:57,179 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.201.8.108:46028 (size: 71.1 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:25:57,358 INFO codegen.CodeGenerator: Code generated in 105.64594 ms\n",
      "2023-01-25 09:25:57,360 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,375 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,376 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:25:57,376 INFO spark.SparkContext: Created broadcast 75 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:25:57,377 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:25:57,387 INFO scheduler.DAGScheduler: Registering RDD 333 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 34\n",
      "2023-01-25 09:25:57,388 INFO scheduler.DAGScheduler: Got map stage job 37 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-01-25 09:25:57,388 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 70 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:25:57,388 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:25:57,388 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:25:57,388 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 70 (MapPartitionsRDD[333] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:25:57,392 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 277.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,394 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 78.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:25:57,400 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 78.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:25:57,400 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:25:57,400 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[333] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:25:57,400 INFO scheduler.TaskSchedulerImpl: Adding task set 70.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:25:57,401 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 70.0 (TID 118) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:25:57,418 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.201.8.107:34250 (size: 78.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:25:57,962 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:25:58,068 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:25:58,167 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:26:00,895 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 70.0 (TID 119) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:00,895 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 69.0 (TID 117) in 3726 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:26:00,974 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.200.140.66:38378 (size: 78.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:26:01,478 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:26:02,463 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 70.0 (TID 118) in 5062 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:26:02,580 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 116) in 5411 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:26:02,580 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:26:02,581 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 5.419 s\n",
      "2023-01-25 09:26:02,581 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:26:02,581 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 70)\n",
      "2023-01-25 09:26:02,581 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:26:02,581 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:26:04,081 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 70.0 (TID 119) in 3187 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:26:04,081 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:26:04,082 INFO scheduler.DAGScheduler: ShuffleMapStage 70 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 6.694 s\n",
      "2023-01-25 09:26:04,082 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:26:04,082 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:26:04,082 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:26:04,082 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:26:04,321 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 78.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:26:04,323 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.201.8.107:34250 in memory (size: 78.0 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:26:04,323 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.200.140.66:38378 in memory (size: 78.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:26:04,332 INFO adaptive.ShufflePartitionsUtil: For shuffle(33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34), advisory target size: 67108864, actual target size 2965498, minimum partition size: 1048576\n",
      "2023-01-25 09:26:04,875 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:04,932 INFO codegen.CodeGenerator: Code generated in 33.832899 ms\n",
      "2023-01-25 09:26:04,951 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,008 INFO codegen.CodeGenerator: Code generated in 41.337777 ms\n",
      "2023-01-25 09:26:05,021 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,069 INFO codegen.CodeGenerator: Code generated in 33.092767 ms\n",
      "2023-01-25 09:26:05,082 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,130 INFO codegen.CodeGenerator: Code generated in 32.914681 ms\n",
      "2023-01-25 09:26:05,140 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,189 INFO codegen.CodeGenerator: Code generated in 33.671392 ms\n",
      "2023-01-25 09:26:05,198 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,246 INFO codegen.CodeGenerator: Code generated in 32.661666 ms\n",
      "2023-01-25 09:26:05,256 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,302 INFO codegen.CodeGenerator: Code generated in 32.13921 ms\n",
      "2023-01-25 09:26:05,312 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,358 INFO codegen.CodeGenerator: Code generated in 31.654788 ms\n",
      "2023-01-25 09:26:05,366 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,413 INFO codegen.CodeGenerator: Code generated in 32.05544 ms\n",
      "2023-01-25 09:26:05,422 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,469 INFO codegen.CodeGenerator: Code generated in 25.347296 ms\n",
      "2023-01-25 09:26:05,477 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,516 INFO codegen.CodeGenerator: Code generated in 25.163641 ms\n",
      "2023-01-25 09:26:05,525 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,564 INFO codegen.CodeGenerator: Code generated in 25.159154 ms\n",
      "2023-01-25 09:26:05,582 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,620 INFO codegen.CodeGenerator: Code generated in 24.80678 ms\n",
      "2023-01-25 09:26:05,629 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,667 INFO codegen.CodeGenerator: Code generated in 24.850965 ms\n",
      "2023-01-25 09:26:05,676 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,719 INFO codegen.CodeGenerator: Code generated in 26.118902 ms\n",
      "2023-01-25 09:26:05,728 INFO aggregate.HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "2023-01-25 09:26:05,766 INFO codegen.CodeGenerator: Code generated in 24.09849 ms\n",
      "2023-01-25 09:26:05,871 INFO scheduler.DAGScheduler: Registering RDD 384 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 35\n",
      "2023-01-25 09:26:05,871 INFO scheduler.DAGScheduler: Got map stage job 38 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 48 output partitions\n",
      "2023-01-25 09:26:05,871 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 73 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:26:05,871 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 71, ShuffleMapStage 72)\n",
      "2023-01-25 09:26:05,871 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:26:05,872 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 73 (MapPartitionsRDD[384] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:26:05,964 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "2023-01-25 09:26:05,964 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 2.4 MiB, free 2.2 GiB)\n",
      "2023-01-25 09:26:05,972 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 650.7 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:26:05,972 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 650.7 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:26:05,973 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:26:05,974 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 73 (MapPartitionsRDD[384] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-01-25 09:26:05,974 INFO scheduler.TaskSchedulerImpl: Adding task set 73.0 with 48 tasks resource profile 0\n",
      "2023-01-25 09:26:05,975 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 73.0 (TID 120) (10.201.8.107, executor 1, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:05,975 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 121) (10.201.8.108, executor 3, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:05,975 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 73.0 (TID 122) (10.200.140.66, executor 2, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:05,990 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.200.140.66:38378 (size: 650.7 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:26:05,992 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.201.8.107:34250 (size: 650.7 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:26:05,994 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.201.8.108:46028 (size: 650.7 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:26:06,271 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.201.8.108:33356\n",
      "2023-01-25 09:26:06,362 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.201.8.107:45122\n",
      "2023-01-25 09:26:06,368 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.200.140.66:42774\n",
      "2023-01-25 09:26:12,158 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 73.0 (TID 123) (10.201.8.107, executor 1, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:12,158 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 73.0 (TID 120) in 6184 ms on 10.201.8.107 (executor 1) (1/48)\n",
      "2023-01-25 09:26:13,591 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 73.0 (TID 124) (10.200.140.66, executor 2, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:13,592 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 73.0 (TID 122) in 7617 ms on 10.200.140.66 (executor 2) (2/48)\n",
      "2023-01-25 09:26:14,966 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 73.0 (TID 125) (10.201.8.107, executor 1, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:14,966 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 73.0 (TID 123) in 2808 ms on 10.201.8.107 (executor 1) (3/48)\n",
      "2023-01-25 09:26:14,967 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 121) in 8992 ms on 10.201.8.108 (executor 3) (4/48)\n",
      "2023-01-25 09:26:17,876 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 73.0 (TID 126) (10.200.140.66, executor 2, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:17,877 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 73.0 (TID 124) in 4285 ms on 10.200.140.66 (executor 2) (5/48)\n",
      "2023-01-25 09:26:17,978 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 73.0 (TID 127) (10.201.8.107, executor 1, partition 7, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:17,979 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 73.0 (TID 125) in 3013 ms on 10.201.8.107 (executor 1) (6/48)\n",
      "2023-01-25 09:26:18,071 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.200.140.66:42774\n",
      "2023-01-25 09:26:18,362 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 73.0 (TID 128) (10.201.8.108, executor 3, partition 8, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:18,462 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.201.8.108:33356\n",
      "2023-01-25 09:26:20,371 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 73.0 (TID 129) (10.200.140.66, executor 2, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:20,371 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 73.0 (TID 126) in 2495 ms on 10.200.140.66 (executor 2) (7/48)\n",
      "2023-01-25 09:26:20,858 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 73.0 (TID 130) (10.201.8.107, executor 1, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:20,858 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 73.0 (TID 127) in 2880 ms on 10.201.8.107 (executor 1) (8/48)\n",
      "2023-01-25 09:26:21,058 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 73.0 (TID 128) in 2696 ms on 10.201.8.108 (executor 3) (9/48)\n",
      "2023-01-25 09:26:22,777 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 73.0 (TID 131) (10.200.140.66, executor 2, partition 11, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:22,778 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 73.0 (TID 129) in 2408 ms on 10.200.140.66 (executor 2) (10/48)\n",
      "2023-01-25 09:26:23,480 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 73.0 (TID 132) (10.201.8.107, executor 1, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:23,480 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 73.0 (TID 130) in 2622 ms on 10.201.8.107 (executor 1) (11/48)\n",
      "2023-01-25 09:26:24,362 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 73.0 (TID 133) (10.201.8.108, executor 3, partition 13, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:25,278 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 73.0 (TID 134) (10.200.140.66, executor 2, partition 14, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:25,278 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 73.0 (TID 131) in 2501 ms on 10.200.140.66 (executor 2) (12/48)\n",
      "2023-01-25 09:26:26,371 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 73.0 (TID 135) (10.201.8.107, executor 1, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:26,372 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 73.0 (TID 132) in 2892 ms on 10.201.8.107 (executor 1) (13/48)\n",
      "2023-01-25 09:26:27,377 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 73.0 (TID 133) in 3015 ms on 10.201.8.108 (executor 3) (14/48)\n",
      "2023-01-25 09:26:28,079 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 73.0 (TID 136) (10.200.140.66, executor 2, partition 16, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:28,079 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 73.0 (TID 134) in 2802 ms on 10.200.140.66 (executor 2) (15/48)\n",
      "2023-01-25 09:26:29,283 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 73.0 (TID 137) (10.201.8.107, executor 1, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:29,283 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 73.0 (TID 135) in 2912 ms on 10.201.8.107 (executor 1) (16/48)\n",
      "2023-01-25 09:26:30,362 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 73.0 (TID 138) (10.201.8.108, executor 3, partition 18, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:30,793 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 73.0 (TID 139) (10.200.140.66, executor 2, partition 19, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:30,793 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 73.0 (TID 136) in 2714 ms on 10.200.140.66 (executor 2) (17/48)\n",
      "2023-01-25 09:26:31,972 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 73.0 (TID 140) (10.201.8.107, executor 1, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:31,973 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 73.0 (TID 137) in 2689 ms on 10.201.8.107 (executor 1) (18/48)\n",
      "2023-01-25 09:26:32,662 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 73.0 (TID 138) in 2300 ms on 10.201.8.108 (executor 3) (19/48)\n",
      "2023-01-25 09:26:33,186 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 73.0 (TID 141) (10.200.140.66, executor 2, partition 21, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:33,186 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 73.0 (TID 139) in 2393 ms on 10.200.140.66 (executor 2) (20/48)\n",
      "2023-01-25 09:26:34,681 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 73.0 (TID 142) (10.201.8.107, executor 1, partition 22, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:34,681 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 73.0 (TID 140) in 2709 ms on 10.201.8.107 (executor 1) (21/48)\n",
      "2023-01-25 09:26:35,362 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 73.0 (TID 143) (10.201.8.108, executor 3, partition 23, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:35,482 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 73.0 (TID 144) (10.200.140.66, executor 2, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:35,483 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 73.0 (TID 141) in 2298 ms on 10.200.140.66 (executor 2) (22/48)\n",
      "2023-01-25 09:26:37,566 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 73.0 (TID 145) (10.201.8.107, executor 1, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:37,567 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 73.0 (TID 142) in 2886 ms on 10.201.8.107 (executor 1) (23/48)\n",
      "2023-01-25 09:26:37,759 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 73.0 (TID 143) in 2397 ms on 10.201.8.108 (executor 3) (24/48)\n",
      "2023-01-25 09:26:37,785 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 73.0 (TID 146) (10.200.140.66, executor 2, partition 26, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:37,785 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 73.0 (TID 144) in 2303 ms on 10.200.140.66 (executor 2) (25/48)\n",
      "2023-01-25 09:26:39,788 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 73.0 (TID 147) (10.200.140.66, executor 2, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:39,788 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 73.0 (TID 146) in 2003 ms on 10.200.140.66 (executor 2) (26/48)\n",
      "2023-01-25 09:26:40,263 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 73.0 (TID 148) (10.201.8.107, executor 1, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:40,263 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 73.0 (TID 145) in 2697 ms on 10.201.8.107 (executor 1) (27/48)\n",
      "2023-01-25 09:26:41,362 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 73.0 (TID 149) (10.201.8.108, executor 3, partition 29, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:41,881 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 73.0 (TID 150) (10.200.140.66, executor 2, partition 30, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:41,881 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 73.0 (TID 147) in 2093 ms on 10.200.140.66 (executor 2) (28/48)\n",
      "2023-01-25 09:26:42,571 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 73.0 (TID 151) (10.201.8.107, executor 1, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:42,571 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 73.0 (TID 148) in 2309 ms on 10.201.8.107 (executor 1) (29/48)\n",
      "2023-01-25 09:26:43,662 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 73.0 (TID 149) in 2300 ms on 10.201.8.108 (executor 3) (30/48)\n",
      "2023-01-25 09:26:44,077 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 73.0 (TID 152) (10.200.140.66, executor 2, partition 32, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:44,077 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 73.0 (TID 150) in 2196 ms on 10.200.140.66 (executor 2) (31/48)\n",
      "2023-01-25 09:26:44,862 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 73.0 (TID 153) (10.201.8.107, executor 1, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:44,862 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 73.0 (TID 151) in 2291 ms on 10.201.8.107 (executor 1) (32/48)\n",
      "2023-01-25 09:26:46,277 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 73.0 (TID 154) (10.200.140.66, executor 2, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:46,277 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 73.0 (TID 152) in 2200 ms on 10.200.140.66 (executor 2) (33/48)\n",
      "2023-01-25 09:26:46,362 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 73.0 (TID 155) (10.201.8.108, executor 3, partition 35, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:47,071 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 73.0 (TID 156) (10.201.8.107, executor 1, partition 36, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:47,071 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 73.0 (TID 153) in 2209 ms on 10.201.8.107 (executor 1) (34/48)\n",
      "2023-01-25 09:26:48,470 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 73.0 (TID 155) in 2109 ms on 10.201.8.108 (executor 3) (35/48)\n",
      "2023-01-25 09:26:48,685 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 73.0 (TID 157) (10.200.140.66, executor 2, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:48,685 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 73.0 (TID 154) in 2408 ms on 10.200.140.66 (executor 2) (36/48)\n",
      "2023-01-25 09:26:49,379 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 73.0 (TID 158) (10.201.8.107, executor 1, partition 38, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:49,380 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 73.0 (TID 156) in 2308 ms on 10.201.8.107 (executor 1) (37/48)\n",
      "2023-01-25 09:26:50,363 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 73.0 (TID 159) (10.201.8.108, executor 3, partition 39, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:50,970 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 73.0 (TID 160) (10.200.140.66, executor 2, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:50,970 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 73.0 (TID 157) in 2285 ms on 10.200.140.66 (executor 2) (38/48)\n",
      "2023-01-25 09:26:51,165 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 73.0 (TID 161) (10.201.8.107, executor 1, partition 41, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:51,166 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 73.0 (TID 158) in 1786 ms on 10.201.8.107 (executor 1) (39/48)\n",
      "2023-01-25 09:26:52,277 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 73.0 (TID 159) in 1914 ms on 10.201.8.108 (executor 3) (40/48)\n",
      "2023-01-25 09:26:52,879 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 73.0 (TID 162) (10.200.140.66, executor 2, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:52,880 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 73.0 (TID 160) in 1912 ms on 10.200.140.66 (executor 2) (41/48)\n",
      "2023-01-25 09:26:53,182 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 73.0 (TID 163) (10.201.8.107, executor 1, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:53,183 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 73.0 (TID 161) in 2020 ms on 10.201.8.107 (executor 1) (42/48)\n",
      "2023-01-25 09:26:54,362 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 73.0 (TID 164) (10.201.8.108, executor 3, partition 44, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:54,984 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 73.0 (TID 165) (10.201.8.107, executor 1, partition 45, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:54,985 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 73.0 (TID 163) in 1803 ms on 10.201.8.107 (executor 1) (43/48)\n",
      "2023-01-25 09:26:55,085 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 73.0 (TID 166) (10.200.140.66, executor 2, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:55,086 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 73.0 (TID 162) in 2207 ms on 10.200.140.66 (executor 2) (44/48)\n",
      "2023-01-25 09:26:56,558 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 73.0 (TID 164) in 2196 ms on 10.201.8.108 (executor 3) (45/48)\n",
      "2023-01-25 09:26:56,962 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 73.0 (TID 167) (10.201.8.107, executor 1, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:56,962 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 73.0 (TID 165) in 1978 ms on 10.201.8.107 (executor 1) (46/48)\n",
      "2023-01-25 09:26:57,380 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 73.0 (TID 166) in 2295 ms on 10.200.140.66 (executor 2) (47/48)\n",
      "2023-01-25 09:26:58,769 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 73.0 (TID 167) in 1807 ms on 10.201.8.107 (executor 1) (48/48)\n",
      "2023-01-25 09:26:58,769 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:26:58,770 INFO scheduler.DAGScheduler: ShuffleMapStage 73 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 52.874 s\n",
      "2023-01-25 09:26:58,770 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:26:58,770 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:26:58,770 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:26:58,770 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:26:58,851 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 650.7 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:26:58,851 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.200.140.66:38378 in memory (size: 650.7 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:26:58,852 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.201.8.108:46028 in memory (size: 650.7 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:26:58,857 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.201.8.107:34250 in memory (size: 650.7 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:26:59,620 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:26:59,622 INFO scheduler.DAGScheduler: Got job 39 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 1 output partitions\n",
      "2023-01-25 09:26:59,622 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:26:59,622 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 76)\n",
      "2023-01-25 09:26:59,622 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:26:59,622 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[387] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:26:59,742 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "2023-01-25 09:26:59,742 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 2.7 MiB, free 2.2 GiB)\n",
      "2023-01-25 09:26:59,749 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 594.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:26:59,750 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 594.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:26:59,750 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:26:59,750 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[387] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0))\n",
      "2023-01-25 09:26:59,750 INFO scheduler.TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "2023-01-25 09:26:59,751 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 168) (10.201.8.107, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:26:59,765 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.201.8.107:34250 (size: 594.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:26:59,977 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.201.8.107:45122\n",
      "2023-01-25 09:27:02,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 168) in 3132 ms on 10.201.8.107 (executor 1) (1/1)\n",
      "2023-01-25 09:27:02,883 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:27:02,883 INFO scheduler.DAGScheduler: ResultStage 77 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 3.260 s\n",
      "2023-01-25 09:27:02,883 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-25 09:27:02,883 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "2023-01-25 09:27:02,884 INFO scheduler.DAGScheduler: Job 39 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104, took 3.263162 s\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2023-01-25 09:27:03,741 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 594.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:27:03,743 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.201.8.107:34250 in memory (size: 594.9 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:27:48,933 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 71.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:27:48,934 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.200.140.66:38378 in memory (size: 71.1 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:27:48,935 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.201.8.108:46028 in memory (size: 71.1 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:28:30,638 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,639 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-01-25 09:28:30,639 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,643 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,643 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2743 as timestamp))\n",
      "2023-01-25 09:28:30,643 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,648 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,648 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2833 as timestamp))\n",
      "2023-01-25 09:28:30,648 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,654 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,654 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2925 as timestamp))\n",
      "2023-01-25 09:28:30,654 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,660 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,660 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3019 as timestamp))\n",
      "2023-01-25 09:28:30,660 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,666 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,666 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3115 as timestamp))\n",
      "2023-01-25 09:28:30,666 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,673 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,673 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3213 as timestamp))\n",
      "2023-01-25 09:28:30,673 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,680 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,680 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3313 as timestamp))\n",
      "2023-01-25 09:28:30,680 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,688 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,688 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3415 as timestamp))\n",
      "2023-01-25 09:28:30,688 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,697 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,697 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3519 as timestamp))\n",
      "2023-01-25 09:28:30,697 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,705 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,705 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3625 as timestamp))\n",
      "2023-01-25 09:28:30,706 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,714 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,714 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3733 as timestamp))\n",
      "2023-01-25 09:28:30,715 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,724 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,724 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3843 as timestamp))\n",
      "2023-01-25 09:28:30,725 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,734 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,734 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3955 as timestamp))\n",
      "2023-01-25 09:28:30,734 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,744 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,744 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4069 as timestamp))\n",
      "2023-01-25 09:28:30,744 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:30,755 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:28:30,755 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4185 as timestamp))\n",
      "2023-01-25 09:28:30,755 INFO datasources.FileSourceStrategy: Output Data Schema: struct<amount: double, timestamp: string, source: string ... 1 more fields>\n",
      "2023-01-25 09:28:31,818 INFO codegen.CodeGenerator: Code generated in 63.296276 ms\n",
      "2023-01-25 09:28:31,820 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:31,836 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:31,836 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:31,837 INFO spark.SparkContext: Created broadcast 79 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:28:31,837 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Registering RDD 391 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 36\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Got map stage job 40 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:28:31,870 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[391] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:28:31,874 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 201.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:31,876 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 61.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:31,877 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 61.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:31,878 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:28:31,878 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[391] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:28:31,878 INFO scheduler.TaskSchedulerImpl: Adding task set 78.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:28:31,879 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 169) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:31,879 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 78.0 (TID 170) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:31,889 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.201.8.108:46028 (size: 61.8 KiB, free: 1007.3 MiB)\n",
      "2023-01-25 09:28:31,891 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.200.140.66:38378 (size: 61.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:28:32,038 INFO codegen.CodeGenerator: Code generated in 65.983013 ms\n",
      "2023-01-25 09:28:32,040 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:32,056 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:32,056 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:32,057 INFO spark.SparkContext: Created broadcast 81 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:28:32,057 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Registering RDD 395 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 37\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Got map stage job 41 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 79 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:28:32,067 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[395] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:28:32,071 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 228.7 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:32,073 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 68.6 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:32,073 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 68.6 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:32,073 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:28:32,074 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[395] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:28:32,074 INFO scheduler.TaskSchedulerImpl: Adding task set 79.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:28:32,074 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 79.0 (TID 171) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:32,083 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.201.8.107:34250 (size: 68.6 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:28:32,263 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:28:32,275 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:28:32,567 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:28:33,961 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 79.0 (TID 172) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:33,961 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 78.0 (TID 170) in 2082 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:28:33,970 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.201.8.108:46028 (size: 68.6 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:28:34,275 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 169) in 2396 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:28:34,275 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:28:34,276 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 2.405 s\n",
      "2023-01-25 09:28:34,276 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:28:34,276 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 79)\n",
      "2023-01-25 09:28:34,276 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:28:34,276 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:28:34,367 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:28:34,568 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 79.0 (TID 171) in 2494 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:28:36,058 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 79.0 (TID 172) in 2097 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:28:36,058 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:28:36,059 INFO scheduler.DAGScheduler: ShuffleMapStage 79 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 3.991 s\n",
      "2023-01-25 09:28:36,059 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:28:36,059 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:28:36,059 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:28:36,059 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:28:36,113 INFO adaptive.ShufflePartitionsUtil: For shuffle(36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37), advisory target size: 67108864, actual target size 2088108, minimum partition size: 1048576\n",
      "2023-01-25 09:28:36,382 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 61.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:36,384 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.201.8.108:46028 in memory (size: 61.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:28:36,384 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.200.140.66:38378 in memory (size: 61.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:28:36,387 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 68.6 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:36,389 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.201.8.107:34250 in memory (size: 68.6 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:28:36,389 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.201.8.108:46028 in memory (size: 68.6 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:28:36,808 INFO scheduler.DAGScheduler: Registering RDD 430 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 38\n",
      "2023-01-25 09:28:36,809 INFO scheduler.DAGScheduler: Got map stage job 42 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 48 output partitions\n",
      "2023-01-25 09:28:36,809 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 82 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:28:36,809 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81, ShuffleMapStage 80)\n",
      "2023-01-25 09:28:36,809 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:28:36,809 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 82 (MapPartitionsRDD[430] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:28:36,889 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1243.7 KiB\n",
      "2023-01-25 09:28:36,889 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 1243.7 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:36,893 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 286.6 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:28:36,893 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 286.6 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:28:36,894 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:28:36,895 INFO scheduler.DAGScheduler: Submitting 48 missing tasks from ShuffleMapStage 82 (MapPartitionsRDD[430] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "2023-01-25 09:28:36,895 INFO scheduler.TaskSchedulerImpl: Adding task set 82.0 with 48 tasks resource profile 0\n",
      "2023-01-25 09:28:36,896 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 173) (10.201.8.108, executor 3, partition 0, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:36,896 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 82.0 (TID 174) (10.200.140.66, executor 2, partition 1, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:36,896 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 82.0 (TID 175) (10.201.8.107, executor 1, partition 3, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:36,908 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.201.8.108:46028 (size: 286.6 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:28:36,908 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.201.8.107:34250 (size: 286.6 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:28:36,910 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.200.140.66:38378 (size: 286.6 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:28:36,962 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.201.8.108:33356\n",
      "2023-01-25 09:28:36,967 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.201.8.107:45122\n",
      "2023-01-25 09:28:37,076 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.200.140.66:42774\n",
      "2023-01-25 09:28:41,164 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 82.0 (TID 176) (10.201.8.107, executor 1, partition 4, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:41,165 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 82.0 (TID 175) in 4269 ms on 10.201.8.107 (executor 1) (1/48)\n",
      "2023-01-25 09:28:41,478 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 82.0 (TID 177) (10.200.140.66, executor 2, partition 2, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:41,478 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 82.0 (TID 174) in 4582 ms on 10.200.140.66 (executor 2) (2/48)\n",
      "2023-01-25 09:28:41,980 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 82.0 (TID 178) (10.201.8.108, executor 3, partition 5, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:41,980 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 173) in 5084 ms on 10.201.8.108 (executor 3) (3/48)\n",
      "2023-01-25 09:28:42,170 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.201.8.108:33356\n",
      "2023-01-25 09:28:42,563 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 82.0 (TID 179) (10.201.8.107, executor 1, partition 6, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:42,563 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 82.0 (TID 176) in 1399 ms on 10.201.8.107 (executor 1) (4/48)\n",
      "2023-01-25 09:28:44,158 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 82.0 (TID 180) (10.201.8.108, executor 3, partition 7, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:44,158 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 82.0 (TID 178) in 2179 ms on 10.201.8.108 (executor 3) (5/48)\n",
      "2023-01-25 09:28:44,675 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 82.0 (TID 181) (10.201.8.107, executor 1, partition 8, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:44,675 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 82.0 (TID 179) in 2112 ms on 10.201.8.107 (executor 1) (6/48)\n",
      "2023-01-25 09:28:44,688 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 82.0 (TID 177) in 3210 ms on 10.200.140.66 (executor 2) (7/48)\n",
      "2023-01-25 09:28:46,160 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 82.0 (TID 182) (10.201.8.108, executor 3, partition 9, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:46,160 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 82.0 (TID 180) in 2002 ms on 10.201.8.108 (executor 3) (8/48)\n",
      "2023-01-25 09:28:46,668 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 82.0 (TID 183) (10.201.8.107, executor 1, partition 10, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:46,668 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 82.0 (TID 181) in 1993 ms on 10.201.8.107 (executor 1) (9/48)\n",
      "2023-01-25 09:28:48,362 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 82.0 (TID 184) (10.200.140.66, executor 2, partition 11, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:48,476 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 82.0 (TID 185) (10.201.8.108, executor 3, partition 12, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:48,476 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 82.0 (TID 182) in 2316 ms on 10.201.8.108 (executor 3) (10/48)\n",
      "2023-01-25 09:28:48,485 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.200.140.66:42774\n",
      "2023-01-25 09:28:49,079 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 82.0 (TID 186) (10.201.8.107, executor 1, partition 13, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:49,080 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 82.0 (TID 183) in 2412 ms on 10.201.8.107 (executor 1) (11/48)\n",
      "2023-01-25 09:28:50,370 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 82.0 (TID 184) in 2008 ms on 10.200.140.66 (executor 2) (12/48)\n",
      "2023-01-25 09:28:50,782 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 82.0 (TID 187) (10.201.8.108, executor 3, partition 14, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:50,783 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 82.0 (TID 185) in 2307 ms on 10.201.8.108 (executor 3) (13/48)\n",
      "2023-01-25 09:28:50,973 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 82.0 (TID 188) (10.201.8.107, executor 1, partition 15, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:50,974 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 82.0 (TID 186) in 1895 ms on 10.201.8.107 (executor 1) (14/48)\n",
      "2023-01-25 09:28:52,362 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 82.0 (TID 189) (10.200.140.66, executor 2, partition 16, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:52,457 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 82.0 (TID 190) (10.201.8.108, executor 3, partition 17, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:52,457 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 82.0 (TID 187) in 1675 ms on 10.201.8.108 (executor 3) (15/48)\n",
      "2023-01-25 09:28:53,173 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 82.0 (TID 191) (10.201.8.107, executor 1, partition 18, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:53,173 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 82.0 (TID 188) in 2200 ms on 10.201.8.107 (executor 1) (16/48)\n",
      "2023-01-25 09:28:54,287 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 82.0 (TID 189) in 1925 ms on 10.200.140.66 (executor 2) (17/48)\n",
      "2023-01-25 09:28:54,559 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 82.0 (TID 192) (10.201.8.108, executor 3, partition 19, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:54,559 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 82.0 (TID 190) in 2102 ms on 10.201.8.108 (executor 3) (18/48)\n",
      "2023-01-25 09:28:55,477 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 82.0 (TID 193) (10.201.8.107, executor 1, partition 20, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:55,478 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 82.0 (TID 191) in 2305 ms on 10.201.8.107 (executor 1) (19/48)\n",
      "2023-01-25 09:28:56,362 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 82.0 (TID 194) (10.200.140.66, executor 2, partition 21, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:56,474 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 82.0 (TID 195) (10.201.8.108, executor 3, partition 22, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:56,474 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 82.0 (TID 192) in 1915 ms on 10.201.8.108 (executor 3) (20/48)\n",
      "2023-01-25 09:28:57,068 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 82.0 (TID 196) (10.201.8.107, executor 1, partition 23, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:57,068 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 82.0 (TID 193) in 1591 ms on 10.201.8.107 (executor 1) (21/48)\n",
      "2023-01-25 09:28:58,572 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 82.0 (TID 197) (10.201.8.108, executor 3, partition 24, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:58,573 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 82.0 (TID 195) in 2099 ms on 10.201.8.108 (executor 3) (22/48)\n",
      "2023-01-25 09:28:58,577 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 82.0 (TID 194) in 2215 ms on 10.200.140.66 (executor 2) (23/48)\n",
      "2023-01-25 09:28:58,972 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 82.0 (TID 198) (10.201.8.107, executor 1, partition 25, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:28:58,972 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 82.0 (TID 196) in 1904 ms on 10.201.8.107 (executor 1) (24/48)\n",
      "2023-01-25 09:29:00,777 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 82.0 (TID 199) (10.201.8.108, executor 3, partition 26, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:00,777 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 82.0 (TID 197) in 2205 ms on 10.201.8.108 (executor 3) (25/48)\n",
      "2023-01-25 09:29:01,064 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 82.0 (TID 200) (10.201.8.107, executor 1, partition 27, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:01,064 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 82.0 (TID 198) in 2092 ms on 10.201.8.107 (executor 1) (26/48)\n",
      "2023-01-25 09:29:02,167 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 82.0 (TID 201) (10.201.8.108, executor 3, partition 28, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:02,168 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 82.0 (TID 199) in 1392 ms on 10.201.8.108 (executor 3) (27/48)\n",
      "2023-01-25 09:29:02,362 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 82.0 (TID 202) (10.200.140.66, executor 2, partition 29, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:02,859 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 82.0 (TID 203) (10.201.8.107, executor 1, partition 30, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:02,859 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 82.0 (TID 200) in 1795 ms on 10.201.8.107 (executor 1) (28/48)\n",
      "2023-01-25 09:29:03,967 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 82.0 (TID 204) (10.201.8.108, executor 3, partition 31, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:03,968 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 82.0 (TID 201) in 1801 ms on 10.201.8.108 (executor 3) (29/48)\n",
      "2023-01-25 09:29:04,370 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 82.0 (TID 202) in 2008 ms on 10.200.140.66 (executor 2) (30/48)\n",
      "2023-01-25 09:29:04,670 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 82.0 (TID 205) (10.201.8.107, executor 1, partition 32, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:04,671 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 82.0 (TID 203) in 1812 ms on 10.201.8.107 (executor 1) (31/48)\n",
      "2023-01-25 09:29:05,668 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 82.0 (TID 206) (10.201.8.108, executor 3, partition 33, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:05,669 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 82.0 (TID 204) in 1702 ms on 10.201.8.108 (executor 3) (32/48)\n",
      "2023-01-25 09:29:05,979 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 82.0 (TID 207) (10.201.8.107, executor 1, partition 34, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:05,979 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 82.0 (TID 205) in 1309 ms on 10.201.8.107 (executor 1) (33/48)\n",
      "2023-01-25 09:29:07,362 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 82.0 (TID 208) (10.200.140.66, executor 2, partition 35, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:07,569 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 82.0 (TID 209) (10.201.8.108, executor 3, partition 36, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:07,569 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 82.0 (TID 206) in 1901 ms on 10.201.8.108 (executor 3) (34/48)\n",
      "2023-01-25 09:29:08,072 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 82.0 (TID 210) (10.201.8.107, executor 1, partition 37, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:08,073 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 82.0 (TID 207) in 2094 ms on 10.201.8.107 (executor 1) (35/48)\n",
      "2023-01-25 09:29:09,168 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 82.0 (TID 208) in 1807 ms on 10.200.140.66 (executor 2) (36/48)\n",
      "2023-01-25 09:29:09,278 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 82.0 (TID 211) (10.201.8.108, executor 3, partition 38, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:09,278 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 82.0 (TID 209) in 1710 ms on 10.201.8.108 (executor 3) (37/48)\n",
      "2023-01-25 09:29:09,763 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 82.0 (TID 212) (10.201.8.107, executor 1, partition 39, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:09,764 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 82.0 (TID 210) in 1692 ms on 10.201.8.107 (executor 1) (38/48)\n",
      "2023-01-25 09:29:10,771 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 82.0 (TID 213) (10.201.8.108, executor 3, partition 40, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:10,771 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 82.0 (TID 211) in 1494 ms on 10.201.8.108 (executor 3) (39/48)\n",
      "2023-01-25 09:29:11,362 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 82.0 (TID 214) (10.200.140.66, executor 2, partition 41, ANY, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:11,577 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 82.0 (TID 215) (10.201.8.107, executor 1, partition 42, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:11,578 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 82.0 (TID 212) in 1814 ms on 10.201.8.107 (executor 1) (40/48)\n",
      "2023-01-25 09:29:12,480 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 82.0 (TID 216) (10.201.8.108, executor 3, partition 43, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:12,481 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 82.0 (TID 213) in 1710 ms on 10.201.8.108 (executor 3) (41/48)\n",
      "2023-01-25 09:29:12,988 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 82.0 (TID 214) in 1627 ms on 10.200.140.66 (executor 2) (42/48)\n",
      "2023-01-25 09:29:13,460 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 82.0 (TID 217) (10.201.8.107, executor 1, partition 44, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:13,460 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 82.0 (TID 215) in 1883 ms on 10.201.8.107 (executor 1) (43/48)\n",
      "2023-01-25 09:29:14,274 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 82.0 (TID 218) (10.201.8.108, executor 3, partition 45, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:14,274 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 82.0 (TID 216) in 1794 ms on 10.201.8.108 (executor 3) (44/48)\n",
      "2023-01-25 09:29:14,663 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 82.0 (TID 219) (10.201.8.107, executor 1, partition 46, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:14,663 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 82.0 (TID 217) in 1203 ms on 10.201.8.107 (executor 1) (45/48)\n",
      "2023-01-25 09:29:16,359 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 82.0 (TID 220) (10.201.8.108, executor 3, partition 47, NODE_LOCAL, 4570 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:16,359 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 82.0 (TID 218) in 2085 ms on 10.201.8.108 (executor 3) (46/48)\n",
      "2023-01-25 09:29:16,559 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 82.0 (TID 219) in 1896 ms on 10.201.8.107 (executor 1) (47/48)\n",
      "2023-01-25 09:29:17,682 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 82.0 (TID 220) in 1323 ms on 10.201.8.108 (executor 3) (48/48)\n",
      "2023-01-25 09:29:17,682 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:17,682 INFO scheduler.DAGScheduler: ShuffleMapStage 82 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 40.851 s\n",
      "2023-01-25 09:29:17,682 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:17,682 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-01-25 09:29:17,682 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:17,682 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:18,350 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-01-25 09:29:18,352 INFO scheduler.DAGScheduler: Got job 43 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 1 output partitions\n",
      "2023-01-25 09:29:18,352 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-01-25 09:29:18,352 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 85)\n",
      "2023-01-25 09:29:18,352 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:18,352 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[433] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-01-25 09:29:18,471 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "2023-01-25 09:29:18,471 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 2.6 MiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:18,478 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 502.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:18,478 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 502.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:18,478 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:18,479 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[433] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0))\n",
      "2023-01-25 09:29:18,479 INFO scheduler.TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "2023-01-25 09:29:18,479 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 221) (10.201.8.107, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:18,491 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.201.8.107:34250 (size: 502.3 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:29:18,581 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 10.201.8.107:45122\n",
      "2023-01-25 09:29:20,284 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 221) in 1805 ms on 10.201.8.107 (executor 1) (1/1)\n",
      "2023-01-25 09:29:20,284 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:20,284 INFO scheduler.DAGScheduler: ResultStage 86 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 1.931 s\n",
      "2023-01-25 09:29:20,285 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-01-25 09:29:20,285 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished\n",
      "2023-01-25 09:29:20,285 INFO scheduler.DAGScheduler: Job 43 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104, took 1.934128 s\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2023-01-25 09:29:20,745 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 286.6 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:20,747 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.201.8.107:34250 in memory (size: 286.6 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:20,747 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.201.8.108:46028 in memory (size: 286.6 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:29:20,747 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.200.140.66:38378 in memory (size: 286.6 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:20,752 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 502.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:20,752 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.201.8.107:34250 in memory (size: 502.3 KiB, free: 1007.2 MiB)\n",
      "2023-01-25 09:29:22,347 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,347 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#25 as timestamp))\n",
      "2023-01-25 09:29:22,347 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,349 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,349 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2743 as timestamp))\n",
      "2023-01-25 09:29:22,349 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,351 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,351 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2833 as timestamp))\n",
      "2023-01-25 09:29:22,351 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,353 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,354 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#2925 as timestamp))\n",
      "2023-01-25 09:29:22,354 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,356 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,356 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3019 as timestamp))\n",
      "2023-01-25 09:29:22,356 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,358 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,358 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3115 as timestamp))\n",
      "2023-01-25 09:29:22,358 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,360 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,361 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3213 as timestamp))\n",
      "2023-01-25 09:29:22,361 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,363 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,363 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3313 as timestamp))\n",
      "2023-01-25 09:29:22,363 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,365 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,365 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3415 as timestamp))\n",
      "2023-01-25 09:29:22,365 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,368 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,368 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3519 as timestamp))\n",
      "2023-01-25 09:29:22,368 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,370 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,370 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3625 as timestamp))\n",
      "2023-01-25 09:29:22,370 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,372 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,372 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3733 as timestamp))\n",
      "2023-01-25 09:29:22,372 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,375 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,375 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3843 as timestamp))\n",
      "2023-01-25 09:29:22,375 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,377 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,377 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#3955 as timestamp))\n",
      "2023-01-25 09:29:22,377 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,379 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,379 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4069 as timestamp))\n",
      "2023-01-25 09:29:22,379 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:22,382 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-01-25 09:29:22,382 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(cast(timestamp#4185 as timestamp))\n",
      "2023-01-25 09:29:22,382 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-01-25 09:29:23,287 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,301 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,302 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,302 INFO spark.SparkContext: Created broadcast 85 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,303 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,334 INFO scheduler.DAGScheduler: Registering RDD 438 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 39\n",
      "2023-01-25 09:29:23,335 INFO scheduler.DAGScheduler: Got map stage job 44 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,335 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,335 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,335 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,335 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[438] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,346 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 229.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,348 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 67.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,348 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 67.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,348 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,349 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[438] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,349 INFO scheduler.TaskSchedulerImpl: Adding task set 87.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:23,350 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 222) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:23,350 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 87.0 (TID 223) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:23,358 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.201.8.107:34250 (size: 67.3 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:29:23,358 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.201.8.108:46028 (size: 67.3 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:29:23,428 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,442 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,442 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,443 INFO spark.SparkContext: Created broadcast 87 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,444 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,455 INFO scheduler.DAGScheduler: Registering RDD 443 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 40\n",
      "2023-01-25 09:29:23,456 INFO scheduler.DAGScheduler: Got map stage job 45 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,456 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 88 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,456 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,456 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,456 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[443] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,462 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 260.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,464 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 74.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,465 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 74.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,465 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,465 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[443] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,465 INFO scheduler.TaskSchedulerImpl: Adding task set 88.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:23,466 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 88.0 (TID 224) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:23,474 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.200.140.66:38378 (size: 74.1 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:23,542 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,570 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.1 MiB)\n",
      "2023-01-25 09:29:23,573 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,574 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,574 INFO spark.SparkContext: Created broadcast 89 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,575 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,581 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:23,582 INFO scheduler.DAGScheduler: Registering RDD 448 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 41\n",
      "2023-01-25 09:29:23,582 INFO scheduler.DAGScheduler: Got map stage job 46 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,582 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 89 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,582 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,582 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,583 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 89 (MapPartitionsRDD[448] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,588 INFO memory.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,589 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:23,590 INFO memory.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,590 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,590 INFO spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,591 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 89 (MapPartitionsRDD[448] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,591 INFO scheduler.TaskSchedulerImpl: Adding task set 89.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:23,664 INFO memory.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,679 INFO memory.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,680 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,680 INFO spark.SparkContext: Created broadcast 91 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,681 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Registering RDD 453 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 42\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Got map stage job 47 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 90 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,691 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[453] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,697 INFO memory.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,699 INFO memory.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,700 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,700 INFO spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,700 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[453] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,700 INFO scheduler.TaskSchedulerImpl: Adding task set 90.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:23,776 INFO memory.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,790 INFO memory.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,791 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,791 INFO spark.SparkContext: Created broadcast 93 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,792 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,799 INFO scheduler.DAGScheduler: Registering RDD 458 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 43\n",
      "2023-01-25 09:29:23,799 INFO scheduler.DAGScheduler: Got map stage job 48 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,799 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 91 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,799 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,799 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,800 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 91 (MapPartitionsRDD[458] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,805 INFO memory.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,807 INFO memory.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 72.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,807 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,808 INFO spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,808 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 91 (MapPartitionsRDD[458] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,808 INFO scheduler.TaskSchedulerImpl: Adding task set 91.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:23,901 INFO memory.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,916 INFO memory.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,916 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,917 INFO spark.SparkContext: Created broadcast 95 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:23,918 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:23,927 INFO scheduler.DAGScheduler: Registering RDD 463 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 44\n",
      "2023-01-25 09:29:23,927 INFO scheduler.DAGScheduler: Got map stage job 49 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:23,927 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:23,927 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:23,927 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:23,928 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[463] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:23,933 INFO memory.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,935 INFO memory.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:23,936 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:23,936 INFO spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:23,936 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[463] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:23,936 INFO scheduler.TaskSchedulerImpl: Adding task set 92.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,013 INFO memory.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,026 INFO memory.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,027 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,027 INFO spark.SparkContext: Created broadcast 97 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,028 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,035 INFO scheduler.DAGScheduler: Registering RDD 468 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 45\n",
      "2023-01-25 09:29:24,035 INFO scheduler.DAGScheduler: Got map stage job 50 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,035 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 93 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,035 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,036 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,036 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 93 (MapPartitionsRDD[468] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,041 INFO memory.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,043 INFO memory.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 72.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,043 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,044 INFO spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,044 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 93 (MapPartitionsRDD[468] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,044 INFO scheduler.TaskSchedulerImpl: Adding task set 93.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,116 INFO memory.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,131 INFO memory.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,131 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,132 INFO spark.SparkContext: Created broadcast 99 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,133 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,141 INFO scheduler.DAGScheduler: Registering RDD 473 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 46\n",
      "2023-01-25 09:29:24,142 INFO scheduler.DAGScheduler: Got map stage job 51 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,142 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 94 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,142 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,142 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,142 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 94 (MapPartitionsRDD[473] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,147 INFO memory.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,149 INFO memory.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,149 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,150 INFO spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,150 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 94 (MapPartitionsRDD[473] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,150 INFO scheduler.TaskSchedulerImpl: Adding task set 94.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,230 INFO memory.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,244 INFO memory.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,245 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,246 INFO spark.SparkContext: Created broadcast 101 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,246 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Registering RDD 478 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 47\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Got map stage job 52 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 95 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,254 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[478] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,259 INFO memory.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,261 INFO memory.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 72.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,262 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,262 INFO spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,263 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[478] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,263 INFO scheduler.TaskSchedulerImpl: Adding task set 95.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,337 INFO memory.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,351 INFO memory.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,352 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,352 INFO spark.SparkContext: Created broadcast 103 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,353 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Registering RDD 483 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 48\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Got map stage job 53 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,365 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[483] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,370 INFO memory.MemoryStore: Block broadcast_104 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,372 INFO memory.MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,373 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,373 INFO spark.SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,376 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[483] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,376 INFO scheduler.TaskSchedulerImpl: Adding task set 96.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,455 INFO memory.MemoryStore: Block broadcast_105 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,469 INFO memory.MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,470 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,470 INFO spark.SparkContext: Created broadcast 105 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,471 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,477 INFO scheduler.DAGScheduler: Registering RDD 488 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 49\n",
      "2023-01-25 09:29:24,478 INFO scheduler.DAGScheduler: Got map stage job 54 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,478 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 97 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,478 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,478 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,478 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 97 (MapPartitionsRDD[488] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,483 INFO memory.MemoryStore: Block broadcast_106 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,485 INFO memory.MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 72.9 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,485 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,486 INFO spark.SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,486 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 97 (MapPartitionsRDD[488] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,486 INFO scheduler.TaskSchedulerImpl: Adding task set 97.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,560 INFO memory.MemoryStore: Block broadcast_107 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,575 INFO memory.MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,575 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,575 INFO spark.SparkContext: Created broadcast 107 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,576 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Registering RDD 493 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 50\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Got map stage job 55 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 98 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,585 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 98 (MapPartitionsRDD[493] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,590 INFO memory.MemoryStore: Block broadcast_108 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,592 INFO memory.MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,592 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,592 INFO spark.SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,593 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 98 (MapPartitionsRDD[493] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,593 INFO scheduler.TaskSchedulerImpl: Adding task set 98.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,666 INFO memory.MemoryStore: Block broadcast_109 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,681 INFO memory.MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,681 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,682 INFO spark.SparkContext: Created broadcast 109 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,682 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Registering RDD 498 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 51\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Got map stage job 56 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 99 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,689 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[498] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,694 INFO memory.MemoryStore: Block broadcast_110 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,703 INFO memory.MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,704 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,704 INFO spark.SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,704 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[498] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,704 INFO scheduler.TaskSchedulerImpl: Adding task set 99.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,781 INFO memory.MemoryStore: Block broadcast_111 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,797 INFO memory.MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,798 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,799 INFO spark.SparkContext: Created broadcast 111 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,799 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,807 INFO scheduler.DAGScheduler: Registering RDD 503 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 52\n",
      "2023-01-25 09:29:24,808 INFO scheduler.DAGScheduler: Got map stage job 57 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,808 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 100 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,808 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,808 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,808 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 100 (MapPartitionsRDD[503] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,815 INFO memory.MemoryStore: Block broadcast_112 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,818 INFO memory.MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,818 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,818 INFO spark.SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,819 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 100 (MapPartitionsRDD[503] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,819 INFO scheduler.TaskSchedulerImpl: Adding task set 100.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:24,895 INFO memory.MemoryStore: Block broadcast_113 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,911 INFO memory.MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,911 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,911 INFO spark.SparkContext: Created broadcast 113 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:24,912 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:24,919 INFO scheduler.DAGScheduler: Registering RDD 508 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 53\n",
      "2023-01-25 09:29:24,920 INFO scheduler.DAGScheduler: Got map stage job 58 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:24,920 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:24,920 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:24,920 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:24,920 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[508] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:24,925 INFO memory.MemoryStore: Block broadcast_114 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,927 INFO memory.MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:24,927 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:24,928 INFO spark.SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:24,928 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[508] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:24,928 INFO scheduler.TaskSchedulerImpl: Adding task set 101.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:25,009 INFO memory.MemoryStore: Block broadcast_115 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:25,024 INFO memory.MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:25,025 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:25,025 INFO spark.SparkContext: Created broadcast 115 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-01-25 09:29:25,026 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Registering RDD 513 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) as input to shuffle 54\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Got map stage job 59 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 2 output partitions\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 102 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-01-25 09:29:25,037 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 102 (MapPartitionsRDD[513] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-01-25 09:29:25,042 INFO memory.MemoryStore: Block broadcast_116 stored as values in memory (estimated size 258.0 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:25,044 INFO memory.MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 73.1 KiB, free 2.2 GiB)\n",
      "2023-01-25 09:29:25,045 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:25,045 INFO spark.SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1478\n",
      "2023-01-25 09:29:25,045 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 102 (MapPartitionsRDD[513] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-01-25 09:29:25,045 INFO scheduler.TaskSchedulerImpl: Adding task set 102.0 with 2 tasks resource profile 0\n",
      "2023-01-25 09:29:28,590 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 88.0 (TID 225) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:28,590 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 88.0 (TID 224) in 5124 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:29:28,873 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 226) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:28,873 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 87.0 (TID 223) in 5523 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:29:28,880 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.201.8.108:46028 (size: 73.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:29,257 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:30,875 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 89.0 (TID 227) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:30,876 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 222) in 7526 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:29:30,876 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:30,876 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 7.541 s\n",
      "2023-01-25 09:29:30,876 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:30,876 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 88, ShuffleMapStage 89, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:30,876 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:30,876 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:30,957 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.201.8.107:34250 (size: 73.0 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:31,277 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:31,480 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 228) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:31,480 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 88.0 (TID 225) in 2890 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:29:31,480 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:31,480 INFO scheduler.DAGScheduler: ShuffleMapStage 88 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 8.023 s\n",
      "2023-01-25 09:29:31,481 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:31,481 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 89, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:31,481 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:31,481 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:31,486 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.200.140.66:38378 (size: 73.1 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:31,609 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 74.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:31,675 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.200.140.66:38378 in memory (size: 74.1 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:31,679 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 67.3 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:31,757 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.201.8.108:46028 in memory (size: 67.3 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:31,758 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.201.8.107:34250 in memory (size: 67.3 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:31,789 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:35,180 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 90.0 (TID 229) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:35,180 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 226) in 6307 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:29:35,188 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.201.8.108:46028 (size: 73.1 KiB, free: 1007.0 MiB)\n",
      "2023-01-25 09:29:35,379 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:36,159 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 230) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:36,159 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 89.0 (TID 227) in 5284 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:29:36,159 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:36,159 INFO scheduler.DAGScheduler: ShuffleMapStage 89 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 12.576 s\n",
      "2023-01-25 09:29:36,160 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:36,160 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 90, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:36,160 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:36,160 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:36,166 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.201.8.107:34250 (size: 72.9 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:36,191 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 91.0 (TID 231) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:36,191 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 228) in 4712 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:29:36,199 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.200.140.66:38378 (size: 72.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:36,374 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.9 MiB)\n",
      "2023-01-25 09:29:36,479 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:39,270 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 232) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:39,271 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 90.0 (TID 229) in 4091 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:29:39,271 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:39,271 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 15.579 s\n",
      "2023-01-25 09:29:39,271 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:39,271 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 91, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:39,271 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:39,271 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:39,278 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.201.8.108:46028 (size: 73.1 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:39,663 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:40,192 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 92.0 (TID 233) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:40,192 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 91.0 (TID 231) in 4001 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:29:40,199 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.200.140.66:38378 (size: 73.1 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:40,472 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:41,465 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 93.0 (TID 234) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:41,465 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 230) in 5307 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:29:41,465 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:41,465 INFO scheduler.DAGScheduler: ShuffleMapStage 91 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 17.665 s\n",
      "2023-01-25 09:29:41,466 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:41,466 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 92, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:41,466 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:41,466 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:41,472 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.201.8.107:34250 (size: 72.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:41,767 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:44,203 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 93.0 (TID 235) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:44,204 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 92.0 (TID 233) in 4012 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:29:44,271 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.200.140.66:38378 (size: 72.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:44,381 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 236) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:44,381 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 232) in 5111 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:29:44,381 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:44,381 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 20.453 s\n",
      "2023-01-25 09:29:44,382 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:44,382 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 93, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:44,382 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:44,382 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:44,462 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.201.8.108:46028 (size: 73.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:44,471 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:44,758 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:47,260 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 94.0 (TID 237) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:47,261 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 93.0 (TID 234) in 5797 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:29:47,268 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.201.8.107:34250 (size: 73.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:47,594 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:48,195 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 238) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:48,196 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 93.0 (TID 235) in 3993 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:29:48,196 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:48,197 INFO scheduler.DAGScheduler: ShuffleMapStage 93 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 24.160 s\n",
      "2023-01-25 09:29:48,197 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:48,197 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 94, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:48,197 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:48,197 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:48,270 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.200.140.66:38378 (size: 72.9 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:29:48,570 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:29:49,270 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 95.0 (TID 239) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:49,271 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 236) in 4891 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:29:49,279 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.201.8.108:46028 (size: 72.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:49,294 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:51,189 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 240) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:51,190 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 94.0 (TID 237) in 3930 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:29:51,190 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:51,190 INFO scheduler.DAGScheduler: ShuffleMapStage 94 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 27.048 s\n",
      "2023-01-25 09:29:51,190 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:51,190 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101, ShuffleMapStage 95)\n",
      "2023-01-25 09:29:51,190 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:51,190 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:51,260 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.201.8.107:34250 (size: 73.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:51,461 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:51,575 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 96.0 (TID 241) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:51,575 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 95.0 (TID 239) in 2305 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:29:51,582 INFO storage.BlockManagerInfo: Added broadcast_104_piece0 in memory on 10.201.8.108:46028 (size: 73.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:51,872 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:29:52,802 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 97.0 (TID 242) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:52,802 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 238) in 4607 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:29:52,802 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:52,802 INFO scheduler.DAGScheduler: ShuffleMapStage 95 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 28.547 s\n",
      "2023-01-25 09:29:52,803 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:52,803 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 96, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-01-25 09:29:52,803 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:52,803 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:52,871 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.200.140.66:38378 (size: 72.9 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:29:53,079 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:29:53,347 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,357 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.201.8.107:34250 in memory (size: 72.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:53,368 INFO storage.BlockManagerInfo: Removed broadcast_94_piece0 on 10.200.140.66:38378 in memory (size: 72.9 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:29:53,372 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,374 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on 10.201.8.108:46028 in memory (size: 73.0 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:53,374 INFO storage.BlockManagerInfo: Removed broadcast_100_piece0 on 10.201.8.107:34250 in memory (size: 73.0 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:53,461 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,462 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.201.8.108:46028 in memory (size: 73.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:53,469 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.200.140.66:38378 in memory (size: 73.1 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:29:53,473 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 73.1 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,473 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.200.140.66:38378 in memory (size: 73.1 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:53,474 INFO storage.BlockManagerInfo: Removed broadcast_96_piece0 on 10.201.8.108:46028 in memory (size: 73.1 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:53,568 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,569 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on 10.200.140.66:38378 in memory (size: 72.9 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:53,570 INFO storage.BlockManagerInfo: Removed broadcast_102_piece0 on 10.201.8.108:46028 in memory (size: 72.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:53,577 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 73.0 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,578 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.201.8.107:34250 in memory (size: 73.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:53,667 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.201.8.108:46028 in memory (size: 73.0 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:53,673 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on transactions-ingest-e4b8e45c-3a39f385e838d282-driver-svc.default-tenant.svc:7079 in memory (size: 72.9 KiB, free: 2.2 GiB)\n",
      "2023-01-25 09:29:53,674 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on 10.200.140.66:38378 in memory (size: 72.9 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:53,675 INFO storage.BlockManagerInfo: Removed broadcast_98_piece0 on 10.201.8.107:34250 in memory (size: 72.9 KiB, free: 1006.8 MiB)\n",
      "2023-01-25 09:29:55,874 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 97.0 (TID 243) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:55,875 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 96.0 (TID 241) in 4301 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:29:55,882 INFO storage.BlockManagerInfo: Added broadcast_106_piece0 in memory on 10.201.8.108:46028 (size: 72.9 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:56,167 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 244) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:56,167 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 240) in 4978 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:29:56,167 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:29:56,168 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 31.802 s\n",
      "2023-01-25 09:29:56,168 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:29:56,168 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 97, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-01-25 09:29:56,168 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:29:56,168 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:29:56,171 INFO storage.BlockManagerInfo: Added broadcast_105_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:56,180 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.201.8.107:34250 (size: 73.0 KiB, free: 1006.7 MiB)\n",
      "2023-01-25 09:29:56,384 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:29:57,900 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 98.0 (TID 245) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:29:57,900 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 97.0 (TID 242) in 5098 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:29:57,968 INFO storage.BlockManagerInfo: Added broadcast_108_piece0 in memory on 10.200.140.66:38378 (size: 73.0 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:29:58,192 INFO storage.BlockManagerInfo: Added broadcast_107_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:30:00,086 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 246) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:00,086 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 97.0 (TID 243) in 4212 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:30:00,086 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:30:00,086 INFO scheduler.DAGScheduler: ShuffleMapStage 97 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 35.607 s\n",
      "2023-01-25 09:30:00,086 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:30:00,086 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 98, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-01-25 09:30:00,086 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:30:00,086 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:30:00,093 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.201.8.108:46028 (size: 73.0 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:30:00,162 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:30:01,364 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 99.0 (TID 247) (10.201.8.107, executor 1, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:01,365 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 244) in 5198 ms on 10.201.8.107 (executor 1) (1/2)\n",
      "2023-01-25 09:30:01,374 INFO storage.BlockManagerInfo: Added broadcast_110_piece0 in memory on 10.201.8.107:34250 (size: 73.0 KiB, free: 1006.6 MiB)\n",
      "2023-01-25 09:30:01,584 INFO storage.BlockManagerInfo: Added broadcast_109_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.5 MiB)\n",
      "2023-01-25 09:30:01,704 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 248) (10.200.140.66, executor 2, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:01,704 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 98.0 (TID 245) in 3805 ms on 10.200.140.66 (executor 2) (2/2)\n",
      "2023-01-25 09:30:01,704 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:30:01,704 INFO scheduler.DAGScheduler: ShuffleMapStage 98 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 37.118 s\n",
      "2023-01-25 09:30:01,704 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:30:01,704 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 99, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-01-25 09:30:01,704 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:30:01,704 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:30:01,770 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.200.140.66:38378 (size: 73.1 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:01,979 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:03,466 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 100.0 (TID 249) (10.201.8.108, executor 3, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:03,467 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 246) in 3382 ms on 10.201.8.108 (executor 3) (1/2)\n",
      "2023-01-25 09:30:03,474 INFO storage.BlockManagerInfo: Added broadcast_112_piece0 in memory on 10.201.8.108:46028 (size: 73.1 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:03,770 INFO storage.BlockManagerInfo: Added broadcast_111_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:05,170 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 250) (10.201.8.107, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:05,170 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 99.0 (TID 247) in 3806 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:30:05,171 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:30:05,172 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 40.482 s\n",
      "2023-01-25 09:30:05,172 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:30:05,172 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 100, ShuffleMapStage 101)\n",
      "2023-01-25 09:30:05,172 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:30:05,172 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:30:05,179 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.201.8.107:34250 (size: 73.0 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:05,557 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.201.8.107:34250 (size: 54.8 KiB, free: 1006.4 MiB)\n",
      "2023-01-25 09:30:06,303 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 101.0 (TID 251) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:06,303 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 248) in 4600 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:30:06,359 INFO storage.BlockManagerInfo: Added broadcast_114_piece0 in memory on 10.200.140.66:38378 (size: 73.0 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:30:06,377 INFO storage.BlockManagerInfo: Added broadcast_113_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:30:07,483 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 102.0 (TID 252) (10.201.8.108, executor 3, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:07,484 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 100.0 (TID 249) in 4018 ms on 10.201.8.108 (executor 3) (2/2)\n",
      "2023-01-25 09:30:07,484 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:30:07,485 INFO scheduler.DAGScheduler: ShuffleMapStage 100 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 42.677 s\n",
      "2023-01-25 09:30:07,485 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:30:07,485 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102, ShuffleMapStage 101)\n",
      "2023-01-25 09:30:07,485 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:30:07,485 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-01-25 09:30:07,491 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on 10.201.8.108:46028 (size: 73.1 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:30:07,880 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.201.8.108:46028 (size: 54.8 KiB, free: 1006.3 MiB)\n",
      "2023-01-25 09:30:09,301 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 102.0 (TID 253) (10.200.140.66, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-01-25 09:30:09,301 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 101.0 (TID 251) in 2999 ms on 10.200.140.66 (executor 2) (1/2)\n",
      "2023-01-25 09:30:09,465 INFO storage.BlockManagerInfo: Added broadcast_116_piece0 in memory on 10.200.140.66:38378 (size: 73.1 KiB, free: 1006.2 MiB)\n",
      "2023-01-25 09:30:09,758 INFO storage.BlockManagerInfo: Added broadcast_115_piece0 in memory on 10.200.140.66:38378 (size: 54.8 KiB, free: 1006.1 MiB)\n",
      "2023-01-25 09:30:10,157 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 250) in 4987 ms on 10.201.8.107 (executor 1) (2/2)\n",
      "2023-01-25 09:30:10,157 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool \n",
      "2023-01-25 09:30:10,157 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 45.237 s\n",
      "2023-01-25 09:30:10,157 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-01-25 09:30:10,157 INFO scheduler.DAGScheduler: running: Set(ShuffleMapStage 102)\n",
      "2023-01-25 09:30:10,157 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-01-25 09:30:10,157 INFO scheduler.DAGScheduler: failed: Set()\n"
     ]
    }
   ],
   "source": [
    "from mlrun.datastore.sources import CSVSource\n",
    "\n",
    "# Creating our partitioned csv source\n",
    "transaction_source = CSVSource(\"flow1_source\", path=project.get_artifact('transactions_data').target_path)\n",
    "\n",
    "# Ingest our transactions dataset through our defined pipeline\n",
    "fstore.ingest(transaction_set, transaction_source, run_config=run_config, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ingestion process, you will be able to see all the different features that were created with the help of the UI, as you can see in the image below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Features Catalog - fraud prevention](./images/features-catalog-transaction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>_c0</th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>es_barsandrestaurants_count_14d</th>\n",
       "      <th>es_tech_count_14d</th>\n",
       "      <th>es_sportsandtoys_count_14d</th>\n",
       "      <th>es_wellnessandbeauty_count_14d</th>\n",
       "      <th>es_hyper_count_14d</th>\n",
       "      <th>es_fashion_count_14d</th>\n",
       "      <th>es_home_count_14d</th>\n",
       "      <th>es_contents_count_14d</th>\n",
       "      <th>es_travel_count_14d</th>\n",
       "      <th>es_leisure_count_14d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1000148617</td>\n",
       "      <td>2023-01-23 10:00:00</td>\n",
       "      <td>323447</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C100045114</td>\n",
       "      <td>2023-01-23 10:00:00</td>\n",
       "      <td>448323</td>\n",
       "      <td>140</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>59.52</td>\n",
       "      <td>0</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1002759277</td>\n",
       "      <td>2023-01-23 10:00:00</td>\n",
       "      <td>384109</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>39.65</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1004300450</td>\n",
       "      <td>2023-01-23 10:00:00</td>\n",
       "      <td>433885</td>\n",
       "      <td>136</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1005126300</td>\n",
       "      <td>2023-01-23 10:00:00</td>\n",
       "      <td>372213</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>29.02</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125973</th>\n",
       "      <td>C1187799344</td>\n",
       "      <td>2023-02-08 00:00:00</td>\n",
       "      <td>176188</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>31.21</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125974</th>\n",
       "      <td>C1187979419</td>\n",
       "      <td>2023-02-08 00:00:00</td>\n",
       "      <td>124869</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0</td>\n",
       "      <td>M209847108</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125975</th>\n",
       "      <td>C1188590334</td>\n",
       "      <td>2023-02-08 00:00:00</td>\n",
       "      <td>449851</td>\n",
       "      <td>140</td>\n",
       "      <td>6</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125976</th>\n",
       "      <td>C1189231170</td>\n",
       "      <td>2023-02-08 00:00:00</td>\n",
       "      <td>87663</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>35.51</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125977</th>\n",
       "      <td>C1189752912</td>\n",
       "      <td>2023-02-08 00:00:00</td>\n",
       "      <td>246646</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>18.16</td>\n",
       "      <td>0</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125978 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             source           timestamp     _c0  step age  zipcodeOri  \\\n",
       "0       C1000148617 2023-01-23 10:00:00  323447   105   5       28007   \n",
       "1        C100045114 2023-01-23 10:00:00  448323   140   4       28007   \n",
       "2       C1002759277 2023-01-23 10:00:00  384109   122   1       28007   \n",
       "3       C1004300450 2023-01-23 10:00:00  433885   136   4       28007   \n",
       "4       C1005126300 2023-01-23 10:00:00  372213   119   4       28007   \n",
       "...             ...                 ...     ...   ...  ..         ...   \n",
       "125973  C1187799344 2023-02-08 00:00:00  176188    61   3       28007   \n",
       "125974  C1187979419 2023-02-08 00:00:00  124869    44   2       28007   \n",
       "125975  C1188590334 2023-02-08 00:00:00  449851   140   6       28007   \n",
       "125976  C1189231170 2023-02-08 00:00:00   87663    32   3       28007   \n",
       "125977  C1189752912 2023-02-08 00:00:00  246646    82   2       28007   \n",
       "\n",
       "        zipMerchant  amount  fraud       target  ...  \\\n",
       "0             28007    4.64      0  M1823072687  ...   \n",
       "1             28007   59.52      0   M348934600  ...   \n",
       "2             28007   39.65      0  M1823072687  ...   \n",
       "3             28007    2.23      0  M1823072687  ...   \n",
       "4             28007   29.02      0  M1823072687  ...   \n",
       "...             ...     ...    ...          ...  ...   \n",
       "125973        28007   31.21      0  M1823072687  ...   \n",
       "125974        28007    3.40      0   M209847108  ...   \n",
       "125975        28007   16.02      0  M1823072687  ...   \n",
       "125976        28007   35.51      0  M1823072687  ...   \n",
       "125977        28007   18.16      0   M348934600  ...   \n",
       "\n",
       "       es_barsandrestaurants_count_14d es_tech_count_14d  \\\n",
       "0                                  NaN               NaN   \n",
       "1                                  NaN               NaN   \n",
       "2                                  NaN               NaN   \n",
       "3                                  NaN               NaN   \n",
       "4                                  NaN               NaN   \n",
       "...                                ...               ...   \n",
       "125973                            23.0              23.0   \n",
       "125974                            19.0              19.0   \n",
       "125975                            18.0              18.0   \n",
       "125976                            26.0              26.0   \n",
       "125977                             6.0               6.0   \n",
       "\n",
       "       es_sportsandtoys_count_14d es_wellnessandbeauty_count_14d  \\\n",
       "0                             NaN                            NaN   \n",
       "1                             NaN                            NaN   \n",
       "2                             NaN                            NaN   \n",
       "3                             NaN                            NaN   \n",
       "4                             NaN                            NaN   \n",
       "...                           ...                            ...   \n",
       "125973                       23.0                           23.0   \n",
       "125974                       19.0                           19.0   \n",
       "125975                       18.0                           18.0   \n",
       "125976                       26.0                           26.0   \n",
       "125977                        6.0                            6.0   \n",
       "\n",
       "        es_hyper_count_14d  es_fashion_count_14d  es_home_count_14d  \\\n",
       "0                      NaN                   NaN                NaN   \n",
       "1                      NaN                   NaN                NaN   \n",
       "2                      NaN                   NaN                NaN   \n",
       "3                      NaN                   NaN                NaN   \n",
       "4                      NaN                   NaN                NaN   \n",
       "...                    ...                   ...                ...   \n",
       "125973                23.0                  23.0               23.0   \n",
       "125974                19.0                  19.0               19.0   \n",
       "125975                18.0                  18.0               18.0   \n",
       "125976                26.0                  26.0               26.0   \n",
       "125977                 6.0                   6.0                6.0   \n",
       "\n",
       "        es_contents_count_14d  es_travel_count_14d  es_leisure_count_14d  \n",
       "0                         NaN                  NaN                   NaN  \n",
       "1                         NaN                  NaN                   NaN  \n",
       "2                         NaN                  NaN                   NaN  \n",
       "3                         NaN                  NaN                   NaN  \n",
       "4                         NaN                  NaN                   NaN  \n",
       "...                       ...                  ...                   ...  \n",
       "125973                   23.0                 23.0                  23.0  \n",
       "125974                   19.0                 19.0                  19.0  \n",
       "125975                   18.0                 18.0                  18.0  \n",
       "125976                   26.0                 26.0                  26.0  \n",
       "125977                    6.0                  6.0                   6.0  \n",
       "\n",
       "[125978 rows x 52 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_set.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>event</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1974668487</td>\n",
       "      <td>details_change</td>\n",
       "      <td>2023-01-24 19:49:18.023360086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1973547259</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-24 22:50:52.280500508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C515668508</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-24 19:23:58.561677302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source           event                     timestamp\n",
       "0  C1974668487  details_change 2023-01-24 19:49:18.023360086\n",
       "1  C1973547259           login 2023-01-24 22:50:52.280500508\n",
       "2   C515668508           login 2023-01-24 19:23:58.561677302"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch our user_events dataset from the server\n",
    "user_events_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/events.csv', \n",
    "                               index_col=0, quotechar=\"\\'\", parse_dates=['timestamp'])\n",
    "\n",
    "# Adjust to the last 2 days to see the latest aggregations in our online feature vectors\n",
    "user_events_data = adjust_data_timespan(user_events_data, new_period='2d')\n",
    "\n",
    "# Preview\n",
    "user_events_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Create a FeatureSet and Preprocessing Pipeline\n",
    "\n",
    "Now we will define the events feature set.\n",
    "This is a pretty straight forward pipeline in which we only one hot encode the event categories and save the data to the default targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set = fstore.FeatureSet(\"events\",\n",
    "                                    entities=[fstore.Entity(\"source\")],\n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"user events feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and add value mapping\n",
    "events_mapping = {'event': list(user_events_data.event.unique())}\n",
    "\n",
    "# One Hot Encode\n",
    "user_events_set.graph.to(OneHotEncoder(mapping=events_mapping))\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "user_events_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "user_events_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"transaction_spark_function\").getOrCreate()\n",
    "\n",
    "# Ingestion of our newly created events feature set locally\n",
    "events_df = fstore.ingest(user_events_set, user_events_data, spark_context=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a labels dataset for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Create a FeatureSet\n",
    "This feature set contains the label for the fraud demo, it will be ingested directly to the default targets without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    labels = df[['fraud','timestamp']].copy()\n",
    "    labels = labels.rename(columns={\"fraud\": \"label\"})\n",
    "    labels['timestamp'] = labels['timestamp'].astype(\"datetime64[ms]\")\n",
    "    labels['label'] = labels['label'].astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.datastore import ParquetTarget\n",
    "import os\n",
    "\n",
    "# Define the \"labels\" feature set\n",
    "labels_set = fstore.FeatureSet(\"labels\", \n",
    "                           entities=[fstore.Entity(\"source\")], \n",
    "                           timestamp_key='timestamp',\n",
    "                           description=\"training labels\",\n",
    "                           engine=\"pandas\")\n",
    "\n",
    "labels_set.graph.to(name=\"create_labels\", handler=create_labels)\n",
    "\n",
    "\n",
    "# specify only Parquet (offline) target since its not used for real-time\n",
    "target = ParquetTarget(name='labels',path=f'v3io:///projects/{project.name}/target.parquet')\n",
    "labels_set.set_targets([target], with_defaults=False)\n",
    "labels_set.plot(with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the labels feature set\n",
    "labels_df = fstore.ingest(labels_set, transactions_data)\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Deploy a real-time pipeline\n",
    "\n",
    "When dealing with real-time aggregation, it's important to be able to update these aggregations in real-time.\n",
    "For this purpose, we will create live serving functions that will update the online feature store of the `transactions` FeatureSet and `Events` FeatureSet.\n",
    "\n",
    "Using MLRun's `serving` runtime, craetes a nuclio function loaded with our feature set's computational graph definition\n",
    "and an `HttpSource` to define the HTTP trigger.\n",
    "\n",
    "Notice that the implementation below does not require any rewrite of the pipeline logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Deploy our FeatureSet live endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Storey engine to deploy the live endpoint.<br>\n",
    "Storey is streaming library for real time event processing and feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_set_deploy = fstore.FeatureSet(\"transactions\", \n",
    "                                           entities=[fstore.Entity(\"source\")], \n",
    "                                           timestamp_key='timestamp', \n",
    "                                           description=\"transactions feature set\",\n",
    "                                           engine='storey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "transaction_stream = f'v3io:///projects/{project.name}/streams/transaction'\n",
    "transaction_pusher = mlrun.datastore.get_stream_pusher(transaction_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=transaction_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "transaction_set_endpoint = fstore.deploy_ingestion_service(featureset=transaction_set_deploy, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining our `transactions` feature set we can now use MLRun and Storey to deploy it as a live endpoint, ready to ingest new data!\n",
    "\n",
    "Using MLRun's `serving` runtime, we will create a nuclio function loaded with our feature set's computational graph definition and an `HttpSource` to define the HTTP trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Select a sample from the dataset and serialize it to JSON\n",
    "transaction_sample = json.loads(transactions_data.sample(1).to_json(orient='records'))[0]\n",
    "transaction_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "transaction_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(transaction_set_endpoint, json=transaction_sample).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Deploy our FeatureSet live endpoint\n",
    "Deploy the events feature set's ingestion service using the feature set and all the previously defined resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set_deploy = fstore.FeatureSet(\"events\",\n",
    "                                    entities=[fstore.Entity(\"source\")],\n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"user events feature set\",\n",
    "                                    engine='storey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "events_stream = f'v3io:///projects/{project.name}/streams/events'\n",
    "events_pusher = mlrun.datastore.get_stream_pusher(events_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=events_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "events_set_endpoint = fstore.deploy_ingestion_service(featureset=user_events_set_deploy, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample from the events dataset and serialize it to JSON\n",
    "user_events_sample = json.loads(user_events_data.sample(1).to_json(orient='records'))[0]\n",
    "user_events_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "user_events_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(events_set_endpoint, json=user_events_sample).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "You've completed Part 1 of the data-ingestion with the feature store.\n",
    "Proceed to [Part 2](02-create-training-model.ipynb) to learn how to train an ML model using the feature store data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
