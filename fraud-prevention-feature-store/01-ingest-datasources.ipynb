{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - This demo works with the online feature store, which is currently not part of the Open Source default deployment.\n",
    "\n",
    "This demo showcases financial fraud prevention and using the MLRun feature store to define complex features that help identify fraud. Fraud prevention specifically is a challenge as it requires processing raw transaction and events in real-time and being able to quickly respond and block transactions before they occur.\n",
    "\n",
    "To address this, we create a development pipeline and a production pipeline. Both pipelines share the same feature engineering and model code, but serve data very differently. Furthermore, we automate the data and model monitoring process, identify drift and trigger retraining in a CI/CD pipeline. This process is described in the diagram below:\n",
    "\n",
    "![Feature store demo diagram - fraud prevention](./images/feature_store_demo_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is described as follows:\n",
    "\n",
    "| TRANSACTIONS                                                                    || &#x2551; |USER EVENTS                                                                           || \n",
    "|-----------------|----------------------------------------------------------------|----------|-----------------|----------------------------------------------------------------|\n",
    "| **age**         | age group value 0-6. Some values are marked as U for unknown   | &#x2551; | **source**      | The party/entity related to the event                          |\n",
    "| **gender**      | A character to define the gender                               | &#x2551; | **event**       | event, such as login or password change                        |\n",
    "| **zipcodeOri**  | ZIP code of the person originating the transaction             | &#x2551; | **timestamp**   | The date and time of the event                                 |\n",
    "| **zipMerchant** | ZIP code of the merchant receiving the transaction             | &#x2551; |                 |                                                                |\n",
    "| **category**    | category of the transaction (e.g., transportation, food, etc.) | &#x2551; |                 |                                                                |\n",
    "| **amount**      | the total amount of the transaction                            | &#x2551; |                 |                                                                |\n",
    "| **fraud**       | whether the transaction is fraudulent                          | &#x2551; |                 |                                                                |\n",
    "| **timestamp**   | the date and time in which the transaction took place          | &#x2551; |                 |                                                                |\n",
    "| **source**      | the ID of the party/entity performing the transaction          | &#x2551; |                 |                                                                |\n",
    "| **target**      | the ID of the party/entity receiving the transaction           | &#x2551; |                 |                                                                |\n",
    "| **device**      | the device ID used to perform the transaction                  | &#x2551; |                 |                                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces how to **Ingest** different data sources to the **Feature Store**.\n",
    "\n",
    "The following FeatureSets will be created:\n",
    "- **Transactions**: Monetary transactions between a source and a target.\n",
    "- **Events**: Account events such as account login or a password change.\n",
    "- **Label**: Fraud label for the data.\n",
    "\n",
    "By the end of this tutorial youâ€™ll learn how to:\n",
    "\n",
    "- Create an ingestion pipeline for each data source.\n",
    "- Define preprocessing, aggregation and validation of the pipeline.\n",
    "- Run the pipeline locally within the notebook.\n",
    "- Launch a real-time function to ingest live data.\n",
    "- Schedule a cron to run the task when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'fraud-demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:43:07,152 [info] loaded project fraud-demo from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Fetch, Process and Ingest our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions to adjust the timestamps of our data\n",
    "# while keeping the order of the selected events and\n",
    "# the relative distance from one event to the other\n",
    "\n",
    "def date_adjustment(sample, data_max, new_max, old_data_period, new_data_period):\n",
    "    '''\n",
    "        Adjust a specific sample's date according to the original and new time periods\n",
    "    '''\n",
    "    sample_dates_scale = ((data_max - sample) / old_data_period)\n",
    "    sample_delta = new_data_period * sample_dates_scale\n",
    "    new_sample_ts = new_max - sample_delta\n",
    "    return new_sample_ts\n",
    "\n",
    "def adjust_data_timespan(dataframe, timestamp_col='timestamp', new_period='2d', new_max_date_str='now'):\n",
    "    '''\n",
    "        Adjust the dataframe timestamps to the new time period\n",
    "    '''\n",
    "    # Calculate old time period\n",
    "    data_min = dataframe.timestamp.min()\n",
    "    data_max = dataframe.timestamp.max()\n",
    "    old_data_period = data_max-data_min\n",
    "    \n",
    "    # Set new time period\n",
    "    new_time_period = pd.Timedelta(new_period)\n",
    "    new_max = pd.Timestamp(new_max_date_str)\n",
    "    new_min = new_max-new_time_period\n",
    "    new_data_period = new_max-new_min\n",
    "    \n",
    "    # Apply the timestamp change\n",
    "    df = dataframe.copy()\n",
    "    df[timestamp_col] = df[timestamp_col].apply(lambda x: date_adjustment(x, data_max, new_max, old_data_period, new_data_period))\n",
    "    df[timestamp_col].astype(\"datetime64[s]\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184435</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>72.84</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-31 20:55:36.008571510</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333175</th>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>23.58</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 19:42:30.283951113</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>2f13bbd87b9f42f98f5f3fce05890c82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451032</th>\n",
       "      <td>141</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>es_transportation</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 03:27:45.773003439</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>741ccdad2743422c98939329976a9c06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        step age gender  zipcodeOri  zipMerchant           category  amount  \\\n",
       "184435    63   5      M       28007        28007  es_transportation   72.84   \n",
       "333175   108   5      M       28007        28007  es_transportation   23.58   \n",
       "451032   141   5      M       28007        28007  es_transportation    4.31   \n",
       "\n",
       "        fraud                     timestamp       source       target  \\\n",
       "184435      0 2023-01-31 20:55:36.008571510  C1000148617  M1823072687   \n",
       "333175      0 2023-01-30 19:42:30.283951113  C1000148617  M1823072687   \n",
       "451032      0 2023-02-01 03:27:45.773003439  C1000148617   M348934600   \n",
       "\n",
       "                                  device  \n",
       "184435  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "333175  2f13bbd87b9f42f98f5f3fce05890c82  \n",
       "451032  741ccdad2743422c98939329976a9c06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fetch the transactions dataset from the server\n",
    "transactions_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/data.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# use only first 50k\n",
    "transactions_data = transactions_data.sort_values(by='source', axis=0)[:50000]\n",
    "\n",
    "# Adjust the samples timestamp for the past 2 days\n",
    "transactions_data = adjust_data_timespan(transactions_data, new_period='2d')\n",
    "\n",
    "# logging the data\n",
    "project.log_dataset('transactions_data', transactions_data, format='csv')\n",
    "\n",
    "# Preview\n",
    "transactions_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Create a FeatureSet and Preprocessing Pipeline\n",
    "Create the FeatureSet (data pipeline) definition for the **credit transaction processing** which describes the offline/online data transformations and aggregations.<br>\n",
    "The feature store will automatically add an offline `parquet` target and an online `NoSQL` target by using `set_targets()`.\n",
    "\n",
    "The data pipeline consists of:\n",
    "\n",
    "* **Extracting** the data components (hour, day of week)\n",
    "* **Mapping** the age values\n",
    "* **One hot encoding** for the transaction category and the gender\n",
    "* **Aggregating** the amount (avg, sum, count, max over 2/12/24 hour time windows)\n",
    "* **Aggregating** the transactions per category (over 14 days time windows)\n",
    "* **Writing** the results to **offline** (Parquet) and **online** (NoSQL) targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLRun's Feature Store\n",
    "import mlrun.feature_store as fstore\n",
    "from mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transactions FeatureSet\n",
    "transaction_set = fstore.FeatureSet(\"transactions\", \n",
    "                                    entities=[fstore.Entity(\"source\")], \n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"transactions feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"744pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 744.06 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 740.06,-94 740.06,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-27.05 40.7,-27.15 42.83,-27.3 44.92,-27.49 46.98,-27.74 48.99,-28.03 50.95,-28.36 52.84,-28.75 54.66,-29.18 56.4,-29.65 58.06,-30.16 59.63,-30.71 61.11,-31.31 62.49,-31.94 63.76,-32.61 64.93,-33.31 65.99,-34.04 66.93,-34.8 67.77,-35.59 68.48,-36.41 69.09,-37.25 69.58,-38.11 69.95,-38.99 70.21,-39.89 70.36,-40.8 70.4,-41.72 70.33,-42.65 70.16,-43.59 69.89,-44.53 69.53,-45.47 69.07,-46.41 68.52,-47.35 67.89,-48.28 67.18,-49.2 66.4,-50.11 65.55,-51.01 64.63,-51.89 63.65,-52.75 62.62,-53.59 61.53,-54.41 60.4,-55.2 59.23,-55.96 58.02,-56.69 56.78,-57.39 55.5,-58.06 54.2,-58.69 52.88,-59.29 51.53,-59.84 50.17,-60.35 48.79,-60.82 47.4,-61.25 46,-61.64 44.59,-61.97 43.17,-62.26 41.75,-62.51 40.32,-62.7 38.89,-62.85 37.45,-62.95 36.02,-63 34.58,-63 33.15,-62.95 31.71,-62.85 30.28,-62.7 28.85,-62.51 27.43,-62.26 26.01,-61.97 24.6,-61.64 23.2,-61.25 21.81,-60.82 20.43,-60.35 19.07,-59.84 17.72,-59.29 16.4,-58.69 15.1,-58.06 13.82,-57.39 12.58,-56.69 11.37,-55.96 10.2,-55.2 9.07,-54.41 7.98,-53.59 6.95,-52.75 5.97,-51.89 5.05,-51.01 4.2,-50.11 3.42,-49.2 2.71,-48.28 2.08,-47.35 1.53,-46.41 1.07,-45.47 0.71,-44.53 0.44,-43.59 0.27,-42.65 0.2,-41.72 0.24,-40.8 0.39,-39.89 0.65,-38.99 1.02,-38.11 1.51,-37.25 2.11,-36.41 2.83,-35.59 3.66,-34.8 4.61,-34.04 5.67,-33.31 6.84,-32.61 8.11,-31.94 9.49,-31.31 10.97,-30.71 12.54,-30.16 14.2,-29.65 15.94,-29.18 17.76,-28.75 19.65,-28.36 21.61,-28.03 23.62,-27.74 25.68,-27.49 27.77,-27.3 29.9,-27.15 32.05,-27.05 34.22,-27 36.38,-27 38.55,-27.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- DateExtractor -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>DateExtractor</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"183.94\" cy=\"-45\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"183.94\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">DateExtractor</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;DateExtractor -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;DateExtractor</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.94,-45C78.02,-45 87.01,-45 96.29,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.46,-48.5 106.45,-45 96.45,-41.5 96.46,-48.5\"/>\n",
       "</g>\n",
       "<!-- MapValues -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>MapValues</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"359.03\" cy=\"-45\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"359.03\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">MapValues</text>\n",
       "</g>\n",
       "<!-- DateExtractor&#45;&gt;MapValues -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>DateExtractor&#45;&gt;MapValues</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.36,-45C269.86,-45 278.49,-45 286.92,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.08,-48.5 297.08,-45 287.08,-41.5 287.08,-48.5\"/>\n",
       "</g>\n",
       "<!-- OneHotEncoder -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>OneHotEncoder</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"541.92\" cy=\"-45\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"541.92\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- MapValues&#45;&gt;OneHotEncoder -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>MapValues&#45;&gt;OneHotEncoder</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.02,-45C429.27,-45 437.9,-45 446.57,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"446.75,-48.5 456.75,-45 446.75,-41.5 446.75,-48.5\"/>\n",
       "</g>\n",
       "<!-- parquet/parquet -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.06,-86.73C736.06,-88.53 719.7,-90 699.56,-90 679.42,-90 663.06,-88.53 663.06,-86.73 663.06,-86.73 663.06,-57.27 663.06,-57.27 663.06,-55.47 679.42,-54 699.56,-54 719.7,-54 736.06,-55.47 736.06,-57.27 736.06,-57.27 736.06,-86.73 736.06,-86.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.06,-86.73C736.06,-84.92 719.7,-83.45 699.56,-83.45 679.42,-83.45 663.06,-84.92 663.06,-86.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"699.56\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">parquet</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;parquet/parquet -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M608.47,-56.37C623.3,-58.94 638.74,-61.62 652.56,-64.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"652.33,-67.53 662.78,-65.79 653.52,-60.64 652.33,-67.53\"/>\n",
       "</g>\n",
       "<!-- nosql/nosql -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M727.06,-32.73C727.06,-34.53 714.73,-36 699.56,-36 684.39,-36 672.06,-34.53 672.06,-32.73 672.06,-32.73 672.06,-3.27 672.06,-3.27 672.06,-1.47 684.39,0 699.56,0 714.73,0 727.06,-1.47 727.06,-3.27 727.06,-3.27 727.06,-32.73 727.06,-32.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M727.06,-32.73C727.06,-30.92 714.73,-29.45 699.56,-29.45 684.39,-29.45 672.06,-30.92 672.06,-32.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"699.56\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">nosql</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;nosql/nosql -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M608.47,-33.63C626.74,-30.46 645.93,-27.13 661.88,-24.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"662.6,-27.79 671.85,-22.63 661.4,-20.89 662.6,-27.79\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f92c153cac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and add value mapping\n",
    "main_categories = [\"es_transportation\", \"es_health\", \"es_otherservices\",\n",
    "       \"es_food\", \"es_hotelservices\", \"es_barsandrestaurants\",\n",
    "       \"es_tech\", \"es_sportsandtoys\", \"es_wellnessandbeauty\",\n",
    "       \"es_hyper\", \"es_fashion\", \"es_home\", \"es_contents\",\n",
    "       \"es_travel\", \"es_leisure\"]\n",
    "\n",
    "# One Hot Encode the newly defined mappings\n",
    "one_hot_encoder_mapping = {'category': main_categories,\n",
    "                           'gender': list(transactions_data.gender.unique())}\n",
    "\n",
    "# Define the graph steps\n",
    "transaction_set.graph\\\n",
    "    .to(DateExtractor(parts = ['hour','month'], timestamp_col = 'timestamp'))\\\n",
    "    .to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\\\n",
    "    .to(OneHotEncoder(mapping=one_hot_encoder_mapping))\n",
    "\n",
    "\n",
    "# # Add aggregations for 2, 12, and 24 hour time windows\n",
    "# transaction_set.add_aggregation(name='amount',\n",
    "#                                 column='amount',\n",
    "#                                 operations=['avg','sum', 'count','max'],\n",
    "#                                 windows=['12h'],\n",
    "#                                 period='1h')\n",
    "\n",
    "\n",
    "# # Add the category aggregations over a 14 day window\n",
    "# for category in main_categories:\n",
    "#     transaction_set.add_aggregation(name=category,column=f'category_{category}',\n",
    "#                                     operations=['count'], windows=['14d'], period='1d')\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "transaction_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "transaction_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running with a Spark operator, the MLRun execution details are returned, allowing tracking of the jobâ€™s status and results.\n",
    "Spark operator ingestion is always executed remotely.\n",
    "\n",
    "The cell below should be executed only once to build the spark job image before running the first ingest. It may take a few minutes to prepare the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.runtimes import RemoteSparkRuntime\n",
    "# RemoteSparkRuntime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlrun: start-code\n",
    "\n",
    "from mlrun.feature_store.api import ingest\n",
    "def ingest_handler(context):\n",
    "    ingest(mlrun_context=context) # The handler function must call ingest with the mlrun_context\n",
    "        \n",
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = mlrun.code_to_function(name='transactions_func', kind='spark', handler=\"ingest_handler\")\n",
    "\n",
    "function.with_driver_requests(cpu=\"1000m\", mem=\"4G\")\n",
    "function.with_driver_limits(cpu=\"1000\")\n",
    "function.with_executor_requests(cpu=\"500m\", mem=\"2G\")\n",
    "function.with_executor_limits(cpu=\"500m\")\n",
    "function.spec.replicas = 3\n",
    "function.with_igz_spark()\n",
    "function.spec.use_default_image = True\n",
    "\n",
    "run_config = fstore.RunConfig(function=function, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:43:32,371 [info] starting run transactions-ingest uid=7c498d1fd0514934aa5ce18643b51a30 DB=http://mlrun-api:8080\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.3.1.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=10.201.8.188 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner local:///etc/config/mlrun/spark-function-code.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2023-02-01 06:43:42,667 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2023-02-01 06:43:45,870 [info] starting ingestion task to store://feature-sets/fraud-demo-dani/transactions:latest.\n",
      "2023-02-01 06:43:46,493 INFO spark.SparkContext: Running Spark version 3.2.1\n",
      "2023-02-01 06:43:46,527 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-02-01 06:43:46,528 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-02-01 06:43:46,528 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-02-01 06:43:46,528 INFO spark.SparkContext: Submitted application: transactions-ingest-7c498d1fd0514934aa5ce18643b51a30\n",
      "2023-02-01 06:43:46,557 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-02-01 06:43:46,575 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2023-02-01 06:43:46,578 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-02-01 06:43:46,650 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2023-02-01 06:43:46,650 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2023-02-01 06:43:46,651 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-02-01 06:43:46,651 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-02-01 06:43:46,652 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2023-02-01 06:43:46,969 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2023-02-01 06:43:46,998 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-02-01 06:43:47,037 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-02-01 06:43:47,064 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-02-01 06:43:47,065 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-02-01 06:43:47,069 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-02-01 06:43:47,097 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-ca63be48-6786-45f8-8ae3-ad76fc6940a7/blockmgr-773eaa17-0977-48ad-845a-1bb7d43b6cfa\n",
      "2023-02-01 06:43:47,130 INFO memory.MemoryStore: MemoryStore started with capacity 2.2 GiB\n",
      "2023-02-01 06:43:47,149 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-02-01 06:43:47,258 INFO util.log: Logging initialized @6675ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-02-01 06:43:47,346 INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.11+9-LTS\n",
      "2023-02-01 06:43:47,369 INFO server.Server: Started @6787ms\n",
      "2023-02-01 06:43:47,409 INFO server.AbstractConnector: Started ServerConnector@95d03c7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-02-01 06:43:47,409 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-02-01 06:43:47,436 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6db968e7{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,439 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ccb55f1{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,440 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c704284{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,442 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d3e3d14{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,442 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@547f618d{/stages,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,443 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d6c0b1a{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,444 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15c23819{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,446 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65c610ad{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,447 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56077b97{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,448 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52a0b9e2{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,449 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65cdb25e{/storage,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,450 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@166c82c{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,451 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c47ef18{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,451 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d0aeb1f{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,452 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d51fd86{/environment,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,453 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e74c6d4{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,454 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d34a820{/executors,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,455 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15f83934{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,456 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c83806b{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,457 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25c8843d{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,467 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23cb1a7d{/static,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,468 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@a594972{/,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,469 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fb97f80{/api,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,470 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f9a7cc1{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,471 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58748fc6{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:43:47,473 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:4040\n",
      "2023-02-01 06:43:47,489 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1675233826483\n",
      "2023-02-01 06:43:47,489 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1675233826483\n",
      "2023-02-01 06:43:47,489 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1675233826483\n",
      "2023-02-01 06:43:47,489 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1675233826483\n",
      "2023-02-01 06:43:47,489 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1675233826483\n",
      "2023-02-01 06:43:47,491 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2023-02-01 06:43:47,591 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2023-02-01 06:43:48,954 INFO k8s.ExecutorPodsAllocator: Going to request 3 executors from Kubernetes for ResourceProfile Id: 0, target: 3, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
      "2023-02-01 06:43:49,052 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 06:43:49,475 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 06:43:49,485 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2023-02-01 06:43:49,486 INFO netty.NettyBlockTransferService: Server created on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079\n",
      "2023-02-01 06:43:49,488 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-02-01 06:43:49,500 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 06:43:49,506 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2023-02-01 06:43:49,509 INFO storage.BlockManagerMasterEndpoint: Registering block manager transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 with 2.2 GiB RAM, BlockManagerId(driver, transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 06:43:49,512 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 06:43:49,513 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc, 7079, None)\n",
      "2023-02-01 06:43:49,555 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@395ca8ee{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:07,607 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.136.123:46116) with ID 1,  ResourceProfileId 0\n",
      "2023-02-01 06:44:07,906 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.136.123:40068 with 1007.8 MiB RAM, BlockManagerId(1, 10.200.136.123, 40068, None)\n",
      "2023-02-01 06:44:07,988 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.140.92:40614) with ID 3,  ResourceProfileId 0\n",
      "2023-02-01 06:44:08,369 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.140.92:41805 with 1007.8 MiB RAM, BlockManagerId(3, 10.200.140.92, 41805, None)\n",
      "2023-02-01 06:44:08,675 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.140.91:60848) with ID 2,  ResourceProfileId 0\n",
      "2023-02-01 06:44:08,720 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2023-02-01 06:44:09,177 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.140.91:39504 with 1007.8 MiB RAM, BlockManagerId(2, 10.200.140.91, 39504, None)\n",
      "2023-02-01 06:44:09,384 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "2023-02-01 06:44:09,412 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2023-02-01 06:44:09,429 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aea4353{/SQL,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:09,430 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@574e7d8b{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:09,431 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44cd3dc{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:09,432 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d9829ba{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:09,455 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@293d127b{/static/sql,null,AVAILABLE,@Spark}\n",
      "2023-02-01 06:44:11,285 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "2023-02-01 06:44:12,215 INFO datasources.InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.\n",
      "2023-02-01 06:44:12,393 INFO datasources.InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
      "2023-02-01 06:44:15,031 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:15,034 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "2023-02-01 06:44:15,037 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-02-01 06:44:15,593 INFO codegen.CodeGenerator: Code generated in 223.479716 ms\n",
      "2023-02-01 06:44:15,658 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:15,734 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:15,737 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:15,750 INFO spark.SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:15,762 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:15,900 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:15,918 INFO scheduler.DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-02-01 06:44:15,918 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 06:44:15,919 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:44:15,920 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:44:15,925 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 06:44:15,997 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:16,008 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:16,009 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:16,010 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:44:16,027 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 06:44:16,028 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "2023-02-01 06:44:16,083 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.200.136.123, executor 1, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:44:16,695 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.200.136.123:40068 (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:20,598 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.136.123:40068 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:27,679 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11609 ms on 10.200.136.123 (executor 1) (1/1)\n",
      "2023-02-01 06:44:27,682 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:44:27,687 INFO scheduler.DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 11.746 s\n",
      "2023-02-01 06:44:27,691 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:44:27,691 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "2023-02-01 06:44:27,693 INFO scheduler.DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 11.792997 s\n",
      "2023-02-01 06:44:27,720 INFO codegen.CodeGenerator: Code generated in 13.654186 ms\n",
      "2023-02-01 06:44:27,783 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 5.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:27,788 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:27,788 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:27,788 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "2023-02-01 06:44:27,798 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:27,799 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.200.136.123:40068 in memory (size: 5.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:27,827 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 54.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:27,828 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:27,829 INFO spark.SparkContext: Created broadcast 2 from load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:27,830 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:27,906 INFO spark.SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:27,912 INFO scheduler.DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 06:44:27,912 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 06:44:27,912 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:44:27,913 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:44:27,914 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 06:44:27,943 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:27,952 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:27,952 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 10.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:27,953 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:44:27,954 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 06:44:27,954 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "2023-02-01 06:44:27,955 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.200.140.91, executor 2, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:44:27,956 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.200.140.92, executor 3, partition 1, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:44:28,771 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.140.92:41805 (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:28,873 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.200.140.91:39504 (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:30,437 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:30,438 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.200.136.123:40068 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:38,784 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.140.92:41805 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:39,679 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.200.140.91:39504 (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:47,292 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 19336 ms on 10.200.140.92 (executor 3) (1/2)\n",
      "2023-02-01 06:44:48,074 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 20119 ms on 10.200.140.91 (executor 2) (2/2)\n",
      "2023-02-01 06:44:48,074 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:44:48,075 INFO scheduler.DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 20.160 s\n",
      "2023-02-01 06:44:48,075 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:44:48,076 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-02-01 06:44:48,076 INFO scheduler.DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 20.169876 s\n",
      "2023-02-01 06:44:48,163 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:48,163 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:48,163 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:44:48,230 INFO codegen.CodeGenerator: Code generated in 35.375392 ms\n",
      "2023-02-01 06:44:48,236 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,251 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 10.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:48,255 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.140.92:41805 in memory (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:48,255 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.200.140.91:39504 in memory (size: 10.1 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:48,263 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,264 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:48,265 INFO spark.SparkContext: Created broadcast 4 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:48,269 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:48,374 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:48,374 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:48,374 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:44:48,429 INFO codegen.CodeGenerator: Code generated in 36.237368 ms\n",
      "2023-02-01 06:44:48,435 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,465 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,467 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:48,468 INFO spark.SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:48,469 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:48,470 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:48,471 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.200.140.92:41805 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:48,472 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.200.140.91:39504 in memory (size: 54.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:48,629 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:48,630 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:48,630 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:44:48,678 INFO codegen.CodeGenerator: Code generated in 34.275526 ms\n",
      "2023-02-01 06:44:48,684 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,707 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:48,708 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:48,709 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:48,710 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:49,462 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:49,462 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:49,462 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:44:49,543 INFO codegen.CodeGenerator: Code generated in 50.816327 ms\n",
      "2023-02-01 06:44:49,547 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:49,567 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:49,574 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:49,575 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:49,576 INFO spark.SparkContext: Created broadcast 7 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:49,577 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:50,060 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:44:50,060 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:44:50,061 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:44:50,240 INFO codegen.CodeGenerator: Code generated in 48.981566 ms\n",
      "2023-02-01 06:44:50,245 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:50,277 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:50,278 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:50,279 INFO spark.SparkContext: Created broadcast 8 from summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:44:50,280 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:44:50,298 INFO scheduler.DAGScheduler: Registering RDD 38 (summary at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "2023-02-01 06:44:50,303 INFO scheduler.DAGScheduler: Got map stage job 2 (summary at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 06:44:50,303 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 06:44:50,303 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:44:50,304 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:44:50,306 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[38] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 06:44:50,509 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 136.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:50,528 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 41.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:44:50,529 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 41.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:44:50,532 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:44:50,534 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[38] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 06:44:50,534 INFO scheduler.TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "2023-02-01 06:44:50,546 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (10.200.136.123, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:44:50,546 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (10.200.140.91, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:44:50,600 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.136.123:40068 (size: 41.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:50,674 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.200.140.91:39504 (size: 41.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:52,369 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.140.91:39504 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:44:52,691 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.200.136.123:40068 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:11,679 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 21133 ms on 10.200.140.91 (executor 2) (1/2)\n",
      "2023-02-01 06:45:15,199 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 24654 ms on 10.200.136.123 (executor 1) (2/2)\n",
      "2023-02-01 06:45:15,199 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:45:15,200 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (summary at NativeMethodAccessorImpl.java:0) finished in 24.891 s\n",
      "2023-02-01 06:45:15,201 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 06:45:15,201 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 06:45:15,202 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 06:45:15,202 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 06:45:15,290 INFO spark.SparkContext: Starting job: summary at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:45:15,292 INFO scheduler.DAGScheduler: Got job 3 (summary at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "2023-02-01 06:45:15,292 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 06:45:15,292 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "2023-02-01 06:45:15,292 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:45:15,293 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[41] at summary at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 06:45:15,312 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 287.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:45:15,320 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 69.6 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:45:15,321 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 41.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:45:15,321 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 69.6 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:45:15,321 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:45:15,322 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[41] at summary at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 06:45:15,322 INFO scheduler.TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "2023-02-01 06:45:15,324 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.200.140.91:39504 in memory (size: 41.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:15,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (10.200.136.123, executor 1, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:45:15,386 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.200.136.123:40068 in memory (size: 41.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:15,487 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.200.136.123:40068 (size: 69.6 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:45:15,706 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.200.136.123:46116\n",
      "2023-02-01 06:45:20,008 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 4682 ms on 10.200.136.123 (executor 1) (1/1)\n",
      "2023-02-01 06:45:20,008 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:45:20,009 INFO scheduler.DAGScheduler: ResultStage 4 (summary at NativeMethodAccessorImpl.java:0) finished in 4.709 s\n",
      "2023-02-01 06:45:20,009 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:45:20,009 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "2023-02-01 06:45:20,009 INFO scheduler.DAGScheduler: Job 3 finished: summary at NativeMethodAccessorImpl.java:0, took 4.719412 s\n",
      "2023-02-01 06:45:20,058 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2023-02-01 06:45:20,109 INFO codegen.CodeGenerator: Code generated in 32.544492 ms\n",
      "2023-02-01 06:45:20,448 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 69.6 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:45:20,487 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.200.136.123:40068 in memory (size: 69.6 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:21,383 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.136.123:40068 in memory (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:21,393 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.200.140.91:39504 in memory (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:45:21,401 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:45:21,902 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:45:21,988 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:24,972 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:46:24,972 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:46:24,973 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, gender: string, zipcodeOri: int, zipMerchant: int ... 7 more fields>\n",
      "2023-02-01 06:46:25,055 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:25,071 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:25,072 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:25,073 INFO spark.SparkContext: Created broadcast 11 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 06:46:25,074 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:46:25,126 INFO scheduler.DAGScheduler: Registering RDD 46 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) as input to shuffle 1\n",
      "2023-02-01 06:46:25,127 INFO scheduler.DAGScheduler: Got map stage job 4 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 2 output partitions\n",
      "2023-02-01 06:46:25,127 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 5 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 06:46:25,127 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:46:25,127 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:46:25,128 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[46] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 06:46:25,135 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 105.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:25,137 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:25,138 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 27.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:25,138 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:46:25,139 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[46] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 06:46:25,139 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "2023-02-01 06:46:25,140 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (10.200.136.123, executor 1, partition 0, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:25,141 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (10.200.140.91, executor 2, partition 1, PROCESS_LOCAL, 4896 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:25,155 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.200.136.123:40068 (size: 27.2 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:46:25,156 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.200.140.91:39504 (size: 27.2 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:46:26,484 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.200.136.123:40068 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:46:26,485 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.200.140.91:39504 (size: 54.8 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:46:30,994 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 5854 ms on 10.200.140.91 (executor 2) (1/2)\n",
      "2023-02-01 06:46:33,656 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 8516 ms on 10.200.136.123 (executor 1) (2/2)\n",
      "2023-02-01 06:46:33,656 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:46:33,657 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 8.529 s\n",
      "2023-02-01 06:46:33,657 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-02-01 06:46:33,657 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-02-01 06:46:33,657 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "2023-02-01 06:46:33,657 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-02-01 06:46:33,725 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104\n",
      "2023-02-01 06:46:33,726 INFO scheduler.DAGScheduler: Got job 5 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) with 1 output partitions\n",
      "2023-02-01 06:46:33,726 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104)\n",
      "2023-02-01 06:46:33,726 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "2023-02-01 06:46:33,726 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:46:33,727 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[49] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104), which has no missing parents\n",
      "2023-02-01 06:46:33,737 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 259.4 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:33,739 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 51.7 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:33,740 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 51.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:33,741 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:46:33,741 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[49] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 06:46:33,741 INFO scheduler.TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "2023-02-01 06:46:33,742 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8) (10.200.140.91, executor 2, partition 0, NODE_LOCAL, 4472 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:33,758 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.200.140.91:39504 (size: 51.7 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:46:34,068 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.200.140.91:60848\n",
      "2023-02-01 06:46:36,372 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 2630 ms on 10.200.140.91 (executor 2) (1/1)\n",
      "2023-02-01 06:46:36,372 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:46:36,373 INFO scheduler.DAGScheduler: ResultStage 7 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104) finished in 2.645 s\n",
      "2023-02-01 06:46:36,373 INFO scheduler.DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:46:36,373 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "2023-02-01 06:46:36,374 INFO scheduler.DAGScheduler: Job 5 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:104, took 2.648397 s\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "2023-02-01 06:46:36,899 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:46:36,900 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:46:36,900 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:46:36,919 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:36,935 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:36,936 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:36,936 INFO spark.SparkContext: Created broadcast 14 from toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 06:46:36,937 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:46:36,946 INFO spark.SparkContext: Starting job: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67\n",
      "2023-02-01 06:46:36,946 INFO scheduler.DAGScheduler: Got job 6 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) with 1 output partitions\n",
      "2023-02-01 06:46:36,946 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67)\n",
      "2023-02-01 06:46:36,946 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:46:36,947 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:46:36,947 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[53] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67), which has no missing parents\n",
      "2023-02-01 06:46:36,950 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 30.6 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:36,952 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:36,952 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 11.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:36,953 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:46:36,953 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[53] at toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) (first 15 tasks are for partitions Vector(0))\n",
      "2023-02-01 06:46:36,953 INFO scheduler.TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "2023-02-01 06:46:36,954 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9) (10.200.140.91, executor 2, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:36,976 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.200.140.91:39504 (size: 11.1 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:46:37,172 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.200.140.91:39504 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:46:37,599 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 645 ms on 10.200.140.91 (executor 2) (1/1)\n",
      "2023-02-01 06:46:37,599 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:46:37,600 INFO scheduler.DAGScheduler: ResultStage 8 (toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67) finished in 0.651 s\n",
      "2023-02-01 06:46:37,600 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:46:37,600 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "2023-02-01 06:46:37,600 INFO scheduler.DAGScheduler: Job 6 finished: toPandas at /conda/lib/python3.9/site-packages/mlrun/data_types/spark.py:67, took 0.654301 s\n",
      "> 2023-02-01 06:46:37,632 [info] writing to target parquet, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/transactions/parquet/sets/transactions/1675233997632_442/', 'format': 'parquet', 'partitionBy': ['year', 'month', 'day', 'hour']}\n",
      "2023-02-01 06:46:37,744 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:46:37,744 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:46:37,744 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:46:37,820 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 06:46:37,837 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-02-01 06:46:37,837 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-02-01 06:46:37,839 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 06:46:37,839 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-02-01 06:46:37,839 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-02-01 06:46:37,840 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2023-02-01 06:46:38,061 INFO codegen.CodeGenerator: Code generated in 37.764122 ms\n",
      "2023-02-01 06:46:38,065 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:38,081 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:38,081 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:38,082 INFO spark.SparkContext: Created broadcast 16 from save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:46:38,083 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:46:38,127 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2023-02-01 06:46:38,128 INFO scheduler.DAGScheduler: Got job 7 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2023-02-01 06:46:38,128 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (save at NativeMethodAccessorImpl.java:0)\n",
      "2023-02-01 06:46:38,128 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:46:38,128 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:46:38,129 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[57] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-02-01 06:46:38,175 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 359.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:38,178 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 131.9 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:46:38,178 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 131.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:38,179 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:46:38,179 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[57] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 06:46:38,179 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "2023-02-01 06:46:38,180 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 10) (10.200.140.92, executor 3, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:38,181 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 11) (10.200.140.91, executor 2, partition 1, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:46:38,194 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.200.140.91:39504 (size: 131.9 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 06:46:38,371 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.200.140.92:41805 (size: 131.9 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:46:39,180 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.200.140.91:39504 (size: 54.8 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 06:46:40,371 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.200.140.92:41805 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:46:40,462 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 51.7 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:40,470 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.200.140.91:39504 in memory (size: 51.7 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 06:46:40,474 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 27.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:40,476 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.200.140.91:39504 in memory (size: 27.2 KiB, free: 1007.5 MiB)\n",
      "2023-02-01 06:46:40,476 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.200.136.123:40068 in memory (size: 27.2 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:46:40,481 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 11.1 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:46:40,483 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.200.140.91:39504 in memory (size: 11.1 KiB, free: 1007.6 MiB)\n",
      "2023-02-01 06:47:04,305 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 11) in 26125 ms on 10.200.140.91 (executor 2) (1/2)\n",
      "2023-02-01 06:47:07,373 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 10) in 29193 ms on 10.200.140.92 (executor 3) (2/2)\n",
      "2023-02-01 06:47:07,374 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:47:07,374 INFO scheduler.DAGScheduler: ResultStage 9 (save at NativeMethodAccessorImpl.java:0) finished in 29.245 s\n",
      "2023-02-01 06:47:07,374 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:47:07,375 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "2023-02-01 06:47:07,375 INFO scheduler.DAGScheduler: Job 7 finished: save at NativeMethodAccessorImpl.java:0, took 29.247946 s\n",
      "2023-02-01 06:47:07,377 INFO datasources.FileFormatWriter: Start to commit write Job 4a1d4538-f6ba-4e94-b64c-4002f3692a83.\n",
      "2023-02-01 06:47:08,370 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 131.9 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:47:08,373 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.200.140.91:39504 in memory (size: 131.9 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:08,374 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.200.140.92:41805 in memory (size: 131.9 KiB, free: 1007.8 MiB)\n",
      "2023-02-01 06:47:10,576 INFO datasources.FileFormatWriter: Write Job 4a1d4538-f6ba-4e94-b64c-4002f3692a83 committed. Elapsed time: 3198 ms.\n",
      "2023-02-01 06:47:10,581 INFO datasources.FileFormatWriter: Finished processing stats for write job 4a1d4538-f6ba-4e94-b64c-4002f3692a83.\n",
      "> 2023-02-01 06:47:10,592 [info] writing to target nosql, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/transactions/nosql/sets/transactions/1675233997632_442/', 'format': 'io.iguaz.v3io.spark.sql.kv', 'key': 'source'}\n",
      "2023-02-01 06:47:11,201 INFO datasources.FileSourceStrategy: Pushed Filters: \n",
      "2023-02-01 06:47:11,201 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "2023-02-01 06:47:11,201 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: int, step: int, age: string, gender: string, zipcodeOri: int ... 11 more fields>\n",
      "2023-02-01 06:47:11,219 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 331.3 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:47:11,235 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 54.8 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:47:11,235 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:47:11,236 INFO spark.SparkContext: Created broadcast 18 from foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 06:47:11,237 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "2023-02-01 06:47:11,276 INFO spark.SparkContext: Starting job: foreachPartition at KVRelation.scala:125\n",
      "2023-02-01 06:47:11,277 INFO scheduler.DAGScheduler: Got job 8 (foreachPartition at KVRelation.scala:125) with 2 output partitions\n",
      "2023-02-01 06:47:11,277 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (foreachPartition at KVRelation.scala:125)\n",
      "2023-02-01 06:47:11,277 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2023-02-01 06:47:11,277 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-02-01 06:47:11,278 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[63] at foreachPartition at KVRelation.scala:125), which has no missing parents\n",
      "2023-02-01 06:47:11,291 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.1 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:47:11,296 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 86.2 KiB, free 2.2 GiB)\n",
      "2023-02-01 06:47:11,297 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 (size: 86.2 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:47:11,297 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1478\n",
      "2023-02-01 06:47:11,298 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[63] at foreachPartition at KVRelation.scala:125) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-02-01 06:47:11,298 INFO scheduler.TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
      "2023-02-01 06:47:11,299 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 12) (10.200.136.123, executor 1, partition 0, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:47:11,299 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 13) (10.200.140.92, executor 3, partition 1, PROCESS_LOCAL, 4907 bytes) taskResourceAssignments Map()\n",
      "2023-02-01 06:47:11,313 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.200.136.123:40068 (size: 86.2 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:11,313 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.200.140.92:41805 (size: 86.2 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:12,169 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.200.140.92:41805 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:17,687 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.200.136.123:40068 (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:20,504 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:7079 in memory (size: 54.8 KiB, free: 2.2 GiB)\n",
      "2023-02-01 06:47:20,505 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.200.140.91:39504 in memory (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:20,570 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.200.140.92:41805 in memory (size: 54.8 KiB, free: 1007.7 MiB)\n",
      "2023-02-01 06:47:29,872 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 13) in 18573 ms on 10.200.140.92 (executor 3) (1/2)\n",
      "2023-02-01 06:47:47,189 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 12) in 35888 ms on 10.200.136.123 (executor 1) (2/2)\n",
      "2023-02-01 06:47:47,189 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "2023-02-01 06:47:47,189 INFO scheduler.DAGScheduler: ResultStage 10 (foreachPartition at KVRelation.scala:125) finished in 35.910 s\n",
      "2023-02-01 06:47:47,189 INFO scheduler.DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-02-01 06:47:47,190 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "2023-02-01 06:47:47,190 INFO scheduler.DAGScheduler: Job 8 finished: foreachPartition at KVRelation.scala:125, took 35.913267 s\n",
      "> 2023-02-01 06:47:47,499 [info] ingestion task completed, targets:\n",
      "> 2023-02-01 06:47:47,499 [info] [{'name': 'parquet', 'kind': 'parquet', 'path': 'v3io:///projects/fraud-demo-dani/FeatureStore/transactions/parquet/sets/transactions/{run_id}/', 'status': 'ready', 'updated': '2023-02-01T06:47:10.592677+00:00', 'run_id': '1675233997632_442', 'partitioned': True}, {'name': 'nosql', 'kind': 'nosql', 'path': 'v3io:///projects/fraud-demo-dani/FeatureStore/transactions/nosql/sets/transactions/{run_id}/', 'status': 'ready', 'updated': '2023-02-01T06:47:47.392413+00:00', 'run_id': '1675233997632_442', 'partitioned': False}]\n",
      "2023-02-01 06:47:47,511 INFO server.AbstractConnector: Stopped Spark@95d03c7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-02-01 06:47:47,513 INFO ui.SparkUI: Stopped Spark web UI at http://transactions-ingest-e5cc8a0a-489ae1860bb767db-driver-svc.default-tenant.svc:4040\n",
      "2023-02-01 06:47:47,517 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "2023-02-01 06:47:47,517 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "2023-02-01 06:47:47,524 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n",
      "2023-02-01 06:47:47,730 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-02-01 06:47:47,745 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-02-01 06:47:47,745 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-02-01 06:47:47,748 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-02-01 06:47:47,753 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-02-01 06:47:47,760 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "> 2023-02-01 06:47:48,026 [info] To track results use the CLI: {'info_cmd': 'mlrun get run 7c498d1fd0514934aa5ce18643b51a30 -p fraud-demo-dani', 'logs_cmd': 'mlrun logs 7c498d1fd0514934aa5ce18643b51a30 -p fraud-demo-dani'}\n",
      "> 2023-02-01 06:47:48,026 [info] Or click for UI: {'ui_url': 'https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/7c498d1fd0514934aa5ce18643b51a30/overview'}\n",
      "> 2023-02-01 06:47:48,027 [info] run executed, status=completed\n",
      "2023-02-01 06:47:48,574 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2023-02-01 06:47:48,575 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-e84c4d1a-e74c-457a-8b8e-a97ea48ff170\n",
      "2023-02-01 06:47:48,579 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-ca63be48-6786-45f8-8ae3-ad76fc6940a7/spark-0a041890-ca53-4274-a3d9-3c418e5a8625\n",
      "2023-02-01 06:47:48,583 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-ca63be48-6786-45f8-8ae3-ad76fc6940a7/spark-0a041890-ca53-4274-a3d9-3c418e5a8625/pyspark-ee9fda22-d013-4a0d-9213-38163e99684e\n",
      "final state: completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>fraud-demo-dani</td>\n",
       "      <td><div title=\"7c498d1fd0514934aa5ce18643b51a30\"><a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/7c498d1fd0514934aa5ce18643b51a30/overview\" target=\"_blank\" >...43b51a30</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Feb 01 06:43:45</td>\n",
       "      <td>completed</td>\n",
       "      <td>transactions-ingest</td>\n",
       "      <td><div class=\"dictlist\">job-type=feature-ingest</div><div class=\"dictlist\">feature-set=store://feature-sets/fraud-demo-dani/transactions</div><div class=\"dictlist\">v3io_user=dani</div><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=dani</div><div class=\"dictlist\">mlrun/client_version=1.3.0-rc16</div><div class=\"dictlist\">mlrun/job=transactions-ingest-e5cc8a0a</div><div class=\"dictlist\">host=transactions-ingest-e5cc8a0a-driver</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">infer_options=63</div><div class=\"dictlist\">overwrite=True</div><div class=\"dictlist\">featureset=store://feature-sets/fraud-demo-dani/transactions</div><div class=\"dictlist\">source={'kind': 'csv', 'name': 'flow1_source', 'path': 'v3io:///projects/fraud-demo-dani/artifacts/transactions_data.csv'}</div><div class=\"dictlist\">targets=None</div></td>\n",
       "      <td><div class=\"dictlist\">featureset=store://feature-sets/fraud-demo-dani/transactions:latest</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resultc9713c4c-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resultc9713c4c-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resultc9713c4c\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resultc9713c4c-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.vmdev94.lab.iguazeng.com/mlprojects/fraud-demo-dani/jobs/monitor/7c498d1fd0514934aa5ce18643b51a30/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:47:51,857 [info] run executed, status=completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7f92c94f1550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore.sources import CSVSource\n",
    "\n",
    "# Creating our partitioned csv source\n",
    "transaction_source = CSVSource(\"flow1_source\", path=project.get_artifact('transactions_data').target_path)\n",
    "\n",
    "# Ingest our transactions dataset through our defined pipeline\n",
    "fstore.ingest(transaction_set, transaction_source, run_config=run_config, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ingestion process, you will be able to see all the different features that were created with the help of the UI, as you can see in the image below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Features Catalog - fraud prevention](./images/features-catalog-transaction.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>step</th>\n",
       "      <th>age</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>...</th>\n",
       "      <th>category_es_wellnessandbeauty</th>\n",
       "      <th>category_es_hyper</th>\n",
       "      <th>category_es_fashion</th>\n",
       "      <th>category_es_home</th>\n",
       "      <th>category_es_contents</th>\n",
       "      <th>category_es_travel</th>\n",
       "      <th>category_es_leisure</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333712</td>\n",
       "      <td>108</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>87.45</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 10:23:34.919033</td>\n",
       "      <td>C100045114</td>\n",
       "      <td>M78078399</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324357</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>53.13</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 10:16:00.598159</td>\n",
       "      <td>C100045114</td>\n",
       "      <td>M1649169323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12711</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>25.21</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 10:48:49.596127</td>\n",
       "      <td>C1000699316</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169798</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>326.34</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 10:24:26.761086</td>\n",
       "      <td>C1000699316</td>\n",
       "      <td>M1053599405</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139020</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>14.36</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 10:07:15.601054</td>\n",
       "      <td>C1002658784</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>67947</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>29.24</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 06:33:09.751606</td>\n",
       "      <td>C1188590334</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>459179</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>49.32</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 06:24:04.623460</td>\n",
       "      <td>C1189224644</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>146895</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>58.99</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 06:18:59.224509</td>\n",
       "      <td>C1189231170</td>\n",
       "      <td>M1946091778</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>9022</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>53.52</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 06:30:00.266290</td>\n",
       "      <td>C1189367947</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>237058</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>28007</td>\n",
       "      <td>28007</td>\n",
       "      <td>38.43</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 06:42:26.327731</td>\n",
       "      <td>C1189367947</td>\n",
       "      <td>M348934600</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          _c0  step age  zipcodeOri  zipMerchant  amount  fraud  \\\n",
       "0      333712   108   4       28007        28007   87.45      0   \n",
       "1      324357   105   4       28007        28007   53.13      0   \n",
       "2       12711     5   4       28007        28007   25.21      0   \n",
       "3      169798    59   4       28007        28007  326.34      0   \n",
       "4      139020    49   3       28007        28007   14.36      0   \n",
       "...       ...   ...  ..         ...          ...     ...    ...   \n",
       "49995   67947    25   6       28007        28007   29.24      0   \n",
       "49996  459179   143   2       28007        28007   49.32      0   \n",
       "49997  146895    52   3       28007        28007   58.99      0   \n",
       "49998    9022     3   2       28007        28007   53.52      0   \n",
       "49999  237058    80   2       28007        28007   38.43      0   \n",
       "\n",
       "                       timestamp       source       target  ...  \\\n",
       "0     2023-01-30 10:23:34.919033   C100045114    M78078399  ...   \n",
       "1     2023-01-30 10:16:00.598159   C100045114  M1649169323  ...   \n",
       "2     2023-01-30 10:48:49.596127  C1000699316  M1823072687  ...   \n",
       "3     2023-01-30 10:24:26.761086  C1000699316  M1053599405  ...   \n",
       "4     2023-01-30 10:07:15.601054  C1002658784   M348934600  ...   \n",
       "...                          ...          ...          ...  ...   \n",
       "49995 2023-02-01 06:33:09.751606  C1188590334   M348934600  ...   \n",
       "49996 2023-02-01 06:24:04.623460  C1189224644  M1823072687  ...   \n",
       "49997 2023-02-01 06:18:59.224509  C1189231170  M1946091778  ...   \n",
       "49998 2023-02-01 06:30:00.266290  C1189367947   M348934600  ...   \n",
       "49999 2023-02-01 06:42:26.327731  C1189367947   M348934600  ...   \n",
       "\n",
       "      category_es_wellnessandbeauty category_es_hyper category_es_fashion  \\\n",
       "0                                 1                 0                   0   \n",
       "1                                 0                 0                   0   \n",
       "2                                 0                 0                   0   \n",
       "3                                 0                 0                   0   \n",
       "4                                 0                 0                   0   \n",
       "...                             ...               ...                 ...   \n",
       "49995                             0                 0                   0   \n",
       "49996                             0                 0                   0   \n",
       "49997                             1                 0                   0   \n",
       "49998                             0                 0                   0   \n",
       "49999                             0                 0                   0   \n",
       "\n",
       "      category_es_home  category_es_contents  category_es_travel  \\\n",
       "0                    0                     0                   0   \n",
       "1                    0                     0                   0   \n",
       "2                    0                     0                   0   \n",
       "3                    0                     0                   0   \n",
       "4                    0                     0                   0   \n",
       "...                ...                   ...                 ...   \n",
       "49995                0                     0                   0   \n",
       "49996                0                     0                   0   \n",
       "49997                0                     0                   0   \n",
       "49998                0                     0                   0   \n",
       "49999                0                     0                   0   \n",
       "\n",
       "       category_es_leisure  gender_M  gender_F  gender_E  \n",
       "0                        0         1         0         0  \n",
       "1                        0         1         0         0  \n",
       "2                        0         1         0         0  \n",
       "3                        0         1         0         0  \n",
       "4                        0         1         0         0  \n",
       "...                    ...       ...       ...       ...  \n",
       "49995                    0         0         1         0  \n",
       "49996                    0         1         0         0  \n",
       "49997                    0         1         0         0  \n",
       "49998                    0         1         0         0  \n",
       "49999                    0         1         0         0  \n",
       "\n",
       "[50000 rows x 32 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_set.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>event</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1974668487</td>\n",
       "      <td>details_change</td>\n",
       "      <td>2023-01-31 15:54:58.925487086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1973547259</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-31 18:56:33.182627508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C515668508</td>\n",
       "      <td>login</td>\n",
       "      <td>2023-01-31 15:29:39.463804302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source           event                     timestamp\n",
       "0  C1974668487  details_change 2023-01-31 15:54:58.925487086\n",
       "1  C1973547259           login 2023-01-31 18:56:33.182627508\n",
       "2   C515668508           login 2023-01-31 15:29:39.463804302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch our user_events dataset from the server\n",
    "user_events_data = pd.read_csv('https://s3.wasabisys.com/iguazio/data/fraud-demo-mlrun-fs-docs/events.csv', \n",
    "                               index_col=0, quotechar=\"\\'\", parse_dates=['timestamp'])\n",
    "\n",
    "# Adjust to the last 2 days to see the latest aggregations in our online feature vectors\n",
    "user_events_data = adjust_data_timespan(user_events_data, new_period='2d')\n",
    "\n",
    "# Preview\n",
    "user_events_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Create a FeatureSet and Preprocessing Pipeline\n",
    "\n",
    "Now we will define the events feature set.\n",
    "This is a pretty straight forward pipeline in which we only one hot encode the event categories and save the data to the default targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set = fstore.FeatureSet(\"events\",\n",
    "                                    entities=[fstore.Entity(\"source\")],\n",
    "                                    timestamp_key='timestamp', \n",
    "                                    description=\"user events feature set\",\n",
    "                                    engine='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"394pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 393.88 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 389.88,-94 389.88,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"38.55,-27.05 40.7,-27.15 42.83,-27.3 44.92,-27.49 46.98,-27.74 48.99,-28.03 50.95,-28.36 52.84,-28.75 54.66,-29.18 56.4,-29.65 58.06,-30.16 59.63,-30.71 61.11,-31.31 62.49,-31.94 63.76,-32.61 64.93,-33.31 65.99,-34.04 66.93,-34.8 67.77,-35.59 68.48,-36.41 69.09,-37.25 69.58,-38.11 69.95,-38.99 70.21,-39.89 70.36,-40.8 70.4,-41.72 70.33,-42.65 70.16,-43.59 69.89,-44.53 69.53,-45.47 69.07,-46.41 68.52,-47.35 67.89,-48.28 67.18,-49.2 66.4,-50.11 65.55,-51.01 64.63,-51.89 63.65,-52.75 62.62,-53.59 61.53,-54.41 60.4,-55.2 59.23,-55.96 58.02,-56.69 56.78,-57.39 55.5,-58.06 54.2,-58.69 52.88,-59.29 51.53,-59.84 50.17,-60.35 48.79,-60.82 47.4,-61.25 46,-61.64 44.59,-61.97 43.17,-62.26 41.75,-62.51 40.32,-62.7 38.89,-62.85 37.45,-62.95 36.02,-63 34.58,-63 33.15,-62.95 31.71,-62.85 30.28,-62.7 28.85,-62.51 27.43,-62.26 26.01,-61.97 24.6,-61.64 23.2,-61.25 21.81,-60.82 20.43,-60.35 19.07,-59.84 17.72,-59.29 16.4,-58.69 15.1,-58.06 13.82,-57.39 12.58,-56.69 11.37,-55.96 10.2,-55.2 9.07,-54.41 7.98,-53.59 6.95,-52.75 5.97,-51.89 5.05,-51.01 4.2,-50.11 3.42,-49.2 2.71,-48.28 2.08,-47.35 1.53,-46.41 1.07,-45.47 0.71,-44.53 0.44,-43.59 0.27,-42.65 0.2,-41.72 0.24,-40.8 0.39,-39.89 0.65,-38.99 1.02,-38.11 1.51,-37.25 2.11,-36.41 2.83,-35.59 3.66,-34.8 4.61,-34.04 5.67,-33.31 6.84,-32.61 8.11,-31.94 9.49,-31.31 10.97,-30.71 12.54,-30.16 14.2,-29.65 15.94,-29.18 17.76,-28.75 19.65,-28.36 21.61,-28.03 23.62,-27.74 25.68,-27.49 27.77,-27.3 29.9,-27.15 32.05,-27.05 34.22,-27 36.38,-27 38.55,-27.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"35.3\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>OneHotEncoder</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"191.74\" cy=\"-45\" rx=\"85.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.74\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;OneHotEncoder -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;OneHotEncoder</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.81,-45C77.82,-45 86.77,-45 96.08,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.29,-48.5 106.29,-45 96.29,-41.5 96.29,-48.5\"/>\n",
       "</g>\n",
       "<!-- parquet/parquet -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.88,-86.73C385.88,-88.53 369.52,-90 349.38,-90 329.24,-90 312.88,-88.53 312.88,-86.73 312.88,-86.73 312.88,-57.27 312.88,-57.27 312.88,-55.47 329.24,-54 349.38,-54 369.52,-54 385.88,-55.47 385.88,-57.27 385.88,-57.27 385.88,-86.73 385.88,-86.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M385.88,-86.73C385.88,-84.92 369.52,-83.45 349.38,-83.45 329.24,-83.45 312.88,-84.92 312.88,-86.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"349.38\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">parquet</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;parquet/parquet -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;parquet/parquet</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.3,-56.37C273.13,-58.94 288.57,-61.62 302.39,-64.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.15,-67.53 312.6,-65.79 303.35,-60.64 302.15,-67.53\"/>\n",
       "</g>\n",
       "<!-- nosql/nosql -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.88,-32.73C376.88,-34.53 364.56,-36 349.38,-36 334.21,-36 321.88,-34.53 321.88,-32.73 321.88,-32.73 321.88,-3.27 321.88,-3.27 321.88,-1.47 334.21,0 349.38,0 364.56,0 376.88,-1.47 376.88,-3.27 376.88,-3.27 376.88,-32.73 376.88,-32.73\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.88,-32.73C376.88,-30.92 364.56,-29.45 349.38,-29.45 334.21,-29.45 321.88,-30.92 321.88,-32.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"349.38\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">nosql</text>\n",
       "</g>\n",
       "<!-- OneHotEncoder&#45;&gt;nosql/nosql -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>OneHotEncoder&#45;&gt;nosql/nosql</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.3,-33.63C276.56,-30.46 295.75,-27.13 311.7,-24.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.42,-27.79 321.68,-22.63 311.22,-20.89 312.42,-27.79\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f92edebca60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and add value mapping\n",
    "events_mapping = {'event': list(user_events_data.event.unique())}\n",
    "\n",
    "# One Hot Encode\n",
    "user_events_set.graph.to(OneHotEncoder(mapping=events_mapping))\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "user_events_set.set_targets()\n",
    "\n",
    "# Plot the pipeline so we can see the different steps\n",
    "user_events_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:48:22,329 [info] writing to target parquet, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/events/parquet/sets/events/1675234102329_670/', 'format': 'parquet', 'partitionBy': ['year', 'month', 'day', 'hour']}\n",
      "> 2023-02-01 06:48:37,463 [info] writing to target nosql, spark options {'path': 'v3io://projects/fraud-demo-dani/FeatureStore/events/nosql/sets/events/1675234102329_670/', 'format': 'io.iguaz.v3io.spark.sql.kv', 'key': 'source'}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"transaction_spark_function\").getOrCreate()\n",
    "\n",
    "# Ingestion of our newly created events feature set locally\n",
    "events_df = fstore.ingest(user_events_set, user_events_data, spark_context=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create a labels dataset for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Create a FeatureSet\n",
    "This feature set contains the label for the fraud demo, it will be ingested directly to the default targets without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df):\n",
    "    labels = df[['fraud','timestamp']].copy()\n",
    "    labels = labels.rename(columns={\"fraud\": \"label\"})\n",
    "    labels['timestamp'] = labels['timestamp'].astype(\"datetime64[ms]\")\n",
    "    labels['label'] = labels['label'].astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"151pt\" height=\"193pt\"\n",
       " viewBox=\"0.00 0.00 150.99 193.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 189.25)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-189.25 146.99,-189.25 146.99,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"74.74,-149.3 76.89,-149.4 79.02,-149.55 81.12,-149.74 83.18,-149.99 85.19,-150.28 87.14,-150.61 89.03,-151 90.85,-151.43 92.6,-151.9 94.26,-152.41 95.83,-152.96 97.3,-153.56 98.68,-154.19 99.96,-154.86 101.12,-155.56 102.18,-156.29 103.13,-157.05 103.96,-157.84 104.68,-158.66 105.28,-159.5 105.77,-160.36 106.14,-161.24 106.4,-162.14 106.55,-163.05 106.59,-163.97 106.53,-164.9 106.36,-165.84 106.09,-166.78 105.72,-167.72 105.26,-168.66 104.72,-169.6 104.09,-170.53 103.38,-171.45 102.59,-172.36 101.74,-173.26 100.82,-174.14 99.84,-175 98.81,-175.84 97.73,-176.66 96.6,-177.45 95.42,-178.21 94.21,-178.94 92.97,-179.64 91.7,-180.31 90.39,-180.94 89.07,-181.54 87.73,-182.09 86.36,-182.6 84.98,-183.07 83.59,-183.5 82.19,-183.89 80.78,-184.22 79.36,-184.51 77.94,-184.76 76.51,-184.95 75.08,-185.1 73.65,-185.2 72.21,-185.25 70.78,-185.25 69.34,-185.2 67.91,-185.1 66.47,-184.95 65.05,-184.76 63.62,-184.51 62.2,-184.22 60.79,-183.89 59.39,-183.5 58,-183.07 56.62,-182.6 55.26,-182.09 53.92,-181.54 52.59,-180.94 51.29,-180.31 50.02,-179.64 48.77,-178.94 47.56,-178.21 46.39,-177.45 45.26,-176.66 44.18,-175.84 43.14,-175 42.16,-174.14 41.25,-173.26 40.39,-172.36 39.61,-171.45 38.9,-170.53 38.27,-169.6 37.72,-168.66 37.27,-167.72 36.9,-166.78 36.63,-165.84 36.46,-164.9 36.39,-163.97 36.43,-163.05 36.58,-162.14 36.84,-161.24 37.22,-160.36 37.71,-159.5 38.31,-158.66 39.03,-157.84 39.86,-157.05 40.81,-156.29 41.86,-155.56 43.03,-154.86 44.3,-154.19 45.68,-153.56 47.16,-152.96 48.73,-152.41 50.39,-151.9 52.13,-151.43 53.96,-151 55.85,-150.61 57.8,-150.28 59.81,-149.99 61.87,-149.74 63.97,-149.55 66.1,-149.4 68.25,-149.3 70.41,-149.25 72.58,-149.25 74.74,-149.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.49\" y=\"-163.55\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<!-- create_labels -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>create_labels</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"71.49\" cy=\"-95.25\" rx=\"71.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.49\" y=\"-91.55\" font-family=\"Times,serif\" font-size=\"14.00\">create_labels</text>\n",
       "</g>\n",
       "<!-- _start&#45;&gt;create_labels -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;create_labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.49,-148.95C71.49,-141.23 71.49,-131.96 71.49,-123.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.99,-123.35 71.49,-113.35 67.99,-123.35 74.99,-123.35\"/>\n",
       "</g>\n",
       "<!-- parquet/labels -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>parquet/labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.99,-37.6C100.99,-39.68 87.77,-41.38 71.49,-41.38 55.22,-41.38 41.99,-39.68 41.99,-37.6 41.99,-37.6 41.99,-3.65 41.99,-3.65 41.99,-1.57 55.22,0.13 71.49,0.13 87.77,0.13 100.99,-1.57 100.99,-3.65 100.99,-3.65 100.99,-37.6 100.99,-37.6\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.99,-37.6C100.99,-35.52 87.77,-33.83 71.49,-33.83 55.22,-33.83 41.99,-35.52 41.99,-37.6\"/>\n",
       "<text text-anchor=\"start\" x=\"49.99\" y=\"-21.43\" font-family=\"Times,serif\" font-size=\"14.00\">labels</text>\n",
       "<text text-anchor=\"start\" x=\"52.49\" y=\"-12.22\" font-family=\"Times,serif\" font-size=\"8.00\">(parquet)</text>\n",
       "</g>\n",
       "<!-- create_labels&#45;&gt;parquet/labels -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>create_labels&#45;&gt;parquet/labels</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.49,-77.04C71.49,-69.4 71.49,-60.2 71.49,-51.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"74.99,-51.28 71.49,-41.28 67.99,-51.28 74.99,-51.28\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f92ee049d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlrun.datastore import ParquetTarget\n",
    "import os\n",
    "\n",
    "# Define the \"labels\" feature set\n",
    "labels_set = fstore.FeatureSet(\"labels\", \n",
    "                           entities=[fstore.Entity(\"source\")], \n",
    "                           timestamp_key='timestamp',\n",
    "                           description=\"training labels\",\n",
    "                           engine=\"pandas\")\n",
    "\n",
    "labels_set.graph.to(name=\"create_labels\", handler=create_labels)\n",
    "\n",
    "\n",
    "# specify only Parquet (offline) target since its not used for real-time\n",
    "target = ParquetTarget(name='labels',path=f'v3io:///projects/{project.name}/target.parquet')\n",
    "labels_set.set_targets([target], with_defaults=False)\n",
    "labels_set.plot(with_targets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Set - Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-31 20:55:36.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-01-30 19:42:30.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1000148617</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-01 03:27:45.773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label               timestamp\n",
       "source                                    \n",
       "C1000148617      0 2023-01-31 20:55:36.008\n",
       "C1000148617      0 2023-01-30 19:42:30.283\n",
       "C1000148617      0 2023-02-01 03:27:45.773"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest the labels feature set\n",
    "labels_df = fstore.ingest(labels_set, transactions_data)\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Deploy a real-time pipeline\n",
    "\n",
    "When dealing with real-time aggregation, it's important to be able to update these aggregations in real-time.\n",
    "For this purpose, we will create live serving functions that will update the online feature store of the `transactions` FeatureSet and `Events` FeatureSet.\n",
    "\n",
    "Using MLRun's `serving` runtime, craetes a nuclio function loaded with our feature set's computational graph definition\n",
    "and an `HttpSource` to define the HTTP trigger.\n",
    "\n",
    "Notice that the implementation below does not require any rewrite of the pipeline logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Deploy our FeatureSet live endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Storey engine to deploy the live endpoint.<br>\n",
    "Storey is streaming library for real time event processing and feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_set.spec.engine = 'storey'\n",
    "transaction_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "transaction_stream = f'v3io:///projects/{project.name}/streams/transaction'\n",
    "transaction_pusher = mlrun.datastore.get_stream_pusher(transaction_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:48:45,435 [info] Starting remote function deploy\n",
      "2023-02-01 06:48:45  (info) Deploying function\n",
      "2023-02-01 06:48:45  (info) Building\n",
      "2023-02-01 06:48:45  (info) Staging files and preparing base images\n",
      "2023-02-01 06:48:45  (info) Building processor image\n",
      "2023-02-01 06:49:30  (info) Build complete\n",
      "2023-02-01 06:50:10  (info) Function deploy complete\n",
      "> 2023-02-01 06:50:11,315 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-dani-transactions-ingest.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['fraud-demo-dani-transactions-ingest-fraud-demo-dani.default-tenant.app.vmdev94.lab.iguazeng.com/']}\n"
     ]
    }
   ],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=transaction_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "transaction_set_endpoint = fstore.deploy_ingestion_service(featureset=transaction_set, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defining our `transactions` feature set we can now use MLRun and Storey to deploy it as a live endpoint, ready to ingest new data!\n",
    "\n",
    "Using MLRun's `serving` runtime, we will create a nuclio function loaded with our feature set's computational graph definition and an `HttpSource` to define the HTTP trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 46,\n",
       " 'age': '4',\n",
       " 'gender': 'F',\n",
       " 'zipcodeOri': 28007,\n",
       " 'zipMerchant': 28007,\n",
       " 'category': 'es_food',\n",
       " 'amount': 66.85,\n",
       " 'fraud': 0,\n",
       " 'timestamp': '2023-02-01 06:50:11.360543',\n",
       " 'source': 'C1132565589',\n",
       " 'target': 'M85975013',\n",
       " 'device': 'bbc94faa8a9f45b586d1f747df2d5efd'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Select a sample from the dataset and serialize it to JSON\n",
    "transaction_sample = json.loads(transactions_data.sample(1).to_json(orient='records'))[0]\n",
    "transaction_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "transaction_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"86fea581-2057-4b52-8475-43134bdced04\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(transaction_set_endpoint, json=transaction_sample).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_pusher.push(transaction_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - User Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Deploy our FeatureSet live endpoint\n",
    "Deploy the events feature set's ingestion service using the feature set and all the previously defined resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_events_set.spec.engine = 'storey'\n",
    "user_events_set.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iguazio v3io stream and transactions push API endpoint\n",
    "events_stream = f'v3io:///projects/{project.name}/streams/events'\n",
    "events_pusher = mlrun.datastore.get_stream_pusher(events_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-02-01 06:50:11,605 [info] Starting remote function deploy\n",
      "2023-02-01 06:50:11  (info) Deploying function\n",
      "2023-02-01 06:50:11  (info) Building\n",
      "2023-02-01 06:50:11  (info) Staging files and preparing base images\n",
      "2023-02-01 06:50:11  (info) Building processor image\n",
      "2023-02-01 06:51:40  (info) Build complete\n",
      "2023-02-01 06:52:21  (info) Function deploy complete\n",
      "> 2023-02-01 06:52:21,386 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-fraud-demo-dani-events-ingest.default-tenant.svc.cluster.local:8080'], 'external_invocation_urls': ['fraud-demo-dani-events-ingest-fraud-demo-dani.default-tenant.app.vmdev94.lab.iguazeng.com/']}\n"
     ]
    }
   ],
   "source": [
    "# Define the source stream trigger (use v3io streams)\n",
    "# we will define the `key` and `time` fields (extracted from the Json message).\n",
    "source = mlrun.datastore.sources.StreamSource(path=events_stream , key_field='source', time_field='timestamp')\n",
    "\n",
    "# Deploy the transactions feature set's ingestion service over a real-time (Nuclio) serverless function\n",
    "# you can use the run_config parameter to pass function/service specific configuration\n",
    "events_set_endpoint = fstore.deploy_ingestion_service(featureset=user_events_set, source=source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Events - Test the feature set HTTP endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'C1965115675',\n",
       " 'event': 'login',\n",
       " 'timestamp': '2023-02-01 06:52:21.446742'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a sample from the events dataset and serialize it to JSON\n",
    "user_events_sample = json.loads(user_events_data.sample(1).to_json(orient='records'))[0]\n",
    "user_events_sample['timestamp'] = str(pd.Timestamp.now())\n",
    "user_events_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"c0b1d89f-4036-42cf-9d4a-3a29618105a3\"}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post the sample to the ingestion endpoint\n",
    "requests.post(events_set_endpoint, json=user_events_sample).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_pusher.push(user_events_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "You've completed Part 1 of the data-ingestion with the feature store.\n",
    "Proceed to [Part 2](02-create-training-model.ipynb) to learn how to train an ML model using the feature store data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-extended",
   "language": "python",
   "name": "conda-env-mlrun-extended-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
