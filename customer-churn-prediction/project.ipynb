{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Project\n",
    "  --------------------------------------------------------------------\n",
    "\n",
    "_note_: This notebook was adapted from a number external sources:\n",
    "* **[Churn Prediction and Prevention in Python](https://towardsdatascience.com/churn-prediction-and-prevention-in-python-2d454e5fd9a5)**\n",
    "* **[Telecom Customer Churn Prediction](https://www.kaggle.com/pavanraj159/telecom-customer-churn-prediction*)**\n",
    "\n",
    "#### **notebook how-to's**\n",
    "* Create and test a custom `data_clean` function \n",
    "* Examine data using a serverless (containerized) `describe` function\n",
    "* Train a number of machine learning algorithms\n",
    "* Tune hyperparameters \n",
    "* Create an automated ML pipeline from various library functions\n",
    "* Run and track the pipeline results and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a custom data cleaning function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from cloudpickle import dumps, dump, load\n",
    "\n",
    "from sklearn.preprocessing import (OneHotEncoder,\n",
    "                                   LabelEncoder)\n",
    "\n",
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem\n",
    "\n",
    "def data_clean(\n",
    "    context:MLClientCtx, \n",
    "    src: DataItem,\n",
    "    file_ext: str = \"csv\",\n",
    "    models_dest: str = \"models/encoders\",\n",
    "    cleaned_key: str = \"cleaned-data\",\n",
    "    encoded_key: str = \"encoded-data\"\n",
    "):\n",
    "    \"\"\"process a raw churn data file\n",
    "    \n",
    "    Data has 3 states here: `raw`, `cleaned` and `encoded`\n",
    "    \n",
    "    * `raw` kept by default, the pipeline begins with a raw data artifact\n",
    "    * `cleaned` kept for charts, presentations\n",
    "    * `encoded` is input for a cross validation and training function\n",
    "    \n",
    "    steps (not necessarily in correct order, some parallel)\n",
    "    * column name maps\n",
    "    * deal with nans and other types of missings/junk\n",
    "    * label encode binary and ordinal category columns\n",
    "    * create category ranges from numerical columns\n",
    "    And finally,\n",
    "    * test\n",
    "    \n",
    "    Why we don't one-hot-encode here? One hot encoding isn't a necessary\n",
    "    step for all algorithms. It can also generate a very large feature\n",
    "    matrix that doesn't need to be serialized (even if sparse).\n",
    "    So we leave one-hot-encoding for the training step.\n",
    "    \n",
    "    What about scaling numerical columns? Same as why we don't one hot\n",
    "    encode here. Do we scale before train-test split?  IMHO, no.  Scaling\n",
    "    before splitting introduces a type of data leakage.  In addition,\n",
    "    many estimators are completely immune to the monotonic transformations\n",
    "    implied by scaling, so why waste the cycles?\n",
    "    \n",
    "    TODO: \n",
    "        * parallelize where possible\n",
    "        * more abstraction (more parameters, chain sklearn transformers)\n",
    "        * convert to marketplace function\n",
    "        \n",
    "    :param context:          the function execution context\n",
    "    :param src:              an artifact or file path\n",
    "    :param file_ext:         file type for artifacts\n",
    "    :param models_dest:       label encoders and other preprocessing steps\n",
    "                             should be saved together with other pipeline\n",
    "                             models\n",
    "    :param cleaned_key:      key of cleaned data table in artifact store\n",
    "    :param encoded_key:      key of encoded data table in artifact store\n",
    "    \"\"\"\n",
    "    df = src.as_df()\n",
    "    \n",
    "    # drop columns\n",
    "    drop_cols_list = [\"customerID\", \"TotalCharges\"]\n",
    "    df.drop(drop_cols_list, axis=1, inplace=True)\n",
    "    \n",
    "    # header transformations\n",
    "    old_cols = df.columns\n",
    "    rename_cols_map = {\n",
    "        \"SeniorCitizen\" : \"senior\",\n",
    "        \"Partner\"       : \"partner\",\n",
    "        \"Dependents\"    : \"deps\",\n",
    "        \"Churn\"         : \"labels\"\n",
    "    }\n",
    "    df.rename(rename_cols_map, axis=1, inplace=True)\n",
    "\n",
    "    # add drop column to logs:\n",
    "    for col in drop_cols_list:\n",
    "        rename_cols_map.update({col: \"_DROPPED_\"})\n",
    "    \n",
    "    # log the op\n",
    "    tp = os.path.join(models_dest, \"preproc-column_map.json\")\n",
    "    context.log_artifact(\"preproc-column_map.json\",\n",
    "                         body=json.dumps(rename_cols_map),\n",
    "                         local_path=tp)\n",
    "    \n",
    "    # VALUE transformations\n",
    "\n",
    "    # clean\n",
    "    # truncate reply to \"No\"\n",
    "    df = df.applymap(lambda x: \"No\" if str(x).startswith(\"No \") else x)\n",
    "\n",
    "    # encode numerical type as category bins (ordinal)\n",
    "    bins = [0, 12, 24, 36, 48, 60, np.inf]\n",
    "    labels = [0, 1, 2, 3, 4, 5]\n",
    "    tenure = df.tenure.copy(deep=True)\n",
    "    df[\"tenure_map\"] = pd.cut(df.tenure, bins, labels=False)\n",
    "    tenure_map = dict(zip(bins, labels))\n",
    "    # save this transformation\n",
    "    tp = os.path.join(models_dest, \"preproc-numcat_map.json\")\n",
    "    context.log_artifact(\"preproc-numcat_map.json\", \n",
    "                         body=bytes(json.dumps(tenure_map).encode(\"utf-8\")), \n",
    "                         local_path=tp)\n",
    "    \n",
    "    context.log_dataset(cleaned_key, df=df, format=file_ext, index=False)\n",
    "    \n",
    "    # label encoding - generate model for each column saved in dict\n",
    "    # some of these columns may be hot encoded in the training step\n",
    "    fix_cols = [\"gender\", \"partner\", \"deps\", \"OnlineSecurity\", \n",
    "                \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n",
    "                \"StreamingTV\", \"StreamingMovies\", \"PhoneService\",\n",
    "                \"MultipleLines\", \"PaperlessBilling\", \"InternetService\", \n",
    "                \"Contract\", \"PaymentMethod\", \"labels\"]\n",
    "    \n",
    "    d = defaultdict(LabelEncoder)\n",
    "    df[fix_cols] = df[fix_cols].apply(lambda x: d[x.name].fit_transform(x.astype(str)))\n",
    "    context.log_dataset(encoded_key, df=df, format=file_ext, index=False)\n",
    "\n",
    "    model_bin = dumps(d)\n",
    "    context.log_model(\"model\", \n",
    "                      body=model_bin,\n",
    "                      artifact_path=os.path.join(context.artifact_path, \n",
    "                                                 models_dest),\n",
    "                      model_file=\"model.pkl\")\n",
    "    # would be nice to have a check here on the integrity of all done\n",
    "    # raw->clean->encoded->clean->raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project to host our functions, jobs and artifacts\n",
    "\n",
    "Projects are used to package multiple functions, workflows, and artifacts. We usually store project code and definitions in a Git archive.\n",
    "\n",
    "The following code creates a new project in a local dir and initialize git tracking on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "\n",
    "project_name = \"churn-project\"\n",
    "project_dir = \"./project\"\n",
    "mlrun.set_environment('http://mlrun-api:8080', artifact_path='./data', project=project_name)\n",
    "churn_proj = mlrun.new_project(project_name, project_dir, init_git=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register an input artifact/dataset in the store \n",
    "Log the raw data file as an artifact, enabeling us to refer to it by name (`store:///raw-data`) and ensuring we keep a record of the source data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/mlrun/demos/master/customer-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "\n",
    "churn_proj.log_artifact(\"raw-data\", target_path=DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-locally\"></a>\n",
    "### Run the data generator function locally\n",
    "\n",
    "The functions above can be tested locally. Parameters, inputs, and outputs can be specified in the API or the `Task` object.<br>\n",
    "when using `run_local()` the function inputs and outputs are automatically recorded by MLRun experiment and data tracking DB.\n",
    "\n",
    "In each run we can specify the function, inputs, parameters/hyper-parameters, etc... For more details, see the [mlrun_basics notebook](mlrun_basics.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-07-09 21:02:45,913 starting run data_clean uid=ca9043ad6a114271960220ce99b1f374  -> http://mlrun-api:8080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #b3edff;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #ffe6cc;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>churn-project</td>\n",
       "      <td><div title=\"ca9043ad6a114271960220ce99b1f374\"><a href=\"https://mlrun-ui.default-tenant.app.tmvgeckijwgk.iguazio-cd2.com/projects/churn-project/jobs/ca9043ad6a114271960220ce99b1f374/info\" target=\"_blank\" >...99b1f374</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jul 09 21:02:45</td>\n",
       "      <td>completed</td>\n",
       "      <td>data_clean</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=handler</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=jupyter-5db6b86589-zgpmp</div></td>\n",
       "      <td><div title=\"store:///raw-data\">src</div></td>\n",
       "      <td><div class=\"dictlist\">file_ext=csv</div><div class=\"dictlist\">apply_tenure_map=False</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result649f10d5\" title=\"/files/ml/demos/customer-churn-prediction/data/models/encoders/preproc-column_map.json\">preproc-column_map.json</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result649f10d5\" title=\"/files/ml/demos/customer-churn-prediction/data/models/encoders/preproc-numcat_map.json\">preproc-numcat_map.json</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result649f10d5\" title=\"/files/ml/demos/customer-churn-prediction/data/cleaned-data.csv\">cleaned-data</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"result649f10d5\" title=\"/files/ml/demos/customer-churn-prediction/data/encoded-data.csv\">encoded-data</div><div title=\"/User/ml/demos/customer-churn-prediction/data/models/encoders\">model</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result649f10d5-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result649f10d5-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result649f10d5\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result649f10d5-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run ca9043ad6a114271960220ce99b1f374 --project churn-project , !mlrun logs ca9043ad6a114271960220ce99b1f374 --project churn-project\n",
      "[mlrun] 2020-07-09 21:02:47,314 run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "# run the function locally\n",
    "cleaner = mlrun.run_local(\n",
    "    name=\"data_clean\",\n",
    "    handler=data_clean, \n",
    "    inputs={\"src\": \"store:///raw-data\"},\n",
    "    params={\"file_ext\" : \"csv\",\n",
    "            \"apply_tenure_map\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Fully Automated ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we using GPU's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert our local code to a distributed serverless function object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f3b6f76b550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_fn_params = {\n",
    "    \"name\"        : \"clean_data\",\n",
    "    \"kind\"        : \"job\",\n",
    "    \"image\"       : \"mlrun/ml-models-gpu\" if GPUS else \"mlrun/ml-models\",\n",
    "    \"description\" : \"clean and encode raw data\",\n",
    "    \"categories\"  : [\"data-prep\"],\n",
    "    \"labels\"      : {\"author\": \"yasha\", \"framework\": \"xgboost\"}      \n",
    "}\n",
    "\n",
    "clean_func = mlrun.code_to_function(**clean_data_fn_params)\n",
    "churn_proj.set_function(clean_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more functions to our project to be used in our pipeline (from the functions hub/marketplace)\n",
    "\n",
    "AutoML training (classifier), Model validation (test_classifier), Real-time model server, and Model REST API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f3b69939e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_proj.set_function(\"hub://describe\", \"describe\")\n",
    "\n",
    "churn_proj.set_function(\"hub://xgb_trainer\", \"classify\")\n",
    "churn_proj.set_function(\"hub://xgb_test\", \"xgbtest\")\n",
    "\n",
    "churn_proj.set_function(\"hub://coxph_trainer\", \"survive\")\n",
    "churn_proj.set_function(\"hub://coxph_test\", \"coxtest\")\n",
    "\n",
    "churn_proj.set_function(\"hub://churn_server\", \"server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and save a pipeline \n",
    "\n",
    "The following workflow definition will be written into a file, it describes a Kubeflow execution graph (DAG)<br>\n",
    "and how functions and data are connected  to form an end to end pipeline. \n",
    "\n",
    "Check the code below to see how functions objects are initialized and used (by name) inside the workflow.<br>\n",
    "The `workflow.py` file has two parts, initialize the function objects and define pipeline dsl (connect the function inputs and outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting project/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile project/workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "GPUS = False\n",
    "\n",
    "# init functions is used to configure function resources and local settings\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())\n",
    "        \n",
    "    functions[\"server\"].set_env(\"INFERENCE_STREAM\", \"users/admin/artifacts/customer-churn-prediction/model_stream\")\n",
    "\n",
    "    \n",
    "@dsl.pipeline(\n",
    "    name=\"Demo training pipeline\",\n",
    "    description=\"Shows how to use mlrun.\"\n",
    ")\n",
    "def kfpipeline():\n",
    "    \n",
    "    # encode the data\n",
    "    clean = funcs[\"clean-data\"].as_step(\n",
    "        name=\"clean-data\",\n",
    "        handler=\"data_clean\",\n",
    "        params={\"file_ext\": \"csv\",\n",
    "                \"models_dest\": \"models/encoders\"},\n",
    "        inputs={\"src\": \"store:///raw-data\"},  # use an artifact from the feature store\n",
    "        outputs=[\"cleaned-data\",\n",
    "                 \"encoded-data\"])\n",
    "\n",
    "    # analyze our dataset\n",
    "    describe = funcs[\"describe\"].as_step(\n",
    "        name=\"summary\",\n",
    "        params={\"label_column\"  : \"labels\"},\n",
    "        inputs={\"table\": clean.outputs[\"encoded-data\"]})\n",
    "    \n",
    "    # train with hyper-paremeters\n",
    "    xgb = funcs[\"classify\"].as_step(\n",
    "        name=\"current-state\",\n",
    "        handler=\"train_model\",\n",
    "        params={\"sample\"                  : -1, \n",
    "                \"label_column\"            : \"labels\",\n",
    "                \"model_type\"              : \"classifier\",\n",
    "                # xgb class initializers (tuning candidates):\n",
    "                \"CLASS_tree_method\"       : \"gpu_hist\" if GPUS else \"hist\",\n",
    "                \"CLASS_objective\"         : \"binary:logistic\",\n",
    "                \"CLASS_n_estimators\"      : 50,\n",
    "                \"CLASS_max_depth\"         : 5,\n",
    "                \"CLASS_learning_rate\"     : 0.15,\n",
    "                \"CLASS_colsample_bylevel\" : 0.7,\n",
    "                \"CLASS_colsample_bytree\"  : 0.8,\n",
    "                \"CLASS_gamma\"             : 1.0,\n",
    "                \"CLASS_max_delta_step\"    : 3,\n",
    "                \"CLASS_min_child_weight\"  : 1.0,\n",
    "                \"CLASS_reg_lambda\"        : 10.0,\n",
    "                \"CLASS_scale_pos_weight\"  : 1,\n",
    "                \"FIT_verbose\"             : 0,\n",
    "                \"CLASS_subsample\"         : 0.9,\n",
    "                \"CLASS_booster\"           : \"gbtree\",\n",
    "                \"CLASS_random_state\"      : 1,\n",
    "                # encoding:\n",
    "                \"encode_cols\"        : {\"InternetService\": \"ISP\",\n",
    "                                        \"Contract\"       : \"Contract\",\n",
    "                                        \"PaymentMethod\"   : \"Payment\"},\n",
    "                # outputs\n",
    "                \"models_dest\"        : \"models\",\n",
    "                \"plots_dest\"         : \"plots\",\n",
    "                \"file_ext\"           : \"csv\"\n",
    "               },\n",
    "        inputs={\"dataset\"   : clean.outputs[\"encoded-data\"]},\n",
    "        outputs=[\"model\", \"test_set\"])\n",
    "\n",
    "    cox = funcs[\"survive\"].as_step(\n",
    "        name=\"survival-curves\",\n",
    "        params={\"sample\"                  : -1, \n",
    "                \"event_column\"            : \"labels\",\n",
    "                \"strata_cols\" : ['InternetService', 'StreamingMovies', \n",
    "                                 'StreamingTV', 'PhoneService'],\n",
    "                \"encode_cols\" : {\"Contract\"       : \"Contract\",\n",
    "                                 \"PaymentMethod\"  : \"Payment\"},\n",
    "                # outputs\n",
    "                \"models_dest\"        : \"models/cox\",\n",
    "                \"plots_dest\"         : \"plots\",\n",
    "                \"file_ext\"           : \"csv\"\n",
    "               },\n",
    "        inputs={\"dataset\"   : clean.outputs[\"encoded-data\"]},\n",
    "        outputs=[\"cx-model\", \"tenured-test-set\"])\n",
    "\n",
    "    test_xgb = funcs[\"xgbtest\"].as_step(\n",
    "        name=\"test-classifier\",\n",
    "        params={\"label_column\": \"labels\",\n",
    "                \"plots_dest\"  : \"customer-churn-prediction/test/xgb\"},\n",
    "        inputs={\"models_path\" : xgb.outputs[\"model\"],\n",
    "                \"test_set\"    : xgb.outputs[\"test_set\"]})\n",
    "\n",
    "    test_cox = funcs[\"coxtest\"].as_step(\n",
    "        name=\"test-regressor\",\n",
    "        params={\"label_column\": \"labels\",\n",
    "                \"plots_dest\"  : \"customer-churn-prediction/test/cox\"},\n",
    "        inputs={\"models_path\" : cox.outputs[\"cx-model\"],\n",
    "                \"test_set\"    : cox.outputs[\"tenured-test-set\"]})\n",
    "\n",
    "    # deploy our model as a serverless function\n",
    "    deploy_xgb = funcs[\"server\"].deploy_step(\n",
    "        models={\"churn_server_v1\": xgb.outputs[\"model\"]})\n",
    "    deploy_xgb.after(cox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the workflow file as \"main\"\n",
    "churn_proj.set_workflow(\"main\", \"workflow.py\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the project definitions to a file (project.yaml), it is recommended to commit all changes to a Git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_proj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(churn_proj.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run-pipeline\"></a>\n",
    "## Run a pipeline workflow\n",
    "use the `run` method to execute a workflow, you can provide alternative arguments and specify the default target for workflow artifacts.<br>\n",
    "The workflow ID is returned and can be used to track the progress or you can use the hyperlinks\n",
    "\n",
    "> Note: The same command can be issued through CLI commands:<br>\n",
    "    `mlrun project my-proj/ -r main -p \"v3io:///users/admin/mlrun/kfp/{{workflow.uid}}/\"`\n",
    "\n",
    "The dirty flag allow us to run a project with uncommited changes (when the notebook is in the same git dir it will always be dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-07-09 21:04:07,413 using in-cluster config.\n"
     ]
    }
   ],
   "source": [
    "artifact_path = os.path.join(mlrun.mlconf.artifact_path, \"pipeline/{{workflow.uid}}\")\n",
    "run_id = churn_proj.run(\n",
    "    \"main\",\n",
    "    arguments={},\n",
    "    artifact_path=artifact_path,\n",
    "    dirty=True, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
