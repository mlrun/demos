{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "--------------------------------------------------------------------\n",
    "This notebook is generates the following: \n",
    "- Enrichment table with postcode as a key and socioeconomic index as it's value.\n",
    "- Event stream simulating 2000 users each with 1001 events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import IPython\n",
    "\n",
    "required = {'mlrun == 0.6.*', 'v3io == 0.6.*', 'faker'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "previously_installed = required.intersection(installed)\n",
    "\n",
    "if missing:\n",
    "    print(f'Installing {\",\".join(missing)}')\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "    print('Restarting kernel')\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n",
    "if previously_installed:\n",
    "    print(f'Already installed: {\",\".join(previously_installed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the `# nuclio: start-code` marker to instruct Nuclio to start processing code only from this location, and then performs basic Nuclio function configuration &mdash; defining the name of the function's container image (`mlrun/ml-models`), the function type (`nuclio`), and some additional package installation commands.\n",
    "\n",
    "> **Note:** You can add code to define function dependencies and perform additional configuration after the `# nuclio: start-code` marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, random\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import uuid\n",
    "from faker import Faker\n",
    "import v3io.dataplane\n",
    "import os\n",
    "from mlrun.execution import MLClientCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_postcode(is_churn):\n",
    "    # if is_churn is true the postcode modulu 3 will return 0 or 1\n",
    "    # if is_churn is false the postcode modulu 3 will return 0 or 2\n",
    "    # this will encode information in postcode that our ML model will learn\n",
    "    base_postcode = 3 * randint(3334, 33333)\n",
    "    group = randint(0, 1)\n",
    "    if is_churn:\n",
    "        return base_postcode + group\n",
    "    else:\n",
    "        return base_postcode + (group * 2)\n",
    "\n",
    "\n",
    "# event functions\n",
    "def new_registration(fake, user_id, event_time, is_churn):\n",
    "    return {'user_id': user_id,\n",
    "            'event_type': 'registration',\n",
    "            'event_time': event_time,\n",
    "            'name': fake.name(),\n",
    "            'date_of_birth': fake.date(),\n",
    "            'street_address': fake.street_address(),\n",
    "            'city': fake.city(),\n",
    "            'country': fake.country(),\n",
    "            'postcode': gen_postcode(is_churn),\n",
    "            'affiliate_url': fake.image_url(),\n",
    "            'campaign': fake.ean8()}\n",
    "\n",
    "\n",
    "def new_purchase(fake, user_id, event_time):\n",
    "    return {'user_id': user_id,\n",
    "            'event_type': 'purchase',\n",
    "            'event_time': event_time,\n",
    "            'amount': fake.randomize_nb_elements(number=50)}\n",
    "\n",
    "\n",
    "def new_bet(fake, user_id, event_time):\n",
    "    return {'user_id': user_id,\n",
    "            'event_type': 'bet',\n",
    "            'event_time': event_time,\n",
    "            'bet_amount': fake.randomize_nb_elements(number=10)}\n",
    "\n",
    "\n",
    "def new_win(fake, user_id, event_time):\n",
    "    return {'user_id': user_id,\n",
    "            'event_type': 'win',\n",
    "            'event_time': event_time,\n",
    "            'win_amount': fake.randomize_nb_elements(number=200)}\n",
    "\n",
    "\n",
    "def gen_event_date(is_churn, prev_event_date=None):\n",
    "    if prev_event_date is None:\n",
    "        # generate first event date\n",
    "        return datetime.now() - timedelta(hours=randint(48, 96))\n",
    "    else:\n",
    "        if prev_event_date + timedelta(hours=30) < datetime.now() and not is_churn and randint(1, 1000) <= 5:\n",
    "            # if the user is not churned and it is possible, generate event in the following day with prbability 0.005\n",
    "            return prev_event_date + timedelta(hours=randint(15, 24))\n",
    "        else:\n",
    "            return prev_event_date + timedelta(seconds=randint(5, 100))\n",
    "\n",
    "\n",
    "def generate_events(fake, num_users, events_dist, num_events, is_churn):\n",
    "    user_ids = generate_user_ids(num_users)\n",
    "    events = []\n",
    "    for user_id in user_ids:\n",
    "        # register\n",
    "        event_time = gen_event_date(is_churn)\n",
    "        reg_event = new_registration(fake, user_id, event_time, is_churn)\n",
    "        reg_event['label'] = int(is_churn)\n",
    "        events.append(reg_event)\n",
    "        for _ in range(num_events):\n",
    "            # generate event according to dist\n",
    "            acc_prob = 0\n",
    "            rand = random()\n",
    "            for event_dist in events_dist:\n",
    "                if rand <= event_dist['probability'] + acc_prob:\n",
    "                    event_time = gen_event_date(is_churn, event_time)\n",
    "                    new_event = event_dist['generator'](fake, user_id, event_time)\n",
    "                    events.append(new_event)\n",
    "                    break\n",
    "                else:\n",
    "                    acc_prob += event_dist['probability']\n",
    "    return events\n",
    "\n",
    "\n",
    "def generate_user_ids(n: int):\n",
    "    return (str(uuid.uuid4()) for _ in range(n))\n",
    "\n",
    "\n",
    "def generate_event_stream(v3io_client,\n",
    "                          container,\n",
    "                          output_stream_path,\n",
    "                          num_users_group1,\n",
    "                          num_users_group2,\n",
    "                          events_per_user):\n",
    "    # 70% churn users\n",
    "\n",
    "    fake = Faker()\n",
    "\n",
    "    group1_events_dist = [{'probability': 0.1, 'generator': new_purchase},\n",
    "                          {'probability': 0.89, 'generator': new_bet},\n",
    "                          {'probability': 0.01, 'generator': new_win}]\n",
    "\n",
    "    group2_events_dist = [{'probability': 0.1, 'generator': new_purchase},\n",
    "                          {'probability': 0.85, 'generator': new_bet},\n",
    "                          {'probability': 0.05, 'generator': new_win}]\n",
    "\n",
    "    group1_events = generate_events(fake, num_users_group1, group1_events_dist, events_per_user, True)\n",
    "    group2_events = generate_events(fake, num_users_group2, group2_events_dist, events_per_user, False)\n",
    "\n",
    "    events = (group1_events + group2_events)\n",
    "    events.sort(key=lambda event: event.get('event_time'))\n",
    "\n",
    "    # ingest events to stream\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(events), batch_size):\n",
    "        # Convert the events to records\n",
    "        records = [{'data': json.dumps(event, default=str)} for event in events[i:i+batch_size]]\n",
    "        v3io_client.batch.stream.put_records(container=container, stream_path=output_stream_path, records=records)\n",
    "\n",
    "    return v3io_client.batch.wait()\n",
    "\n",
    "\n",
    "def create_enrichment_table(v3io_client, container, enrichment_table_path):\n",
    "    for postcode in range(10000, 99999):\n",
    "        remainder = postcode % 3\n",
    "        if remainder == 0:\n",
    "            idx = randint(3, 5)\n",
    "        elif remainder == 1:\n",
    "            idx = randint(1, 3)\n",
    "        else:\n",
    "            idx = randint(5, 7)\n",
    "\n",
    "        attr = {'postcode': postcode, 'socioeconomic_idx': idx}\n",
    "        v3io_client.batch.kv.put(container=container,\n",
    "                                 table_path=enrichment_table_path,\n",
    "                                 key=str(postcode),\n",
    "                                 attributes=attr)\n",
    "    return v3io_client.batch.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(context: MLClientCtx,\n",
    "         container: str,\n",
    "         output_stream_path: str,\n",
    "         enrichment_table_path: str,\n",
    "         num_users_group1: int = 1400,\n",
    "         num_users_group2: int = 600,\n",
    "         events_per_user: int = 1000) -> None:\n",
    "\n",
    "    v3io_client = v3io.dataplane.Client(endpoint=os.getenv('V3IO_API'),\n",
    "                                        access_key=os.getenv('V3IO_ACCESS_KEY'))\n",
    "    \n",
    "    # generate enrichment table\n",
    "    table_resps = create_enrichment_table(v3io_client, container, enrichment_table_path)\n",
    "    written_items = sum(int(resp.status_code == 200) for resp in table_resps)\n",
    "    context.logger.info(f'Created enrichment table with {written_items} items')\n",
    "    \n",
    "    # generate event stream\n",
    "    stream_resps = generate_event_stream(v3io_client, container, output_stream_path,\n",
    "                                         num_users_group1, num_users_group2, events_per_user)\n",
    "\n",
    "    records_sent = sum(len(json.loads(resp.body)['Records']) for resp in stream_resps)\n",
    "    context.logger.info(f'Records sent {records_sent}')\n",
    "\n",
    "    failed_records = sum(json.loads(resp.body)['FailedRecordCount'] for resp in stream_resps)\n",
    "\n",
    "    if failed_records > 0:\n",
    "        context.logger.warn(f'Failed to stream {failed_records}')\n",
    "    else:\n",
    "        context.logger.info('All data streamed successfully.')\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the `# nuclio: end-code` marker to mark the end of a Nuclio code section and instruct Nuclio to stop parsing the notebook at this point.<br>\n",
    "> **IMPORTANT:** Do not remove the end-code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert code to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MLRun `code_to_function` in order to convert the python code to a Nuclio function. We then set the relevant enrivonment variables and streaming trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fdc831d7f50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlrun\n",
    "import os\n",
    "import v3io\n",
    "\n",
    "mlrun.mlconf.dbpath = mlrun.mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlrun.mlconf.artifact_path = mlrun.mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'\n",
    "\n",
    "fn = mlrun.code_to_function(name='data-generator', kind = 'job', handler='main', image=\"mlrun/mlrun\")\n",
    "fn.spec.readinessTimeoutSeconds = 200\n",
    "fn.spec.build.commands = ['pip install faker']\n",
    "\n",
    "fn.apply(mlrun.platforms.v3io_cred())\n",
    "fn.apply(mlrun.mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test target stream \n",
    "test_path = os.path.join(os.getcwd(), 'test')\n",
    "\n",
    "v3io_client = v3io.dataplane.Client()\n",
    "container = 'users'\n",
    "output_stream_path = os.path.join(test_path.replace('/User', os.getenv('V3IO_USERNAME')), 'data-generator-stream')\n",
    "v3io_client.stream.create(container=container, stream_path=output_stream_path, shard_count=1)\n",
    "\n",
    "#set enrichment table path\n",
    "enrichment_table_path = os.path.join(test_path.replace('/User', os.getenv('V3IO_USERNAME')), 'data-generator-table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters and  environment variables\n",
    "envs = {'V3IO_API': os.getenv('V3IO_API'),\n",
    "        'V3IO_ACCESS_KEY': os.getenv('V3IO_ACCESS_KEY')}\n",
    "params = {'container': 'users',\n",
    "         'output_stream_path': output_stream_path,\n",
    "         'enrichment_table_path': enrichment_table_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure function instances\n",
    "Here we configure a function instances for each of the streams we want to use `stream to parquet` upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fdc831d7f50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.set_envs(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-10-03 08:05:27,833 [info] Started building image: .mlrun/func-default-data-generator:latest\n",
      "E1003 08:06:10.785107       1 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.\n",
      "\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors\n",
      "\u001b[36mINFO\u001b[0m[0040] Retrieving image manifest mlrun/mlrun:0.7.0-rc7 \n",
      "\u001b[36mINFO\u001b[0m[0042] Retrieving image manifest mlrun/mlrun:0.7.0-rc7 \n",
      "\u001b[36mINFO\u001b[0m[0045] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0045] Retrieving image manifest mlrun/mlrun:0.7.0-rc7 \n",
      "\u001b[36mINFO\u001b[0m[0047] Retrieving image manifest mlrun/mlrun:0.7.0-rc7 \n",
      "\u001b[36mINFO\u001b[0m[0049] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0049] Unpacking rootfs as cmd RUN pip install faker requires it. \n",
      "\u001b[36mINFO\u001b[0m[0072] RUN pip install faker                        \n",
      "\u001b[36mINFO\u001b[0m[0072] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0079] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0079] args: [-c pip install faker]                 \n",
      "\u001b[36mINFO\u001b[0m[0079] Running: [/bin/sh -c pip install faker]      \n",
      "Collecting faker\n",
      "  Downloading Faker-8.14.1-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/site-packages (from faker) (2.8.2)\n",
      "Collecting text-unidecode==1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Installing collected packages: text-unidecode, faker\n",
      "Successfully installed faker-8.14.1 text-unidecode-1.3\n",
      "WARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0083] Taking snapshot of full filesystem...        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "fn.run(params=params,\n",
    "      artifact_path=mlconf.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!rm -rf {test_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
