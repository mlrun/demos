{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Handler\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function designed to output the incoming events to a configurable output.<br>\n",
    "Currently the following outputs are supported:<br>\n",
    "  * V3IO Stream output\n",
    "        The events will be written to V3IO Stream and will be partitioned across the different stream shards based upon configurable field of the incoming events.\n",
    "  * Parquet\n",
    "        Each batch size (default 1024 records) are stored in parquet file partitioned by the event time (year, month, day, hour)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Test a Local Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nuclio](https://nuclio.io/) is a high-performance open-source and managed serverless framework, which is available as a predefined tenant-wide platform service (`nuclio`).\n",
    "The demo uses Nuclio to create and deploy serverless functions.\n",
    "Therefore, you need to import the Nuclio package and configure Nuclio for your project.\n",
    "\n",
    "The platform's Jupyter Notebook service preinstalls the [nuclio-jupyter SDK](https://github.com/nuclio/nuclio-jupyter/blob/master/README.md) for creating and deploying Nuclio functions with Python and Jupyter Notebook.\n",
    "The tutorial uses the Nuclio magic commands and annotation comments of this SDK to automate function code generation.\n",
    "The magic commands are initialized when you import the `nuclio` package.<br>\n",
    "The `%nuclio` magic commands are used to run Nuclio commands from Jupyter notebooks (`%nuclio <Nuclio command>`).\n",
    "You can also use `%%nuclio` at the start of a cell to identify the entire cell as containing Nuclio code.\n",
    "The magic commands are initialized when you import the `nuclio` package.<br>\n",
    "The `# nuclio: start-code`, `# nuclio: end-code`, and `# nuclio: ignore` section-marker annotations notify Nuclio of the beginning or end of code sections.\n",
    "Nuclio ignores all notebook code before a `# nuclio: start-code` marker or after an `# nuclio: end-code` marker.\n",
    "Nuclio translates all other notebook code sections into function code, except for sections that are marked with the `# nuclio: ignore` marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the `nuclio` Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the `# nuclio: start-code` marker to instruct Nuclio to start processing code only from this location, and then performs basic Nuclio function configuration &mdash; defining the name of the function's container image (`mlrun/ml-models`), the function type (`nuclio`), and some additional package installation commands.\n",
    "\n",
    "> **Note:** You can add code to define function dependencies and perform additional configuration after the `# nuclio: start-code` marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio config\n",
    "spec.build.baseImage = \"mlrun/mlrun\"\n",
    "spec.readinessTimeoutSeconds = 200\n",
    "kind = \"nuclio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import v3io.dataplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    setattr(context, 'stream_sink_flag', os.getenv('STREAM_SINK_FLAG'))\n",
    "    setattr(context, 'parquet_sink_flag', os.getenv('PARQUET_SINK_FLAG'))\n",
    "\n",
    "    # For writing to parquet\n",
    "    if context.parquet_sink_flag.lower() == 'true':\n",
    "        setattr(context, 'batch', [])\n",
    "        setattr(context, 'batch_size', int(os.getenv('PARQUET_BATCH_SIZE', 1024)))\n",
    "\n",
    "        setattr(context, 'timestamp_key', os.getenv('TS_KEY'))\n",
    "        setattr(context, 'timestamp_format', os.getenv('TS_FORMAT', '%Y-%m-%d %H:%M:%S.%f'))\n",
    "\n",
    "        setattr(context, 'pq_partitions', ['pq_year', 'pq_month', 'pq_day', 'pq_hour'])\n",
    "\n",
    "        setattr(context, 'target_path', os.getenv('PARQUET_TARGET_PATH'))\n",
    "        os.makedirs(context.target_path, exist_ok=True)\n",
    "\n",
    "        # in case of an inference stream set the names of features and predictions.\n",
    "        features = os.getenv('FEATURES')\n",
    "        if features is not None:\n",
    "            features = features.split(',')\n",
    "        setattr(context, 'features', features)\n",
    "\n",
    "        predictions = os.getenv('PREDICTIONS')\n",
    "        if predictions is not None:\n",
    "            predictions = predictions.split(',')\n",
    "        setattr(context, 'predictions', predictions)\n",
    "\n",
    "    # For writing to v3io stream\n",
    "    if context.stream_sink_flag.lower() == 'true':\n",
    "        v3io_access_key = os.getenv('V3IO_ACCESS_KEY')\n",
    "        container = os.getenv('CONTAINER')\n",
    "        output_stream_path = os.getenv('OUTPUT_STREAM_PATH')\n",
    "        partition_attr = os.getenv('PARTITION_ATTR')\n",
    "        v3io_api = os.getenv('V3IO_API')\n",
    "        v3io_client = v3io.dataplane.Client(endpoint=v3io_api, access_key=v3io_access_key)\n",
    "\n",
    "        setattr(context, 'v3io_client', v3io_client)\n",
    "        setattr(context, 'partition_attr', partition_attr)\n",
    "        setattr(context, 'container', container)\n",
    "        setattr(context, 'output_stream_path', output_stream_path)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    if type(event.body) is dict:\n",
    "        event_dict = event.body\n",
    "    else:\n",
    "        event_dict = json.loads(event.body)\n",
    "\n",
    "    context.logger.info_with('Got invoked',\n",
    "                             trigger_kind=event.trigger.kind,\n",
    "                             event_body=event_dict)\n",
    "\n",
    "    if context.stream_sink_flag.lower() == 'true':\n",
    "        stream_sink_handler(context, event_dict)\n",
    "    if context.parquet_sink_flag.lower() == 'true':\n",
    "        parquet_sink_handler(context, event_dict)\n",
    "    pass\n",
    "\n",
    "\n",
    "def stream_sink_handler(context, event):\n",
    "    partition_key = event.get(context.partition_attr)\n",
    "    record = event_to_record(event, partition_key)\n",
    "    \n",
    "    resp = context.v3io_client.stream.put_records(container=context.container,\n",
    "                                                  stream_path=context.output_stream_path,\n",
    "                                                  records=[record],\n",
    "                                                  raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "\n",
    "    context.logger.info_with('Sent event to stream',\n",
    "                             record=record,\n",
    "                             response_status=resp.status_code,\n",
    "                             response_body=resp.body.decode('utf-8'))\n",
    "    pass\n",
    "\n",
    "\n",
    "def event_to_record(event_dict, partition_key):\n",
    "    event_str = json.dumps(event_dict)\n",
    "    return {'data': event_str, 'partition_key': str(partition_key)}\n",
    "\n",
    "\n",
    "def parquet_sink_handler(context, event):\n",
    "    # for inference events\n",
    "    if context.features is not None and context.predictions is not None:\n",
    "        event = flatten_inference_event(context, event)\n",
    "\n",
    "    event_with_time_partitions = add_time_partition_attributes(context, event)\n",
    "\n",
    "    # add the incoming event to the current batch\n",
    "    context.batch.append(event_with_time_partitions)\n",
    "\n",
    "    # check if batch size reached\n",
    "    if context.batch_size == len(context.batch):\n",
    "        written_records = write_batch(context)\n",
    "        context.logger.info_with('Written batch',\n",
    "                                 Writtent_records=written_records)\n",
    "    pass\n",
    "\n",
    "\n",
    "def flatten_inference_event(context, event):\n",
    "    # add parsed features to the event\n",
    "    feature_values = event['request']['instances'][0]\n",
    "    event.update(zip(context.features, feature_values))\n",
    "\n",
    "    # add parsed predictions to the event\n",
    "    prediction_values = event['resp']\n",
    "    event.update(zip(context.predictions, prediction_values))\n",
    "\n",
    "    return event\n",
    "\n",
    "\n",
    "def add_time_partition_attributes(context, event):\n",
    "    if hasattr(context, 'timestamp_key') and event.get(context.timestamp_key) is not None:\n",
    "        # parse the event time\n",
    "        dt_object = datetime.strptime(event[context.timestamp_key], context.timestamp_format)\n",
    "    else:\n",
    "        # if event time is missing or not configured, use current datetime\n",
    "        dt_object = datetime.now()\n",
    "\n",
    "    # add the partition attributes\n",
    "    event['pq_year'] = dt_object.strftime('%Y')\n",
    "    event['pq_month'] = dt_object.strftime('%m')\n",
    "    event['pq_day'] = dt_object.strftime('%d')\n",
    "    event['pq_hour'] = dt_object.strftime('%H')\n",
    "\n",
    "    return event\n",
    "\n",
    "\n",
    "def write_batch(context):\n",
    "    df = pd.DataFrame.from_records(context.batch)\n",
    "    df.to_parquet(path=context.target_path, partition_cols=context.pq_partitions)\n",
    "    # post write cleanup\n",
    "    context.batch = []\n",
    "    return len(df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the `# nuclio: end-code` marker to mark the end of a Nuclio code section and instruct Nuclio to stop parsing the notebook at this point.<br>\n",
    "> **IMPORTANT:** Do not remove the end-code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v3io.dataplane\n",
    "\n",
    "test_path = os.path.join(os.getcwd(), 'test')\n",
    "\n",
    "# Create a test targer dir for the parquet output\n",
    "target_path = os.path.join(test_path, 'event-handler-pq')\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "# Create a test target stream\n",
    "v3io_client = v3io.dataplane.Client()\n",
    "container = 'users'\n",
    "output_stream_path = os.path.join(test_path.replace('/User', os.getenv('V3IO_USERNAME')), 'event-handler-stream')\n",
    "v3io_client.stream.create(container=container, stream_path=output_stream_path, shard_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set few parameters via environment variables0\n",
    "envs = {'PARQUET_SINK_FLAG': 'true',\n",
    "        'STREAM_SINK_FLAG': 'true',\n",
    "        'PARQUET_TARGET_PATH' : target_path,\n",
    "        'PARQUET_BATCH_SIZE': 10,\n",
    "        'TS_KEY': 'event_time',\n",
    "        'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f',\n",
    "        'CONTAINER': container,\n",
    "        'OUTPUT_STREAM_PATH': output_stream_path,\n",
    "        'PARTITION_ATTR': 'user_id'}\n",
    "\n",
    "for key, value in envs.items():\n",
    "    os.environ[key] = str(value)\n",
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger with 9 events:\n",
    "\n",
    "nine_events = [b'{\"user_id\" : 1 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:22.333332\"}',\n",
    "              b'{\"user_id\" : 2 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:23.333332\"}',\n",
    "              b'{\"user_id\" : 3 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:24.333332\"}',\n",
    "              b'{\"user_id\" : 4 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:25.333332\"}',\n",
    "              b'{\"user_id\" : 5 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:26.333332\"}',\n",
    "              b'{\"user_id\" : 6 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:27.333332\"}',\n",
    "              b'{\"user_id\" : 7 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:28.333332\"}',\n",
    "              b'{\"user_id\" : 8 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:29.333332\"}',\n",
    "              b'{\"user_id\" : 9 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:30.333332\"}']\n",
    "\n",
    "for e in nine_events:\n",
    "    event = nuclio.Event(body=e)\n",
    "    handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether a parquet has been created\n",
    "!ls -l {target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger the tenth event which should trigger the creation of the parquet file.\n",
    "tenth_event = b'{\"user_id\" : 10 , \"event_type\": \"spin\", \"event_time\": \"2020-02-02 12:20:31.333332\"}'\n",
    "event = nuclio.Event(body=tenth_event)\n",
    "handler(context, event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weather a parquet has been created\n",
    "!ls -l {target_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "!rm -rf {test_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclio Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `target_path`s for the parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert code to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MLRun `code_to_function` in order to convert the python code to a Nuclio function. We then set the relevant enrivonment variables and streaming trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "\n",
    "fn = code_to_function(name='event-handler', kind = 'nuclio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure function instances\n",
    "Here we configure a function instances for each of the streams we want to use `stream to parquet` upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.set_envs(envs)\n",
    "# Configure a mount on the nuclio function from '/User' to our home directory '~/'.\n",
    "fn.apply(mount_v3io())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
