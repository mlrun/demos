{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model-deployment-with-streaming\"></a>\n",
    "\n",
    "\n",
    "Deploy a model with streaming information. The demo covers the use case of 1<sup>st</sup>-day churn.\n",
    "\n",
    "The importance of 1<sup>st</sup>-day churn prediction:\n",
    "- In some segments of the gaming industry, the average 1st day churn is as high as 70%.\n",
    "- Acquiring new customers is 5x&ndash;25x more expensive than retaining existing ones.\n",
    "- Reducing churn by just 5% can boost profitability by 75%.\n",
    "- Improving retention has a 2x&ndash;4x greater impact on growth than acquisition.\n",
    "- The probability of selling to an existing customer is 60%&ndash;70%, but only 5%&ndash;20% for a prospect.\n",
    "- Churn rate also informs metrics like customer lifetime value (LTV).\n",
    "\n",
    "This demo is comprised of several steps:\n",
    "\n",
    "![Model deployment Pipeline Real-time operational Pipeline](assets/model-deployment-pipeline.png)\n",
    "\n",
    "While this demo covers the use case of 1<sup>st</sup>-day churn, it is easy to replace the data, related features and training model and reuse the same workflow for different business cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are covered by the following demo:\n",
    "\n",
    "- [**1. Data generator**](functions/data-generator.ipynb) â€” Generates events for the training and serving and Create an enrichment table (lookup values). \n",
    "- [**2. Event handler**](functions/event-handler.ipynb) - Receive data from the input. This is a common input stream for all the data. This way, one can easily replace the event source data (in this case we have a data generator) without affecting the rest of this flow. It also store all incoming data to parquet files.\n",
    "- [**3. Stream to features**](functions/stream-to-features.ipynb) - Enrich the stream using the enrichment table and Update aggregation features using the incoming event handler.\n",
    "- **4. Optional model training steps -**\n",
    " - [**4.1 Get Data Snapshot**](https://github.com/mlrun/functions/tree/master/describe) - Takes a snapshot of the feature table for training.\n",
    "  - [**4.2 Describe the Dataset**](functions/get-data-snapshot.ipynb) - Runs common analysis on the datasets and produces plots suche as histogram, feature importance, corollation and more.\n",
    "  - [**4.3 Training**](https://github.com/mlrun/functions/tree/master/sklearn_classifier) - Runing training with multiple classification models.\n",
    "  - [**4.4 Testing**](https://github.com/mlrun/functions/tree/master/test_classifier) - Testing the best performing model.\n",
    "- [**5. Serving**](https://github.com/mlrun/functions/tree/master/model_server) - Serve the model and process the data from the enriched stream and aggregation features.\n",
    "- [**6. Inference logger**](functions/event-handler.ipynb) - We use the same event handler function from above but only its capability to store incoming data to parquet files.\n",
    "\n",
    "This demo comes with a pre-trained model using the base features, enrichment data and derived features, calculated using the same generated data. You can retrain the model or train a new model by running the  **optional model training steps**. You will need to ensure enough data is collected via the streams to the data storage in order to train a new model.\n",
    "\n",
    "## About this demo\n",
    "\n",
    "### Input Data\n",
    "\n",
    "The data generator ([data-generator.ipynb](functions/-generator.ipynb)) creates the following events: `new_registration`, `new_purchases`, `new_bet` and `new_win` with the following data:\n",
    "\n",
    "| new_registration |   | new_purchases |   | new_bet    |   | new_win    |\n",
    "|------------------|---|---------------|---|------------|---|------------|\n",
    "| user_id          |   | user_id       |   | user_id    |   | user_id    |\n",
    "| event_type       |   | event_type    |   | event_type |   | event_type |\n",
    "| event_time       |   | event_time    |   | event_time |   | event_time |\n",
    "| name             |   | amount        |   | bet_amount |   | win_amount |\n",
    "| date_of_birth    |   |               |   |            |   |            |\n",
    "| street_address   |   |               |   |            |   |            |\n",
    "| city             |   |               |   |            |   |            |\n",
    "| country          |   |               |   |            |   |            |\n",
    "| postcode         |   |               |   |            |   |            |\n",
    "| affiliate_url    |   |               |   |            |   |            |\n",
    "| campaign         |   |               |   |            |   |            |\n",
    "\n",
    "Furthermore, `new_registration` includes a `label` column to indicate whether or not the user has churned (1 for churned and 0 for not)\n",
    "\n",
    "## Enrichment\n",
    "\n",
    "The same data generator ([data-generator.ipynb](functions/-generator.ipynb)) also creates the enrichment table which contains a lookup of postcode and returns a socioeconomic index (`socioeconomic_idx`).\n",
    "\n",
    "## Feature calculation\n",
    "\n",
    "During the feature calculation ([stream-to-features.ipynb](functions/stream-to-features.ipynb)), enriches the events using the enrichment table and calculates sum, mean, count and variance for the 3 amount fields (`amount`, `bet_amount` and `win_amount` for `new_purchases`, `new_bet` and `new_win` respectively). This results with the following list of fields:\n",
    "\n",
    "- purchase_sum\n",
    "- purchase_mean\n",
    "- purchase_count\n",
    "- purchase_var\n",
    "- bet_sum\n",
    "- bet_mean\n",
    "- bet_count\n",
    "- bet_var\n",
    "- win_sum\n",
    "- win_mean\n",
    "- win_count\n",
    "- win_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration below is shared across the notebooks. Change the values in this subsection if you would like different configuration settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects in the platform are used to package multiple functions, workflows, and artifacts. Set here the project base name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_base_name = \"model-deployment-pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data in the platform is stored in user-defined data containers. In this case we use the predefined \"users\" container. For more information refer to [Data containers, collections, and objects documentation](https://www.iguazio.com/docs/latest-release/concepts/containers-collections-objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = 'users'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data path where to store stream data and kv tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv, path, getcwd\n",
    "\n",
    "v3io_username = getenv('V3IO_USERNAME')\n",
    "data_path = path.join(v3io_username, 'examples',project_base_name, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the different stream information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "web_api = \"http://v3io-webapi:8081\"\n",
    "web_api_users = urljoin(web_api, container)\n",
    "stream_configs = {'generated-stream': {\n",
    "                        'path': path.join(data_path, 'generated-stream'),\n",
    "                        'shard_count': 8},\n",
    "                  'incoming-events-stream': {\n",
    "                        'path': path.join(data_path, 'incoming-events-stream'),\n",
    "                        'shard_count': 8\n",
    "                  },\n",
    "                  'serving-stream': {\n",
    "                        'path': path.join(data_path, 'serving-stream'),\n",
    "                        'shard_count': 8\n",
    "                  },\n",
    "                  'inference-stream': {\n",
    "                        'path': path.join(data_path, 'inference-stream'),\n",
    "                        'shard_count': 8\n",
    "                  }\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we stream data, we associate the records with a specific partition key to ensure that similar records are assigned to the same shard. For more information, see the [stream sharding and partitioning description](https://www.iguazio.com/docs/latest-release/concepts/streams/#stream-sharding-and-partitioning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_attr = \"user_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the raw data and the inference data as parquet files.\n",
    "The parquet files will be written via file mount, hence we configure the path to start with '/User' which will be mounted to our home dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_parquet_target_path = path.join(data_path.replace(v3io_username, '/User'),  'events-pq')\n",
    "inference_parquet_target_path = path.join(data_path.replace(v3io_username, '/User'),  'inference-pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the enrichment table (a key-value table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_table_path = path.join(data_path, 'enrichment-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target path to store the calculated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table_path = path.join(data_path, 'feature-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['socioeconomic_idx','purchase_sum','purchase_mean','purchase_count',\n",
    "                'purchase_var','bet_sum','bet_mean','bet_count',\n",
    "                'bet_var','win_sum','win_mean','win_count','win_var']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create V3IO Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataplane client you can manipulate data in the platform's multi-model data layer, including:\n",
    "* Objects\n",
    "* Key-values (NoSQL)\n",
    "* Streams\n",
    "* Containers\n",
    "\n",
    "Under the hood, the client connects through the platform's web API (https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/) and wraps each low level API with an interface. Calls are blocking, but you can use the batching interface to send multiple requests in parallel for greater performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import v3io.dataplane\n",
    "v3io_client = v3io.dataplane.Client(endpoint=web_api,\n",
    "                                    access_key=getenv('V3IO_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup previous streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_name, stream_config in stream_configs.items():\n",
    "    resp = v3io_client.stream.delete(container=container, stream_path=stream_config['path'], \n",
    "                                     raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "    print(f'Delete Stream call for stream {stream_name} returned with status {resp.status_code}, and content: {resp.body.decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_name, stream_config in stream_configs.items():\n",
    "    print(stream_config['path'])\n",
    "    resp = v3io_client.stream.create(container=container,\n",
    "                                     stream_path=stream_config['path'],\n",
    "                                     shard_count=stream_config['shard_count'],\n",
    "                                     raise_for_status=v3io.dataplane.RaiseForStatus.never)\n",
    "    print(f'Create Stream call for stream {stream_name} returned with status {resp.status_code}, and content: {resp.body.decode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up MLRun Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projects are created by using the `new_project` MLRun method, which receives the following parameters:\n",
    "\n",
    "- **`name`** (Required) &mdash; the project name.\n",
    "- **`context`** &mdash; the path to a local project directory (the project's context directory).\n",
    "  The project directory contains a project-configuration file (default: **project.yaml**), which defines the project, and additional generated Python code.\n",
    "  The project file is created when you save your project (using the `save` MLRun project method), as demonstrated in Step 6.\n",
    "- **`functions`** &mdash; a list of functions objects or links to function code or objects.\n",
    "- **`init_git`** &mdash; set to `True` to perform Git initialization of the project directory (`context`).\n",
    "  > **Note:** It's customary to store project code and definitions in a Git repository.\n",
    "\n",
    "Projects are visible in the MLRun dashboard only after they're saved to the MLRun database, which happens whenever you run code for a project.\n",
    "\n",
    "The following code creates a project using the `PROJECT_BASE_NAME`, concatenated with your current running username in the platform (**&lt;V3IO_USERNAME&gt;**), and sets the project directory to a **conf** directory in the current demo directory (**/User/demos/model-deployment-with-streaming/conf**).\n",
    "\n",
    "> **Note:** Platform projects are shared among all users of the parent tenant, to facilitate collaboration. Therefore,\n",
    ">\n",
    "> - Synchronize your projects execution with other users on your platform cluster, as needed, or use unique project names to avoid conflicts.\n",
    ">   You can easily change the default project name for this tutorial by changing the definition of the `PROJECT_BASE_NAME` variable, defined in the beginning of the notebook.\n",
    "> - Don't include in your project proprietary information that you don't want to expose to other users.\n",
    ">   Note that while projects are a useful tool, you can easily develop and run code in the platform without using projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project\n",
    "\n",
    "project_name = '-'.join(filter(None, [project_base_name, getenv('V3IO_USERNAME', None)]))\n",
    "project_path = path.abspath('conf')\n",
    "project = new_project(project_name, project_path, init_git=True)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MLRun](https://github.com/mlrun/mlrun) is a generic and convenient mechanism for data scientists and software developers to describe and run tasks related to machine learning in various, scalable runtime environments and ML pipelines while automatically tracking executed code, metadata, inputs, and outputs.\n",
    "MLRun integrates with the Nuclio serverless framework and with the Kubeflow Pipelines framework for running ML pipelines.\n",
    "The demo uses MLRun to create a project, run Nuclio serverless functions, as well as run the model training.\n",
    "Before running your code, you need to set some MLRun configurations:\n",
    "\n",
    "- <a id=\"gs-mlrun-config-artifcats-path\"></a>**Artifacts path** &mdash; the location for storing versioned data artifacts (such as files, objects, data sets, and models) that are produced or consumed by functions, runs, and workflows.\n",
    "  The path can be defined either as a local directory path or as a URL (of the format `s3://*`, `v3io://*`, etc.).\n",
    "  You can set the artifacts path either by defining an `MLRUN_ARTIFACT_PATH` environment variable (which applies globally throughout the current environment) or as part of the MLRun configuration.\n",
    " \n",
    "  If the target directory doesn't exist, MLRun creates it.\n",
    "  You can use the notation `{{run.uid}}` in the path to signify the current run ID.\n",
    "  For pipelines, you can use the notation `{{workflow.uid}}` to signify the workflow ID.\n",
    "  This allows you to create a unique artifacts directory for each executed job or workflow.\n",
    "\n",
    "  After you run an MLRun job, the artifacts directory might contain one or more of the following directories:\n",
    " \n",
    "  - **plots** &mdash; a directory for storing images, figures, and plotlines.\n",
    "  - **models** &mdash; a directory for storing all trained models.\n",
    "  - **data** &mdash; a directory for storing any other type of data artifact, such as data sets.\n",
    "\n",
    "The following code sets the artifacts path to a **artifacts** directory within the tutorial directory (**/User/demos/model-deployment-with-streaming/artifacts**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "\n",
    "# Target location for storing pipeline artifacts\n",
    "project.artifact_path = path.abspath('artifacts')\n",
    "# MLRun DB path or API service URL\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "print(f'Artifacts path: {project.artifact_path}\\nMLRun DB path: {mlconf.dbpath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set project's functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mount_v3io, code_to_function, NewTask\n",
    "import nuclio\n",
    "\n",
    "data_generator = code_to_function(name='data-generator', handler='main', kind='job', filename='functions/data-generator.ipynb')\n",
    "project.set_function(data_generator)\n",
    "\n",
    "# set parameters and  environment variables\n",
    "v3io_envs = {'V3IO_API': getenv('V3IO_API'),\n",
    "        'V3IO_ACCESS_KEY': getenv('V3IO_ACCESS_KEY')}\n",
    "dg_params = {'container': container,\n",
    "         'output_stream_path': stream_configs['generated-stream']['path'],\n",
    "         'enrichment_table_path': enrichment_table_path}\n",
    "\n",
    "project.func('data-generator').set_envs(v3io_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the image\n",
    "project.func('data-generator').deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the job\n",
    "project.func('data-generator').run(params=dg_params, artifact_path=project.artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_handler = code_to_function(name='event-handler', handler='handler', kind='nuclio', filename='functions/event-handler.ipynb')\n",
    "project.set_function(event_handler)\n",
    "\n",
    "eh_envs = {'PARQUET_SINK_FLAG': 'true',\n",
    "           'STREAM_SINK_FLAG': 'true',\n",
    "           'PARQUET_TARGET_PATH' : raw_parquet_target_path,\n",
    "           'PARQUET_BATCH_SIZE': 8192,\n",
    "           'TS_KEY': 'event_time',\n",
    "           'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f',\n",
    "           'CONTAINER': container,\n",
    "           'OUTPUT_STREAM_PATH': stream_configs['incoming-events-stream']['path'],\n",
    "           'PARTITION_ATTR': partition_attr}\n",
    "\n",
    "project.func('event-handler').set_envs({**v3io_envs, **eh_envs})\n",
    "project.func('event-handler').apply(mount_v3io())\n",
    "\n",
    "generated_stream = '/'.join(s.strip('/') for s in [web_api_users, stream_configs['generated-stream']['path']]) + '@eh'\n",
    "project.func('event-handler').add_trigger('serving_stream',\n",
    "                                           nuclio.triggers.V3IOStreamTrigger(url=generated_stream,\n",
    "                                                                             maxWorkers=stream_configs['generated-stream']['shard_count']+2,\n",
    "                                                                             seekTo='earliest'))\n",
    "\n",
    "project.func('event-handler').spec.replicas=1\n",
    "project.func('event-handler').deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stream to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_to_features = code_to_function(name='stream-to-features', handler='handler', kind='nuclio', filename='functions/stream-to-features.ipynb')\n",
    "project.set_function(stream_to_features)\n",
    "\n",
    "stf_envs = {'FEATURE_TABLE_PATH': feature_table_path,\n",
    "            'SERVING_EVENTS': \",\".join(['bet','win']),\n",
    "            'FEATURE_LIST': \",\".join(feature_list),\n",
    "            'CONTAINER': container,\n",
    "            'OUTPUT_STREAM_PATH': stream_configs['serving-stream']['path'],\n",
    "            'PARTITION_ATTR': partition_attr,\n",
    "            'ENRICHMENT_TABLE_PATH': enrichment_table_path,\n",
    "            'ENRICHMENT_KEY':\"postcode\"}\n",
    "\n",
    "project.func('stream-to-features').set_envs({**v3io_envs, **stf_envs})\n",
    "\n",
    "incoming_events_stream = '/'.join(s.strip('/') for s in [web_api_users, stream_configs['incoming-events-stream']['path']]) + '@stf'\n",
    "project.func('stream-to-features').add_trigger('serving_stream',\n",
    "                                               nuclio.triggers.V3IOStreamTrigger(url=incoming_events_stream,\n",
    "                                                                                 maxWorkers=stream_configs['incoming-events-stream']['shard_count']+2,\n",
    "                                                                                 seekTo='earliest'))\n",
    "\n",
    "project.func('stream-to-features').spec.readiness_timeout = 200\n",
    "project.func('stream-to-features').spec.replicas=1\n",
    "project.func('stream-to-features').deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data Snapshot (part of optional model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    get_data_snapshot = code_to_function(name='get-data-snapshot', handler='snapshot_data', kind='job', filename='functions/get-data-snapshot.ipynb')\n",
    "    project.set_function(get_data_snapshot)\n",
    "\n",
    "    # set parameters and  environment variables\n",
    "    v3io_envs = {'V3IO_API': getenv('V3IO_API'),\n",
    "            'V3IO_ACCESS_KEY': getenv('V3IO_ACCESS_KEY')}\n",
    "    gds_params = {'container': container, \n",
    "                  'table_path': feature_table_path, \n",
    "                  'columns': ['label']+feature_list, \n",
    "                  'format': 'csv'}\n",
    "\n",
    "    project.func('get-data-snapshot').set_envs(v3io_envs)\n",
    "    project.func('get-data-snapshot').apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    project.func('get-data-snapshot').deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    snapshot_data_run = project.func('get-data-snapshot').run(params=gds_params, artifact_path=project.artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the Dataset (part of optional model training)\n",
    "-------------------\n",
    "You can review the plots under - artifacts/plots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    project.set_function('hub://describe', 'describe')\n",
    "\n",
    "    project.func('describe').apply(mount_v3io())\n",
    "    describe_run = project.func('describe').run(params={'label_column': 'label'},\n",
    "                                inputs={\"table\":\n",
    "                                        snapshot_data_run.outputs['snapshot_dataset']},\n",
    "                                artifact_path=project.artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training (part of optional model training)\n",
    "---------------------\n",
    "function's source and full docstrings can be found at https://github.com/mlrun/functions/tree/master/sklearn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    project.set_function('hub://sklearn_classifier', 'train')\n",
    "    project.func('train').apply(mount_v3io())\n",
    "    \n",
    "    # Configure the models to train\n",
    "    models = [\"sklearn.ensemble.RandomForestClassifier\", \n",
    "              \"sklearn.linear_model.LogisticRegression\",\n",
    "              \"sklearn.ensemble.AdaBoostClassifier\"]\n",
    "    \n",
    "    # Create a training task\n",
    "    train_task = NewTask(name=\"train\",\n",
    "                         params={\"sample\": -1,\n",
    "                                 \"label_column\": \"label\",\n",
    "                                 \"test_size\": 0.10},\n",
    "                         inputs={\"dataset\": snapshot_data_run.outputs['snapshot_dataset']})\n",
    "    \n",
    "    # Run the training task\n",
    "    train_run = project.func('train').run(train_task.with_hyper_params({'model_pkg_class': models},\n",
    "                                                                        selector='max.accuracy'),\n",
    "                                                                        artifact_path=project.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    # Display the name of the selected model\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    display(HTML(f'<b>Best model:</b> '\n",
    "                 f'{models[train_run.outputs[\"best_iteration\"]-1]}'))\n",
    "\n",
    "    # Display the accuracy for the optimal run iteration\n",
    "    display(HTML(f'<b>Accuracy:</b> {train_run.outputs[\"accuracy\"]}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing (part of optional model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    project.set_function('hub://test_classifier', 'test')\n",
    "    project.func('test').apply(mount_v3io())\n",
    "    \n",
    "    test_task = NewTask(name=\"test\",\n",
    "                        params={\"label_column\": \"label\",\n",
    "                                \"plots_dest\": path.join(\"plots\", \"test\")},\n",
    "                        inputs={\"models_path\": train_run.outputs['model'],\n",
    "                                \"test_set\": train_run.outputs['test_set']}\n",
    "                        )\n",
    "    test_run = project.func('test').run(test_task,\n",
    "                        artifact_path=project.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    # Display the model accuracy\n",
    "    display(HTML(f'<b>Test Accuracy:</b> {test_run.outputs[\"accuracy\"]}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_function('hub://model_server:development', 'serving')\n",
    "\n",
    "serving = project.func('serving').apply(mount_v3io())\n",
    "if 'train_run' in locals() and train_run.outputs.get('model') is not None:\n",
    "    serving.add_model('my_model', train_run.outputs.get('model'))\n",
    "else:\n",
    "    serving.add_model('my_model', path.join(getcwd(), 'assets/model.pkl'))\n",
    "        \n",
    "serving.set_envs({'INFERENCE_STREAM' : path.join(container, stream_configs['inference-stream']['path']) })\n",
    "\n",
    "serving_stream = '/'.join(s.strip('/') for s in [web_api_users, stream_configs['serving-stream']['path']]) + '@ms'\n",
    "serving.add_trigger('serving_stream',\n",
    "                    nuclio.triggers.V3IOStreamTrigger(url=serving_stream,\n",
    "                                                      maxWorkers=stream_configs['serving-stream']['shard_count']+2,\n",
    "                                                      seekTo='earliest'))\n",
    "serving.spec.config.pop('spec.triggers.http')\n",
    "serving.spec.readiness_timeout = 200\n",
    "serving.spec.replicas = 1\n",
    "\n",
    "serving.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same event-handler function for logging the inference stream to parquet.\n",
    "inference_logger = code_to_function(name='inference-logger', handler='handler', kind='nuclio', filename='functions/event-handler.ipynb')\n",
    "project.set_function(inference_logger)\n",
    "\n",
    "il_envs = {'PARQUET_SINK_FLAG': 'true',\n",
    "           'STREAM_SINK_FLAG': 'false',\n",
    "           'PARQUET_TARGET_PATH' : inference_parquet_target_path,\n",
    "           'PARQUET_BATCH_SIZE': 8192,\n",
    "           'TS_KEY': 'when',\n",
    "           'TS_FORMAT': '%Y-%m-%d %H:%M:%S.%f',\n",
    "           'FEATURES': \",\".join(feature_list),\n",
    "           'PREDICTIONS': 'about_to_churn',\n",
    "           'CONTAINER': container}\n",
    "project.func('inference-logger').set_envs({**v3io_envs, **il_envs})\n",
    "\n",
    "project.func('inference-logger').apply(mount_v3io())\n",
    "\n",
    "inference_stream = '/'.join(s.strip('/') for s in [web_api_users, stream_configs['inference-stream']['path']]) + '@il'\n",
    "project.func('inference-logger').add_trigger('inference_stream',\n",
    "                                               nuclio.triggers.V3IOStreamTrigger(url=inference_stream,\n",
    "                                                                                 maxWorkers=stream_configs['inference-stream']['shard_count']+2,\n",
    "                                                                                 seekTo='earliest'))\n",
    "project.func('inference-logger').spec.replicas=1\n",
    "project.func('inference-logger').deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}