{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end image classification workflow with distributed training\n",
    "The following example demonstrates an end to end data science workflow for building an an image classifier <br>\n",
    "The model is trained on an images dataset of cats and dogs. Then the model is deployed as a function in a serving layer <br>\n",
    "Users can send http request with an image of cats/dogs image and get a respond back that identify whether it is a cat or a dog\n",
    "\n",
    "This typical data science workflow comprises of the following:\n",
    "* Download anb label the dataset\n",
    "* Training a model on the images dataset\n",
    "* Deploy a function with the new model in a serving layer\n",
    "* Testing the function\n",
    "\n",
    "Key technologies:\n",
    "* Tensorflow-Keras for training the model\n",
    "* Horovod for running a distributed training\n",
    "* MLRun (open source library for tracking experiments https://github.com/mlrun/mlrun) for building the functions and tracking experiments\n",
    "* Nuclio function for creating a funciton that runs the model in a serving layer\n",
    "\n",
    "This demo is based on the following:<br>\n",
    "* https://github.com/tensorflow/docs/tree/master/site/en/tutorials\n",
    "* https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification/log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlrun clean -p -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for downloading and labeling images\n",
    "In the code below we have two functions: \n",
    "1. open_archive - Get and extract a zip file that contains cats and dog images. users need to pass the source URL and the target directory which is stored in Iguazio data layer\n",
    "2. categories_map_builder - labeling the dataset based on the file name. the functions creates a pandas dataframe with the filename and category (i.e. cat & dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometime after running pip install you need to restart the jupyer kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function config and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting spec.image to 'mlrun/mlrun'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config spec.image = \"mlrun/mlrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from tempfile import mktemp\n",
    "import pandas as pd\n",
    "\n",
    "# download the image archive\n",
    "def open_archive(context, \n",
    "                 target_dir='content',\n",
    "                 archive_url='',\n",
    "                 refresh=False):\n",
    "    \"\"\"Open a file/object archive into a target directory\n",
    "    \n",
    "    note: if `refresh` is True the content will be re-downloaded\n",
    "    \"\"\"\n",
    "    # does the target already exist, if yes skip download\n",
    "    if not os.path.isdir(target_dir) or refresh is True:\n",
    "        # Define locations\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        context.logger.info('Verified directories')\n",
    "\n",
    "        # Extract dataset from zip\n",
    "        context.logger.info('Extracting zip')\n",
    "        zip_ref = zipfile.ZipFile(archive_url, 'r')\n",
    "        zip_ref.extractall(target_dir)\n",
    "        zip_ref.close()\n",
    "\n",
    "        context.logger.info(f'extracted archive to {target_dir}')\n",
    "    context.log_artifact('content', local_path=target_dir)\n",
    "\n",
    "# build categories \n",
    "def categories_map_builder(context,\n",
    "                           source_dir,\n",
    "                           df_filename='file_categories_df.csv',\n",
    "                           map_filename='categories_map.json'):\n",
    "    \"\"\"Read labeled images from a directory and create category map + df\n",
    "    \n",
    "    filename format: <category>.NN.jpg\"\"\"\n",
    "    \n",
    "    # create filenames list (jpg only)\n",
    "    filenames = [file for file in os.listdir(source_dir) if file.endswith('.jpg')]\n",
    "    categories = []\n",
    "        \n",
    "    # Create a pandas DataFrame for the full sample\n",
    "    for filename in filenames:\n",
    "        category = filename.split('.')[0]\n",
    "        categories.append(category)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'category': categories\n",
    "    })\n",
    "    df['category'] = df['category'].astype('str')\n",
    "    \n",
    "    categories = df.category.unique()\n",
    "    categories = {i: category for i, category in enumerate(categories)}\n",
    "    with open(os.path.join(context.artifact_path, map_filename), 'w') as f:\n",
    "        f.write(json.dumps(categories))\n",
    "        \n",
    "    context.logger.info(categories)\n",
    "    context.log_artifact('categories_map', local_path=map_filename)\n",
    "    context.log_dataset('file_categories', df=df, local_path=df_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the MLRun database location and the base directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Setup and imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from mlrun import run_local, NewTask, mlconf, code_to_function, mount_v3io\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "\n",
    "# specify paths abd artifacts target location\n",
    "artifact_path = mlconf.artifact_path or path.abspath('./')\n",
    "base_dir = path.abspath('./')\n",
    "code_dir = path.join(base_dir, 'src') # Where our source code files are saved\n",
    "images_path = path.join(base_dir, 'images') \n",
    "\n",
    "project_name='image-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test locally, Download and extract image archive\n",
    "The dataset is taken from the Iguazio-sample bucket in S3 <br>\n",
    ">Note that this step is captured in the MLRun database. <br>\n",
    "\n",
    "We create a new local function with our inline code from above.  \n",
    "We then define a `NewTask` with the `open_archive` function handler and the needed parameters and run it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images from s3 using the local `open_archive` function\n",
    "open_archive_task = NewTask(name='download', \n",
    "                            handler=open_archive, \n",
    "                            params={'target_dir': images_path},\n",
    "                            inputs={'archive_url': 'http://iguazio-sample-data.s3.amazonaws.com/catsndogs.zip'})\n",
    "open_archive = run_local(open_archive_task, project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Data-Science Pipeline with MLRun and Kubeflow\n",
    "\n",
    "We are using a library called MLRun for running the functions and storing the experiments meta data in the MLRun database <br>\n",
    "Users can query the database to view all the experiments along with their associated meta data <br>\n",
    "- Get data\n",
    "- Create categories map\n",
    "- Train horovod model on the cluster\n",
    "- Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a multi-stage project (ingest, label, train, deploy model)\n",
    "\n",
    "Projects are used to package multiple functions, workflows, and artifacts. We usually store project code and definitions in a Git archive.\n",
    "\n",
    "The following code creates a new project in a local dir and initialize git tracking on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project, code_to_function\n",
    "project_dir = './'\n",
    "hvdproj = new_project(project_name, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add our `utils` function to the project\n",
    "We convert our inline (notebook) code to a function object and register that under our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = code_to_function(kind='job', name='utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.set_function(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run/test our function on the cluster \n",
    "We define a `NewTask` using the `categories_map_builder` function handler and the needed parameters.\n",
    "We add Iguazio v3io file mount (share the local paths with our function) and run the task remotely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categories map\n",
    "label_task = NewTask(name='label', \n",
    "                           handler=categories_map_builder, \n",
    "                           artifact_path=images_path,\n",
    "                           params={'source_dir': os.path.join(images_path, 'cats_n_dogs'),\n",
    "                                   'map_filename': 'categories_map.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.func('utils').apply(mount_v3io()).run(label_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a new function for distributed Training with TensorFlow, Keras and Horovod\n",
    "\n",
    "Here we use the same structure as before to deploy our **[cats vs. dogs tensorflow model training file](horovod-training.py)** to run on the defined horovod cluster in a distributed manner.  \n",
    "\n",
    "We define the input parameters for the training function.  \n",
    "We set the function's `kind='mpijob'` to let MLRun know to apply the job to the MPI CRD and create the requested horovod cluster.  \n",
    "We set the number of workers for the horovod cluster to use by setting `trainer.spec.replicas = 4` (default is 1 replica).  \n",
    "We set the number of GPUs each worker will receive by setting `trainer.gpus(1)` (default is 0 GPUs).\n",
    "> Please verify that the `HOROVOD_FILE` path is available from the cluster (Local path and Mounted path may vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function\n",
    "\n",
    "HOROVOD_FILE = os.path.join(code_dir, 'horovod-training.py')\n",
    "trainer = new_function(name='trainer',\n",
    "                       kind='mpijob',\n",
    "                       command=HOROVOD_FILE, \n",
    "                       image='mlrun/horovod:0.4.6')\n",
    "trainer.spec.image_pull_policy = 'Always'\n",
    "trainer.spec.replicas = 4\n",
    "trainer.gpus(1)\n",
    "hvdproj.set_function(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a serving function from the functions hub (marketplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.set_function('hub://tf-serving', 'serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the source images directory as a project artifact (can be accessed by name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.log_artifact('images', target_path='http://iguazio-sample-data.s3.amazonaws.com/catsndogs.zip')\n",
    "#print(hvdproj.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and save a pipeline \n",
    "\n",
    "The following workflow definition will be written into a file, it describes an execution graph (DAG) and how functions are conncted to form an end to end pipline. \n",
    "\n",
    "* Download the images \n",
    "* Label the images (Cats & Dogs)\n",
    "* Train the model using distributed TensorFlow (Horovod)\n",
    "* Deploy the model into a serverless function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    '''\n",
    "    This function will run before running the project.\n",
    "    It allows us to add our specific system configurations to the functions\n",
    "    like mounts or secrets if needed.\n",
    "\n",
    "    In this case we will add Iguazio's user mount to our functions using the\n",
    "    `mount_v3io()` function to automatically set the mount with the needed\n",
    "    variables taken from the environment. \n",
    "    * mount_v3io can be replaced with mlrun.platforms.mount_pvc() for \n",
    "    non-iguazio mount\n",
    "\n",
    "    @param functions: <function_name: function_yaml> dict of functions in the\n",
    "                        workflow\n",
    "    @param project: project object\n",
    "    @param secrets: secrets required for the functions for s3 connections and\n",
    "                    such\n",
    "    '''\n",
    "    for f in functions.values():\n",
    "        f.apply(mount_v3io())                  # On Iguazio (Auto-mount /User)\n",
    "        # f.apply(mlrun.platforms.mount_pvc()) # Non-Iguazio mount\n",
    "        \n",
    "    functions['serving'].set_env('MODEL_CLASS', 'TFModel')\n",
    "    functions['serving'].set_env('IMAGE_HEIGHT', '128')\n",
    "    functions['serving'].set_env('IMAGE_WIDTH', '128')\n",
    "    functions['serving'].set_env('ENABLE_EXPLAINER', 'False')\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Image classification demo',\n",
    "    description='Train an Image Classification TF Algorithm using MLRun'\n",
    ")\n",
    "def kfpipeline(\n",
    "        image_archive='store:///images',\n",
    "        images_path='/User/mlrun/examples/images',\n",
    "        source_dir='/User/mlrun/examples/images/cats_n_dogs',\n",
    "        checkpoints_dir='/User/mlrun/examples/checkpoints',\n",
    "        model_path='models/cats_n_dogs.h5',\n",
    "        model_name='cat_vs_dog_v1'):\n",
    "\n",
    "    # step 1: download images\n",
    "    open_archive = funcs['utils'].as_step(name='download',\n",
    "                                          handler='open_archive',\n",
    "                                          out_path=images_path,\n",
    "                                          params={'target_dir': images_path},\n",
    "                                          inputs={'archive_url': image_archive},\n",
    "                                          outputs=['content'])\n",
    "\n",
    "    # step 2: label images\n",
    "    label = funcs['utils'].as_step(name='label',\n",
    "                                   handler='categories_map_builder',\n",
    "                                   out_path=images_path,\n",
    "                                   params={'source_dir': source_dir},\n",
    "                                   outputs=['categories_map',\n",
    "                                            'file_categories']).after(open_archive)\n",
    "\n",
    "    # step 3: train the model\n",
    "    train = funcs['trainer'].as_step(name='train',\n",
    "                                     params={'epochs': 8,\n",
    "                                             'checkpoints_dir': checkpoints_dir,\n",
    "                                             'model_path'     : model_path,\n",
    "                                             'data_path'      : source_dir,\n",
    "                                             'batch_size'     : 224},\n",
    "                                     inputs={\n",
    "                                         'categories_map': label.outputs['categories_map'],\n",
    "                                         'file_categories': label.outputs['file_categories']},\n",
    "                                     outputs=['model'])\n",
    "    train.container.set_image_pull_policy('Always')\n",
    "\n",
    "    # deploy the model using nuclio functions\n",
    "    deploy = funcs['serving'].deploy_step(models={model_name: train.outputs['model']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.set_workflow('main', 'src/workflow.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvdproj.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run-pipeline'></a>\n",
    "## Run a pipeline workflow\n",
    "You can check the **[workflow.py](src/workflow.py)** file to see how functions objects are initialized and used (by name) inside the workflow.\n",
    "The `workflow.py` file has two parts, initialize the function objects and define pipeline dsl (connect the function inputs and outputs).\n",
    "\n",
    "> Note the pipeline can include CI steps like building container images and deploying models.\n",
    "\n",
    "\n",
    "\n",
    "### Run\n",
    "use the `run` method to execute a workflow, you can provide alternative arguments and specify the default target for workflow artifacts.<br>\n",
    "The workflow ID is returned and can be used to track the progress or you can use the hyperlinks\n",
    "\n",
    "> Note: The same command can be issued through CLI commands:<br>\n",
    "    `mlrun project my-proj/ -r main -p \"v3io:///users/admin/mlrun/kfp/{{workflow.uid}}/\"`\n",
    "\n",
    "The dirty flag allow us to run a project with uncommited changes (when the notebook is in the same git dir it will always be dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = hvdproj.run(\n",
    "    'main',\n",
    "    arguments={}, \n",
    "    artifact_path=artifact_path, \n",
    "    dirty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import get_run_db\n",
    "db = get_run_db().connect()\n",
    "db.list_runs(project=hvdproj.name, labels=f'workflow={run_id}').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the serving function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function has been deployed we can test it as a regular REST Endpoint using `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing event\n",
    "cat_image_url = 'https://s3.amazonaws.com/iguazio-sample-data/images/catanddog/cat.102.jpg'\n",
    "response = requests.get(cat_image_url)\n",
    "cat_image = response.content\n",
    "img = Image.open(BytesIO(cat_image))\n",
    "\n",
    "print('Test image:')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test The Serving Function (with Image URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = 'http://tf-images-server:8080' \n",
    "model_name='cat_vs_dog_v1'\n",
    "headers = {'Content-type': 'text/plain'}\n",
    "response = requests.post(url=f'{addr}/{model_name}/predict', \n",
    "                         data=json.dumps({'data_url': cat_image_url}), \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test The Serving Function (with Jpeg Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-type': 'image/jpeg'}\n",
    "response = requests.post(url=f'{addr}/{model_name}/predict', \n",
    "                         data=cat_image, \n",
    "                         headers=headers)\n",
    "print(response.content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
