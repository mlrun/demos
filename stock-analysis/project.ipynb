{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stocks Analysis Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# MLRun imports\n",
    "from mlrun import mlconf\n",
    "\n",
    "\n",
    "# Setup API Endpoint\n",
    "mlconf.dbpath = 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLRun stocks project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_project\n",
    "\n",
    "# update the dir and repo to reflect real locations \n",
    "# the remote git repo must be initialized in GitHub\n",
    "project_dir = os.path.abspath('./')\n",
    "remote_git = 'https://github.com/mlrun/demos.git'\n",
    "\n",
    "# Create the project\n",
    "project = new_project('stocks', project_dir, init_git=False)\n",
    "\n",
    "# We can update our project directory to the latest status by running\n",
    "# newproj.pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an artifact path to keep track of where our artifacts are going\n",
    "ARTIFACT_PATH =  os.path.join(os.path.abspath('./'), 'artifacts')\n",
    "mlconf.artifact_path = ARTIFACT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7f6b6cf9de50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set functions to project\n",
    "# project.set_function('code/00-train-sentiment-analysis-model.ipynb', name='bert_sentiment_classifier_trainer', kind='job')\n",
    "project.set_function('code/00-train-sentiment-analysis-model.ipynb', name='bert_sentiment_classifier_trainer')\n",
    "project.set_function('code/01-read-stocks.ipynb', name='stocks_reader')\n",
    "project.set_function('code/02-read-news.ipynb', name='news_reader')\n",
    "project.set_function('code/03-stream-viewer.ipynb', name='stream_viewer')\n",
    "project.set_function('hub://sentiment_analysis_serving', name='sentiment_analysis_server')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a pre-trained model (optional)\n",
    "Since running the [training](training/bert_sentiment_classification.ipynb) part to achieve good results may take some time, we had already trained and uploaded a model to a public location.  \n",
    "You can easily download it by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to download the pre-trained model to your `models` directory\n",
    "\n",
    "import os\n",
    "model_location = 'https://iguazio-sample-data.s3.amazonaws.com/models/model.pt'\n",
    "saved_models_directory = os.path.join(os.path.abspath('./'), 'models')\n",
    "\n",
    "# Create paths\n",
    "os.makedirs(saved_models_directory, exist_ok=1)\n",
    "model_filepath = os.path.join(saved_models_directory, os.path.basename(model_location))\n",
    "!wget -nc -P {saved_models_directory} {model_location} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Grafana Dashboard  \n",
    "To track the different stocks on a live dashboard we will use **Grafana**.  <br>\n",
    "We will use [Grafwiz](https://github.com/v3io/grafwiz) to define and deploy the dashboard directly from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Grafwiz and the latest grafanalib are installed\n",
    "!python -m pip install git+https://github.com/v3io/grafwiz\n",
    "!python -m pip install grafanalib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasource Iguazio already exists\n",
      "Datasource Iguazio created successfully\n",
      "Datasource stream-viewer already exists\n",
      "Datasource stream-viewer created successfully\n",
      "Dashboard stocks created successfully\n"
     ]
    }
   ],
   "source": [
    "from grafwiz import *\n",
    "import v3io_frames as v3f\n",
    "\n",
    "def deploy_dashboard(grafana_url:str = 'http://grafana', \n",
    "                     streamview_url:str = 'http://nuclio-stocks-stream-viewer:8080',\n",
    "                     v3io_container:str = 'bigdata',\n",
    "                     stocks_kv_table:str = 'stocks/stocks_kv',\n",
    "                     stocks_tsdb_table:str = 'stocks/stocks_tsdb'):\n",
    "    # Create datasources\n",
    "    DataSource(name='Iguazio').deploy(grafana_url, use_auth=True)\n",
    "    DataSource(name='stream-viewer', frames_url=streamview_url).deploy(grafana_url, use_auth=False, overwrite=False)\n",
    "    \n",
    "    # Verify the KV table can be shown\n",
    "    client = v3f.Client('framesd:8081', container=v3io_container)\n",
    "    try:\n",
    "        client.execute(backend='kv', table=stocks_kv_table, command='infer')\n",
    "    except: \n",
    "        print('KV Table has no values yet, please run `client.execute(backend=\"kv\", table=stocks_kv_table, command=\"infer\")` (can be found on the 05-explore notebook.)')\n",
    "    \n",
    "    # Create grafana dashboard\n",
    "    dash = Dashboard(\"stocks\", start='now-7d', dataSource='Iguazio')\n",
    "\n",
    "    # Add a symbol combo box (template) with data from the stocks table\n",
    "    dash.template(name=\"SYMBOL\", label=\"Symbol\", query=\"fields=symbol;table=stocks/stocks_kv;backend=kv;container=bigdata\")\n",
    "\n",
    "    # Create a table and log viewer in one row\n",
    "    tbl = Table('Current Stocks Value', span=12).source(table=stocks_kv_table,fields=['symbol','volume', 'price', 'sentiment', 'last_updated'],container=v3io_container)\n",
    "    dash.row([tbl])\n",
    "\n",
    "    # Create 2 charts on the second row\n",
    "    metrics_row = [Graph(metric).series(table=stocks_tsdb_table, fields=[metric], filter='symbol==\"$SYMBOL\"',container=v3io_container) for metric in ['price','volume']]\n",
    "    metrics_row.append(Graph('sentiment').series(table=stocks_tsdb_table, fields=['sentiment'], filter='symbol==\"$SYMBOL\"', container=v3io_container))\n",
    "    dash.row(metrics_row)\n",
    "    \n",
    "    # Create log veiwer panel\n",
    "    log = Table('Articles Log', dataSource='stream-viewer', span=12)\n",
    "    dash.row([log])\n",
    "    \n",
    "    # Deploy to Grafana\n",
    "    return dash.deploy(grafana_url)\n",
    "deploy_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deployment workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/workflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/workflow.py\n",
    "from kfp import dsl\n",
    "from mlrun import mount_v3io, mlconf\n",
    "import os\n",
    "from nuclio.triggers import V3IOStreamTrigger, CronTrigger\n",
    "\n",
    "funcs = {}\n",
    "\n",
    "# Directories and Paths\n",
    "projdir = os.path.abspath('./')\n",
    "model_filepath = os.path.join(projdir, 'models', 'model.pt') # Previously saved model if downloaded\n",
    "reviews_datafile = os.path.join(projdir, 'data', 'reviews.csv')\n",
    "\n",
    "# Performence limit\n",
    "max_replicas = 1\n",
    "\n",
    "# Readers cron interval\n",
    "readers_cron_interval = '300s'\n",
    "\n",
    "# Training GPU Allocation\n",
    "# Set to 0 if no gpus are to be used\n",
    "training_gpus = 0\n",
    "\n",
    "\n",
    "def init_functions(functions: dict, project=None, secrets=None):\n",
    "    for f in functions.values():\n",
    "        # Add V3IO Mount\n",
    "        f.apply(mount_v3io())\n",
    "        \n",
    "        # Always pull images to keep updates\n",
    "        f.spec.image_pull_policy = 'Always'\n",
    "    \n",
    "    # Define inference-stream related triggers\n",
    "    functions['sentiment_analysis_server'].add_model('bert_classifier_v1', model_filepath)\n",
    "    functions['sentiment_analysis_server'].spec.readiness_timeout = 500\n",
    "    functions['sentiment_analysis_server'].set_config('readinessTimeoutSeconds', 500)\n",
    "    \n",
    "    # Adept image to use CPU if a GPU is not assigned\n",
    "    if training_gpus == 0:\n",
    "        functions['sentiment_analysis_server'].spec.base_spec['spec']['build']['baseImage']='mlrun/ml-models'\n",
    "        functions['bert_sentiment_classifier_trainer'].spec.image='mlrun/ml-models'\n",
    "    \n",
    "    # Add triggers\n",
    "    functions['stocks_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    functions['news_reader'].add_trigger('cron', CronTrigger(readers_cron_interval))\n",
    "    \n",
    "    \n",
    "    # Set max replicas for resource limits\n",
    "    functions['sentiment_analysis_server'].spec.max_replicas = max_replicas\n",
    "    functions['news_reader'].spec.max_replicas = max_replicas\n",
    "    functions['stocks_reader'].spec.max_replicas = max_replicas\n",
    "    \n",
    "    # Add GPU for training\n",
    "    functions['bert_sentiment_classifier_trainer'].gpus(training_gpus)\n",
    "        \n",
    "@dsl.pipeline(\n",
    "    name='Stocks demo deployer',\n",
    "    description='Up to RT Stocks ingestion and analysis'\n",
    ")\n",
    "def kfpipeline(\n",
    "    # General\n",
    "    V3IO_CONTAINER = 'bigdata',\n",
    "    STOCKS_TSDB_TABLE = 'stocks/stocks_tsdb',\n",
    "    STOCKS_KV_TABLE = 'stocks/stocks_kv',\n",
    "    STOCKS_STREAM = 'stocks/stocks_stream',\n",
    "    RUN_TRAINER: bool = False,\n",
    "    \n",
    "    # Trainer\n",
    "    pretrained_model = 'bert-base-cased',\n",
    "    reviews_dataset = reviews_datafile,\n",
    "    models_dir = 'models',\n",
    "    model_filename = 'bert_sentiment_analysis_model.pt',\n",
    "    n_classes: int = 3,\n",
    "    MAX_LEN: int = 128,\n",
    "    BATCH_SIZE: int = 16,\n",
    "    EPOCHS: int =  2,\n",
    "    random_state: int = 42,\n",
    "    \n",
    "    # stocks reader\n",
    "    STOCK_LIST: list = ['GOOGL', 'MSFT', 'AMZN', 'AAPL', 'INTC'],\n",
    "    EXPRESSION_TEMPLATE = \"symbol='{symbol}';price={price};volume={volume};last_updated='{last_updated}'\",\n",
    "    \n",
    "    # Sentiment analysis server\n",
    "    model_name = 'bert_classifier_v1',\n",
    "    model_filepath = model_filepath # if not trained\n",
    "    \n",
    "    ):\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == True):\n",
    "        \n",
    "        deployer = funcs['bert_sentiment_classifier_trainer'].deploy_step()\n",
    "                \n",
    "        trainer = funcs['bert_sentiment_classifier_trainer'].as_step(name='bert_sentiment_classifier_trainer',\n",
    "                                                                     params={'pretrained_model': pretrained_model,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'models_dir': models_dir,\n",
    "                                                                             'model_filename': model_filename,\n",
    "                                                                             'n_classes': n_classes,\n",
    "                                                                             'MAX_LEN': MAX_LEN,\n",
    "                                                                             'BATCH_SIZE': BATCH_SIZE,\n",
    "                                                                             'EPOCHS': EPOCHS,\n",
    "                                                                             'random_state': random_state},\n",
    "                                                                     inputs={'reviews_dataset': reviews_dataset},\n",
    "                                                                     outputs=['bert_sentiment_analysis_model'],\n",
    "                                                                     image=deployer.outputs['image'])\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': trainer.outputs['bert_sentiment_analysis_model']})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    with dsl.Condition(RUN_TRAINER == False):\n",
    "        \n",
    "        sentiment_server = funcs['sentiment_analysis_server'].deploy_step(env={f'SERVING_MODEL_{model_name}': model_filepath})\n",
    "        \n",
    "        news_reader = funcs['news_reader'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'SENTIMENT_MODEL_ENDPOINT': sentiment_server.outputs['endpoint']})\n",
    "    \n",
    "    stocks_reader = funcs['stocks_reader'].deploy_step(env={'STOCK_LIST': STOCK_LIST,\n",
    "                                                            'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_TSDB_TABLE': STOCKS_TSDB_TABLE,\n",
    "                                                            'STOCKS_KV_TABLE': STOCKS_KV_TABLE,\n",
    "                                                            'EXPRESSION_TEMPLATE': EXPRESSION_TEMPLATE})\n",
    "    \n",
    "    stream_viewer = funcs['stream_viewer'].deploy_step(env={'V3IO_CONTAINER': V3IO_CONTAINER,\n",
    "                                                            'STOCKS_STREAM': STOCKS_STREAM}).after(news_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_workflow('main', os.path.join(os.path.abspath(project.context), 'code', 'workflow.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save(os.path.join(project.context, 'project.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-01-13 21:04:53,737 [info] using in-cluster config.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://dashboard.default-tenant.app.yh210.iguazio-cd2.com/pipelines/#/experiments/details/a714f208-21ff-42a6-9979-84813fe09fec\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://dashboard.default-tenant.app.yh210.iguazio-cd2.com/pipelines/#/runs/details/79e575da-134f-49f0-8c9f-67d406b5a79c\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-01-13 21:04:54,592 [info] Pipeline run id=79e575da-134f-49f0-8c9f-67d406b5a79c, check UI or DB for progress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'79e575da-134f-49f0-8c9f-67d406b5a79c'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.run('main', arguments={'RUN_TRAINER': True}, artifact_path=ARTIFACT_PATH, dirty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
