{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing BERT sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "ucTPIIJy8QjS",
    "outputId": "ab54a2ea-8cb6-4925-a9e8-e506f804cbf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.image to 'mlrun/ml-models-gpu'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind=\"job\"\n",
    "%nuclio config spec.image=\"mlrun/ml-models-gpu\"\n",
    "%nuclio cmd -c python -m pip install transformers==3.0.1 torch==1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "XpEvMc9v-hla",
    "outputId": "551914c8-e8e7-412a-92d0-2688ef4f47ea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from mlrun.artifacts import PlotArtifact, ChartArtifact, TableArtifact\n",
    "from mlrun.datastore import DataItem\n",
    "from mlrun import MLClientCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentimentClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model, n_classes):\n",
    "        super(BertSentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.out_linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        out = self.dropout(pooled_out)\n",
    "        out = self.out_linear(out)\n",
    "        return self.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LA_e_S8b8Qjo"
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(data.Dataset):\n",
    "    def __init__(self, review, target, tokenizer, max_len):\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.review[item])\n",
    "        enc = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "            truncation=True)\n",
    "        \n",
    "        return {'input_ids': enc['input_ids'].squeeze(0), \n",
    "                'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "                'targets': torch.tensor(self.target[item], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NVlWc-o8Qjh"
   },
   "outputs": [],
   "source": [
    "def score_to_sents(score):\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    if score == 3:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = ReviewsDataset(\n",
    "        review=df.content.to_numpy(),\n",
    "        target=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len)\n",
    "    \n",
    "    return data.DataLoader(dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    n_examples,\n",
    "    device\n",
    "):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_preds = 0\n",
    "    \n",
    "    for i, d in enumerate(data_loader):\n",
    "        if i % 50 == 0:\n",
    "            print(f'batch {i + 1}/ {len(data_loader)}')\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        targets = d['targets'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        correct_preds += torch.sum(pred == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return (correct_preds.double() / n_examples).detach().cpu().numpy(), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    criterion,\n",
    "    n_examples,\n",
    "    device\n",
    "):\n",
    "    print('evaluation')\n",
    "    model = model.eval()\n",
    "    correct_preds = 0\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(data_loader):\n",
    "            if i % 50 == 0:\n",
    "                print(f'batch {i + 1}/ {len(data_loader)}')\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            correct_preds += torch.sum(pred == targets)\n",
    "            losses.append(loss.item())\n",
    "    return (correct_preds.double() / n_examples).detach().cpu().numpy(), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(model_path, data_loader, device, n_examples, pretrained_model, n_classes):\n",
    "    model = BertSentimentClassifier(pretrained_model, n_classes).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    correct_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, d in enumerate(data_loader):\n",
    "            if i % 50 == 0:\n",
    "                print(f'batch {i + 1}/ {len(data_loader)}')\n",
    "\n",
    "            input_ids = d['input_ids'].to(device)\n",
    "            attention_mask = d['attention_mask'].to(device)\n",
    "            targets = d['targets'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            correct_preds += torch.sum(pred == targets)\n",
    "    return correct_preds.double() / n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiment_analysis_model(context: MLClientCtx, \n",
    "                                   reviews_dataset: DataItem,\n",
    "                                   pretrained_model: str = 'bert-base-cased', \n",
    "                                   models_dir: str = 'models',\n",
    "                                   model_filename: str = 'bert_sentiment_analysis_model.pt',\n",
    "                                   n_classes: int = 3,\n",
    "                                   MAX_LEN: int = 128,\n",
    "                                   BATCH_SIZE: int = 16,\n",
    "                                   EPOCHS: int = 50,\n",
    "                                   random_state: int = 42):\n",
    "\n",
    "    # Check for CPU or GPU \n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    base_path = os.path.abspath(context.artifact_path)\n",
    "    plots_path = os.path.join(base_path, 'plots')\n",
    "    data_path = os.path.join(base_path, 'data')\n",
    "    context.logger.info(f'Using {device}')\n",
    "    \n",
    "    models_basepath = os.path.join(context.artifact_path, models_dir)\n",
    "    os.makedirs(models_basepath, exist_ok=True)\n",
    "    model_filepath = os.path.join(models_basepath, model_filename)\n",
    "    \n",
    "    # Get dataset\n",
    "    df = reviews_dataset.as_df()\n",
    "    \n",
    "    # Save score plot\n",
    "    df = df[['content', 'score']]\n",
    "    sns.distplot(df.score)\n",
    "    reviews_scores_artifact = context.log_artifact(PlotArtifact(f\"reviews-scores\", body=plt.gcf()),\n",
    "                                                   target_path=f\"{plots_path}/reviews-scores.html\")\n",
    "    \n",
    "    # Turn scores to sentiment label\n",
    "    df['sentiment'] = df['score'].apply(score_to_sents)\n",
    "    \n",
    "    # Load bert tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "    \n",
    "    # Tokenize reviews\n",
    "    lens = [len(tokenizer.encode(df.loc[review]['content'])) for review in df.index]\n",
    "    max_length = max(lens)\n",
    "    context.logger.info(f'longest review: {max_length}')\n",
    "    plt.clf()\n",
    "    sns.distplot(lens)\n",
    "    reviews_lengths_artifact = context.log_artifact(PlotArtifact(f\"reviews-lengths\", body=plt.gcf()),\n",
    "                                                    target_path=f\"{plots_path}/reviews-lengths.html\")\n",
    "    \n",
    "    # Create training and validation datasets\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=random_state)\n",
    "    df_dev, df_test = train_test_split(df_test, test_size = 0.5, random_state=random_state)\n",
    "    \n",
    "    # Create dataloaders for all datasets\n",
    "    train_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "    dev_loader = create_data_loader(df_dev, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "    test_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "    \n",
    "    # Load the bert sentiment classifier base\n",
    "    model = BertSentimentClassifier(pretrained_model, n_classes=n_classes).to(device)\n",
    "    \n",
    "    # training\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_acc = train_acc = train_loss = dev_acc = dev_loss = 0\n",
    "\n",
    "    context.logger.info('Started training the model')\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            len(df_train),\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        dev_acc, dev_loss = eval_model(\n",
    "            model,\n",
    "            dev_loader,\n",
    "            criterion,\n",
    "            len(df_dev),\n",
    "            device\n",
    "        )\n",
    "\n",
    "        # Append results to history\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['dev_acc'].append(dev_acc)\n",
    "        history['dev_loss'].append(dev_loss)\n",
    "        context.logger.info(f'Epoch: {epoch + 1}/{EPOCHS}: Train loss: {train_loss}, accuracy: {train_acc} Val loss: {dev_loss}, accuracy: {dev_acc}')\n",
    "\n",
    "        if dev_acc > best_acc:\n",
    "            torch.save(model.state_dict(), model_filepath)\n",
    "            context.logger.info(f'Updating model, Current models is better then the previous one ({best_acc} vs. {dev_acc}).')\n",
    "            best_acc = dev_acc\n",
    "    \n",
    "    context.logger.info('Finished training, testing and logging results')\n",
    "    chart = ChartArtifact('summary')\n",
    "    chart.header = ['epoch', 'accuracy', 'val_accuracy', 'loss', 'val_loss']\n",
    "    for i in range(len(history['train_acc'])):\n",
    "        chart.add_row([i + 1, history['train_acc'][i],\n",
    "                       history['train_loss'][i],\n",
    "                       history['dev_acc'][i],\n",
    "                       history['dev_loss'][i]])\n",
    "    summary = context.log_artifact(chart, local_path=os.path.join('plots', 'summary.html'))\n",
    "\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_table = TableArtifact('history', df=history_df)\n",
    "    history_artifact = context.log_artifact(history_table, target_path=os.path.join(data_path, 'history.csv'))\n",
    "\n",
    "    test_acc = eval_on_test(model_filepath, test_loader, device, len(df_test), pretrained_model, n_classes)\n",
    "    context.logger.info(f'Received {test_acc} on test dataset')\n",
    "    results = {'train_accuracy': train_acc,\n",
    "               'train_loss': train_loss,\n",
    "               'best_acccuracy': best_acc,\n",
    "               'validation_accuracy': dev_acc,\n",
    "               'validation_loss': dev_loss}\n",
    "    context.log_results(results)\n",
    "    context.log_model(key='bert_sentiment_analysis_model',\n",
    "                      model_file=model_filename,\n",
    "                      model_dir=models_dir,\n",
    "                      artifact_path=context.artifact_path,\n",
    "                      upload=False,\n",
    "                      labels={'framework': 'pytorch',\n",
    "                              'category': 'nlp',\n",
    "                              'action': 'sentiment_analysis'},\n",
    "                      metrics=context.results,\n",
    "                      parameters={'pretrained_model': pretrained_model,\n",
    "                                  'MAX_LEN': MAX_LEN,\n",
    "                                  'BATCH_SIZE': BATCH_SIZE,\n",
    "                                  'EPOCHS': EPOCHS,\n",
    "                                  'random_state': random_state},\n",
    "                      extra_data={'reviews_scores': reviews_scores_artifact,\n",
    "                                  'reviews_length': reviews_lengths_artifact,\n",
    "                                  'training_history': history_artifact})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function, mount_v3io, run_local, NewTask\n",
    "\n",
    "reviews_datafile = os.path.join(os.path.abspath('..'), 'data', 'reviews.csv')\n",
    "pretrained_model = 'bert-base-cased'\n",
    "\n",
    "task = NewTask(params={'pretrained_model': pretrained_model,\n",
    "                       'EPOCHS': 2},\n",
    "               inputs={'reviews_dataset': reviews_datafile})\n",
    "# lrun = run_local(task, handler=train_sentiment_analysis_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-14 09:55:39,793 [info] function spec saved to path: bert_sentiment_classification.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f8091940090>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = code_to_function(name='train_sentiment_analysis',\n",
    "                      project='stocks',\n",
    "                      handler='train_sentiment_analysis_model')\n",
    "fn.gpus(1)\n",
    "fn.export('bert_sentiment_classification.yaml')\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-10-14 08:38:17,960 [info] starting run train-sentiment-analysis-train_sentiment_analysis_model uid=cc7a4299758244f6a51574bc2db371f9  -> http://mlrun-api:8080\n",
      "> 2020-10-14 08:38:18,106 [info] Job is running in the background, pod: train-sentiment-analysis-train-sentiment-analysis-model-8q2s9\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 496kB/s]  \n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 274kB/s]\n",
      "Downloading: 100%|██████████| 436M/436M [00:11<00:00, 38.2MB/s] \n",
      "> 2020-10-14 08:38:28,729 [info] Using cuda:0\n",
      "> 2020-10-14 08:38:43,330 [info] longest review: 520\n",
      "> 2020-10-14 08:39:02,284 [info] Started training the model\n",
      "batch 1/ 788\n",
      "batch 51/ 788\n",
      "batch 101/ 788\n",
      "batch 151/ 788\n",
      "batch 201/ 788\n",
      "batch 251/ 788\n",
      "batch 301/ 788\n",
      "batch 351/ 788\n",
      "batch 401/ 788\n",
      "batch 451/ 788\n",
      "batch 501/ 788\n",
      "batch 551/ 788\n",
      "batch 601/ 788\n",
      "batch 651/ 788\n",
      "batch 701/ 788\n",
      "batch 751/ 788\n",
      "evaluation\n",
      "batch 1/ 99\n",
      "batch 51/ 99\n",
      "> 2020-10-14 08:47:00,564 [info] Epoch: 1/2: Train loss: 0.9106895937229776, accuracy: 0.6201174976182915 Val loss: 0.8440537663421246, accuracy: 0.7003174603174603\n",
      "> 2020-10-14 08:47:04,137 [info] Updating model, Current models is better then the previous one (0 vs. 0.7003174603174603).\n",
      "batch 1/ 788\n",
      "batch 51/ 788\n",
      "batch 101/ 788\n",
      "batch 151/ 788\n",
      "batch 201/ 788\n",
      "batch 251/ 788\n",
      "batch 301/ 788\n",
      "batch 351/ 788\n",
      "batch 401/ 788\n",
      "batch 451/ 788\n",
      "batch 501/ 788\n",
      "batch 551/ 788\n",
      "batch 601/ 788\n",
      "batch 651/ 788\n",
      "batch 701/ 788\n",
      "batch 751/ 788\n",
      "evaluation\n",
      "batch 1/ 99\n",
      "batch 51/ 99\n",
      "> 2020-10-14 08:57:38,559 [info] Epoch: 2/2: Train loss: 0.7954725730540183, accuracy: 0.7526992696093998 Val loss: 0.8184037906954987, accuracy: 0.7263492063492063\n",
      "> 2020-10-14 08:57:42,181 [info] Updating model, Current models is better then the previous one (0.7003174603174603 vs. 0.7263492063492063).\n",
      "> 2020-10-14 08:57:42,181 [info] Finished training, testing and logging results\n",
      "batch 1/ 99\n",
      "batch 51/ 99\n",
      "> 2020-10-14 08:58:06,515 [info] Received 0.7320634920634921 on test dataset\n",
      "> 2020-10-14 08:58:06,598 [info] run executed, status=completed\n",
      "final state: succeeded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #b3edff;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #ffe6cc;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #ffe6cc;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>stocks</td>\n",
       "      <td><div title=\"cc7a4299758244f6a51574bc2db371f9\"><a href=\"https://mlrun-ui.default-tenant.app.dev6.lab.iguazeng.com/projects/stocks/jobs/cc7a4299758244f6a51574bc2db371f9/info\" target=\"_blank\" >...2db371f9</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Oct 14 08:38:28</td>\n",
       "      <td>completed</td>\n",
       "      <td>train-sentiment-analysis-train_sentiment_analysis_model</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=admin</div><div class=\"dictlist\">kind=job</div><div class=\"dictlist\">owner=admin</div><div class=\"dictlist\">host=train-sentiment-analysis-train-sentiment-analysis-model-8q2s9</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resultfb4188e7\" title=\"/files/stock-trading/data/reviews.csv\">reviews_dataset</div></td>\n",
       "      <td><div class=\"dictlist\">pretrained_model=bert-base-cased</div><div class=\"dictlist\">EPOCHS=2</div></td>\n",
       "      <td><div class=\"dictlist\">train_accuracy=0.7526992696093998</div><div class=\"dictlist\">train_loss=0.7954725730540183</div><div class=\"dictlist\">best_acccuracy=0.7263492063492063</div><div class=\"dictlist\">validation_accuracy=0.7263492063492063</div><div class=\"dictlist\">validation_loss=0.8184037906954987</div></td>\n",
       "      <td><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resultfb4188e7\" title=\"/files/stock-trading/plots/reviews-scores.html\">reviews-scores</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resultfb4188e7\" title=\"/files/stock-trading/plots/reviews-lengths.html\">reviews-lengths</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resultfb4188e7\" title=\"/files/stock-trading/plots/summary.html\">summary</div><div class=\"artifact\" onclick=\"expandPanel(this)\" paneName=\"resultfb4188e7\" title=\"/files/stock-trading/data/history.csv\">history</div><div title=\"/User/stock-trading/models\">bert_sentiment_analysis_model</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resultfb4188e7-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resultfb4188e7-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resultfb4188e7\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resultfb4188e7-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to track results use .show() or .logs() or in CLI: \n",
      "!mlrun get run cc7a4299758244f6a51574bc2db371f9 --project stocks , !mlrun logs cc7a4299758244f6a51574bc2db371f9 --project stocks\n",
      "> 2020-10-14 08:58:11,565 [info] run executed, status=completed\n"
     ]
    }
   ],
   "source": [
    "run = fn.with_code().run(task, artifact_path='/User/stock-trading/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of bert_sentiment_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
